
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Output Size Limitations &#8212; Taming Language Models: A Practical Guide to LLM Pitfalls with Python Examples</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'output_size_limit-NOTES';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Taming Language Models: A Practical Guide to LLM Pitfalls with Python Examples - Home"/>
    <img src="_static/logo.png" class="logo__image only-dark pst-js-only" alt="Taming Language Models: A Practical Guide to LLM Pitfalls with Python Examples - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Table of Contents
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="nondeterminism.html">Non-determinism</a></li>
<li class="toctree-l1"><a class="reference internal" href="output_size_limit.html">Output Size Limitations</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/souzatharsis/tamingLLMs" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/souzatharsis/tamingLLMs/issues/new?title=Issue%20on%20page%20%2Foutput_size_limit-NOTES.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/output_size_limit-NOTES.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Output Size Limitations</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-token-limits">What are Token Limits?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-statement">Problem Statement</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#content-chunking-with-contextual-linking">Content Chunking with Contextual Linking</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-the-implementation">Testing the Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#best-practices">Best Practices</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implications">Implications</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#future-considerations">Future Considerations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="output-size-limitations">
<h1>Output Size Limitations<a class="headerlink" href="#output-size-limitations" title="Link to this heading">#</a></h1>
<section id="what-are-token-limits">
<h2>What are Token Limits?<a class="headerlink" href="#what-are-token-limits" title="Link to this heading">#</a></h2>
<p>Tokens are the basic units that LLMs process text with. A token can be as short as a single character or as long as a complete word. In English, a general rule of thumb is that 1 token ≈ 4 characters or ¾ of a word.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">max_output_tokens</span></code> is parameter often available in modern LLMs that determines the maximum length of text that an LLM can generate in a single response. Contrary to what one might expect, the model does not “summarizes the answer” such that it does not surpass <code class="docutils literal notranslate"><span class="pre">max_output_tokens</span></code> limit. Instead, it will stop once it reaches this limit, even mid-sentence, i.e. the response may be truncated.</p>
<p>| <strong>Token Cost and Length Limitation Comparison Across Key Models</strong> | | | | |</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>max_output_tokens</p></th>
<th class="head"><p>max_input_tokens</p></th>
<th class="head"><p>input_cost_per_token</p></th>
<th class="head"><p>output_cost_per_token</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>meta.llama3-2-11b-instruct-v1:0</p></td>
<td><p>4096</p></td>
<td><p>128000</p></td>
<td><p>3.5e-7</p></td>
<td><p>3.5e-7</p></td>
</tr>
<tr class="row-odd"><td><p>claude-3-5-sonnet-20241022</p></td>
<td><p>8192</p></td>
<td><p>200000</p></td>
<td><p>3e-6</p></td>
<td><p>1.5e-5</p></td>
</tr>
<tr class="row-even"><td><p>gpt-4-0613</p></td>
<td><p>4096</p></td>
<td><p>8192</p></td>
<td><p>3e-5</p></td>
<td><p>6e-5</p></td>
</tr>
<tr class="row-odd"><td><p>gpt-4-turbo-2024-04-09</p></td>
<td><p>4096</p></td>
<td><p>128000</p></td>
<td><p>1e-5</p></td>
<td><p>3e-5</p></td>
</tr>
<tr class="row-even"><td><p>gpt-4o-mini</p></td>
<td><p>16384</p></td>
<td><p>128000</p></td>
<td><p>1.5e-7</p></td>
<td><p>6e-7</p></td>
</tr>
<tr class="row-odd"><td><p>gemini/gemini-1.5-flash-002</p></td>
<td><p>8192</p></td>
<td><p>1048576</p></td>
<td><p>7.5e-8</p></td>
<td><p>3e-7</p></td>
</tr>
<tr class="row-even"><td><p>gemini/gemini-1.5-pro-002</p></td>
<td><p>8192</p></td>
<td><p>2097152</p></td>
<td><p>3.5e-6</p></td>
<td><p>1.05e-5</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="problem-statement">
<h2>Problem Statement<a class="headerlink" href="#problem-statement" title="Link to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">max_output_tokens</span></code> limit in LLMs poses a significant challenge for users who need to generate long outputs, as it may result in truncated content and/or incomplete information.</p>
<ol class="arabic simple">
<li><p><strong>Truncated Content</strong>: Users aiming to generate extensive content, such as detailed reports or comprehensive articles, may find their outputs abruptly cut off due to the <code class="docutils literal notranslate"><span class="pre">max_output_tokens</span></code> limit. This truncation can result in incomplete information and disrupt the flow of the content.</p></li>
<li><p><strong>Shallow Responses</strong>: When users expect a complete and thorough response but receive only a partial output, it can lead to dissatisfaction and frustration. This is especially true in applications where the completeness of information is critical, such as in educational tools or content creation platforms.</p></li>
</ol>
<p>To effectively address these challenges, developers need to implement robust solutions that balance user expectations with technical and cost constraints, ensuring that long-form content generation remains feasible and efficient.</p>
<section id="content-chunking-with-contextual-linking">
<h3>Content Chunking with Contextual Linking<a class="headerlink" href="#content-chunking-with-contextual-linking" title="Link to this heading">#</a></h3>
<p>Content chunking with contextual linking is a technique used to manage the <code class="docutils literal notranslate"><span class="pre">max_output_tokens</span></code> limitation by breaking down long-form content into smaller, manageable chunks. This approach allows the LLM to focus on smaller sections of the input, enabling it to generate more complete and detailed responses for each chunk while maintaining coherence and context across the entire output.</p>
<ol class="arabic simple">
<li><p><strong>Chunking the Content</strong>: The input content is split into smaller chunks. This allows the LLM to process each chunk individually, focusing on generating a complete and detailed response for that specific section of the input.</p></li>
<li><p><strong>Maintaining Context</strong>: Each chunk is linked with contextual information from the previous chunks. This helps in maintaining the flow and coherence of the content across multiple chunks.</p></li>
<li><p><strong>Generating Linked Prompts</strong>: For each chunk, a prompt is generated that includes the chunk’s content and its context. This prompt is then used to generate the output for that chunk.</p></li>
<li><p><strong>Combining the Outputs</strong>: The outputs of all chunks are combined to form the final long-form content.</p></li>
</ol>
<p>By following these steps, developers can effectively manage the <code class="docutils literal notranslate"><span class="pre">max_output_tokens</span></code> limitation and generate coherent long-form content without truncation.</p>
<p>Let’s examine a robust solution for handling long-form content generation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span>
<span class="kn">import</span> <span class="nn">json</span>

<span class="k">class</span> <span class="nc">ConversationGenerator</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">api_client</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">api_client</span> <span class="o">=</span> <span class="n">api_client</span>
        
    <span class="k">def</span> <span class="nf">chunk_content</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_content</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Split input content into manageable chunks while preserving context.&quot;&quot;&quot;</span>
        <span class="n">sentences</span> <span class="o">=</span> <span class="n">input_content</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;. &#39;</span><span class="p">)</span>
        <span class="n">chunks</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">current_chunk</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">current_length</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
            <span class="n">sentence_length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">current_length</span> <span class="o">+</span> <span class="n">sentence_length</span> <span class="o">&gt;</span> <span class="n">chunk_size</span> <span class="ow">and</span> <span class="n">current_chunk</span><span class="p">:</span>
                <span class="n">chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;. &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">current_chunk</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
                <span class="n">current_chunk</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">current_length</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">current_chunk</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
            <span class="n">current_length</span> <span class="o">+=</span> <span class="n">sentence_length</span>
            
        <span class="k">if</span> <span class="n">current_chunk</span><span class="p">:</span>
            <span class="n">chunks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;. &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">current_chunk</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">chunks</span>

    <span class="k">def</span> <span class="nf">generate_conversation_prompts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">content_chunks</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate linked conversation prompts.&quot;&quot;&quot;</span>
        <span class="n">prompts</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">content_chunks</span><span class="p">):</span>
            <span class="n">prompt</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;part&quot;</span><span class="p">:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                <span class="s2">&quot;total_parts&quot;</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">content_chunks</span><span class="p">),</span>
                <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">chunk</span><span class="p">,</span>
                <span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_context</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">prompts</span><span class="p">)</span>
            <span class="p">}</span>
            <span class="n">prompts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">prompts</span>

    <span class="k">def</span> <span class="nf">_get_context</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">part_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">previous_prompts</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate context from previous parts.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">part_index</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="s2">&quot;Start of conversation&quot;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Continuing from part </span><span class="si">{</span><span class="n">part_index</span><span class="si">}</span><span class="s2">&quot;</span>
</pre></div>
</div>
</section>
<section id="testing-the-implementation">
<h3>Testing the Implementation<a class="headerlink" href="#testing-the-implementation" title="Link to this heading">#</a></h3>
<p>Here’s how to test the chunking functionality:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pytest</span>
<span class="kn">from</span> <span class="nn">conversation_generator</span> <span class="kn">import</span> <span class="n">ConversationGenerator</span>

<span class="k">def</span> <span class="nf">test_content_chunking</span><span class="p">():</span>
    <span class="n">generator</span> <span class="o">=</span> <span class="n">ConversationGenerator</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">long_content</span> <span class="o">=</span> <span class="s2">&quot;First sentence. Second sentence. &quot;</span> <span class="o">*</span> <span class="mi">100</span>
    <span class="n">chunks</span> <span class="o">=</span> <span class="n">generator</span><span class="o">.</span><span class="n">chunk_content</span><span class="p">(</span><span class="n">long_content</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span>
    <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">:</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">chunk</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">100</span>
        <span class="k">assert</span> <span class="n">chunk</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">test_prompt_generation</span><span class="p">():</span>
    <span class="n">generator</span> <span class="o">=</span> <span class="n">ConversationGenerator</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">chunks</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Chunk 1.&quot;</span><span class="p">,</span> <span class="s2">&quot;Chunk 2.&quot;</span><span class="p">,</span> <span class="s2">&quot;Chunk 3.&quot;</span><span class="p">]</span>
    <span class="n">prompts</span> <span class="o">=</span> <span class="n">generator</span><span class="o">.</span><span class="n">generate_conversation_prompts</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span>
    
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span>
    <span class="k">assert</span> <span class="n">prompts</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;part&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>
    <span class="k">assert</span> <span class="n">prompts</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="s2">&quot;context&quot;</span><span class="p">]</span> <span class="o">!=</span> <span class="n">prompts</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;context&quot;</span><span class="p">]</span>
</pre></div>
</div>
</section>
</section>
<section id="best-practices">
<h2>Best Practices<a class="headerlink" href="#best-practices" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>Always Monitor Token Usage</strong></p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">estimate_tokens</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Rough token count estimation.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">())</span> <span class="o">*</span> <span class="mf">1.3</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p><strong>Implement Graceful Fallbacks</strong></p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate_with_fallback</span><span class="p">(</span><span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">max_retries</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">attempt</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_retries</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">generate_full_response</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="k">except</span> <span class="n">TokenLimitError</span><span class="p">:</span>
            <span class="n">prompt</span> <span class="o">=</span> <span class="n">truncate_prompt</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">generate_summary</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p><strong>Use Efficient Prompt Templates</strong></p></li>
</ol>
<ul class="simple">
<li><p>Keep system prompts concise</p></li>
<li><p>Remove redundant context</p></li>
<li><p>Use compression techniques for long contexts</p></li>
</ul>
</section>
<section id="implications">
<h2>Implications<a class="headerlink" href="#implications" title="Link to this heading">#</a></h2>
<p>Implementing context chunking with contextual linking is a practical solution to manage the output size limitations of LLMs. However, this approach comes with its own set of implications that developers must consider.</p>
<ol class="arabic simple">
<li><p><strong>Increased Development Complexity</strong>: Implementing strategies to overcome the maximum output token length introduces additional layers of complexity to the application design. It necessitates meticulous management of context across multiple outputs to maintain coherence. Ensuring that each chunk retains the necessary context for the conversation or document can be challenging and often requires advanced logic to handle transitions seamlessly.</p></li>
<li><p><strong>Cost Implications</strong>: Attempting to circumvent the <code class="docutils literal notranslate"><span class="pre">max_output_tokens</span></code> limitation by making multiple requests can increase the number of tokens processed, thereby raising the operational costs associated with using LLM services. Each additional request contributes to the overall token usage, which can quickly escalate costs, especially for applications with high-frequency interactions or large volumes of data.</p></li>
<li><p><strong>Performance Bottlenecks</strong>: Generating long outputs in segments can lead to performance bottlenecks, as each segment may require additional processing time and resources, impacting the overall efficiency of the application. The need to manage and link multiple chunks can introduce latency and reduce the responsiveness of the system, which is critical for real-time applications.</p></li>
</ol>
<p>By understanding these implications, developers can better prepare for the challenges associated with context chunking and contextual linking, ensuring that their applications remain efficient, cost-effective, and user-friendly.</p>
</section>
<section id="future-considerations">
<h2>Future Considerations<a class="headerlink" href="#future-considerations" title="Link to this heading">#</a></h2>
<p>As models evolve, we can expect several advancements that will significantly impact how we handle output size limitations:</p>
<ol class="arabic simple">
<li><p><strong>Contextual Awareness</strong>: Future LLMs will likely have improved contextual awareness - or as Mustafa Suleyman would call “infinite memory”, enabling them to better understand and manage the context of a conversation or document over long interactions. This will reduce the need for repetitive context setting and improve the overall user experience.</p></li>
<li><p><strong>More Efficient Token Usage</strong>: Advances in model architecture and tokenization strategies will lead to more efficient token usage. This means that models will be able to convey the same amount of information using fewer tokens, reducing costs and improving performance.</p></li>
<li><p><strong>Improved Compression Techniques</strong>: As research progresses, we can expect the development of more sophisticated compression techniques that allow models to retain essential information while reducing the number of tokens required. This will be particularly useful for applications that need to summarize or condense large amounts of data.</p></li>
<li><p><strong>Adaptive Token Limits</strong>: Future models may implement adaptive token limits that dynamically adjust based on the complexity and requirements of the task at hand. This will provide more flexibility and efficiency in handling diverse use cases.</p></li>
<li><p><strong>Enhanced Memory Management</strong>: Innovations in memory management will allow models to handle larger outputs without a significant increase in computational resources. This will make it feasible to deploy advanced LLMs in resource-constrained environments.</p></li>
</ol>
<p>These advancements will collectively enhance the capabilities of LLMs, making them more powerful and versatile tools for a wide range of applications. However, they will also introduce new challenges and considerations that developers and researchers will need to address to fully harness their potential.</p>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>In conclusion, while managing output size limitations in LLMs presents significant challenges, it also drives innovation in application design and optimization strategies. By implementing techniques such as context chunking, efficient prompt templates, and graceful fallbacks, developers can mitigate these limitations and enhance the performance and cost-effectiveness of their applications. As the technology evolves, advancements in contextual awareness, token efficiency, and memory management will further empower developers to build more robust and scalable LLM-powered systems. It is crucial to stay informed about these developments and continuously adapt to leverage the full potential of LLMs while addressing their inherent constraints.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>OpenAI Token Limits Documentation</p></li>
<li><p>Anthropic Claude Documentation</p></li>
<li><p>Google PaLM 2 Technical Specifications</p></li>
<li><p>Research papers on context window management</p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-token-limits">What are Token Limits?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-statement">Problem Statement</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#content-chunking-with-contextual-linking">Content Chunking with Contextual Linking</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-the-implementation">Testing the Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#best-practices">Best Practices</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implications">Implications</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#future-considerations">Future Considerations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Tharsis T. P. Souza
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>