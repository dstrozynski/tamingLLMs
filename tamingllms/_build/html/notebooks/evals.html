<!DOCTYPE html>
<html  lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1"><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

      <title>4. The Evals Gap</title>
    
          <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
          <link rel="stylesheet" href="../_static/theme.css " type="text/css" />
          <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
          <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
          <link rel="stylesheet" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
          <link rel="stylesheet" href="../_static/sphinx-thebe.css" type="text/css" />
          <link rel="stylesheet" href="../_static/sphinx-design.4cbf315f70debaebd550c87a6162cf0f.min.css" type="text/css" />
      
      <!-- sphinx script_files -->
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script src="../_static/scripts/sphinx-book-theme.js"></script>
        <script>let toggleHintShow = 'Click to show';</script>
        <script>let toggleHintHide = 'Click to hide';</script>
        <script>let toggleOpenOnPrint = 'true';</script>
        <script src="../_static/togglebutton.js"></script>
        <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
        <script src="../_static/design-tabs.js"></script>
        <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
        <script async="async" src="../_static/sphinx-thebe.js"></script>

      
      <!-- bundled in js (rollup iife) -->
      <!-- <script src="../_static/theme-vendors.js"></script> -->
      <script src="../_static/theme.js" defer></script>
      <link rel="canonical" href="https://souzatharsis.github.io/tamingllms/notebooks/evals.html" />
    
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="prev" title="3. Wrestling with Structured Output" href="structured_output.html" /> 
  </head>

  <body>
    <div id="app">
    <div class="theme-container" :class="pageClasses"><navbar @toggle-sidebar="toggleSidebar">
  <router-link to="../markdown/toc.html" class="home-link">
    
      <span class="site-name">tamingLLMs</span>
    
  </router-link>

  <div class="links">
    <navlinks class="can-hide">



  
    <div class="nav-item">
      <a href="https://github.com/souzatharsis/tamingllms"
        class="nav-link external">
          Github <outboundlink></outboundlink>
      </a>
    </div>
  

    </navlinks>
  </div>
</navbar>

      
      <div class="sidebar-mask" @click="toggleSidebar(false)">
      </div>
        <sidebar @toggle-sidebar="toggleSidebar">
          
          <navlinks>
            



  
    <div class="nav-item">
      <a href="https://github.com/souzatharsis/tamingllms"
        class="nav-link external">
          Github <outboundlink></outboundlink>
      </a>
    </div>
  

            
          </navlinks><div id="searchbox" class="searchbox" role="search">
  <div class="caption"><span class="caption-text">Quick search</span>
    <div class="searchformwrapper">
      <form class="search" action="../search.html" method="get">
        <input type="text" name="q" />
        <input type="submit" value="Search" />
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
      </form>
    </div>
  </div>
</div><div class="sidebar-links" role="navigation" aria-label="main navigation">
  
    <div class="sidebar-group">
      <p class="caption">
        <span class="caption-text"><a href="../markdown/toc.html#taming-llms">taming llms</a></span>
      </p>
      <ul class="current">
        
          <li class="toctree-l1 ">
            
              <a href="../markdown/intro.html" class="reference internal ">Introduction</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="output_size_limit.html" class="reference internal ">Output Size Limitations</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="structured_output.html" class="reference internal ">Wrestling with Structured Output</a>
            

            
          </li>

        
          <li class="toctree-l1 current">
            
              <a href="#" class="reference internal current">The Evals Gap</a>
            

            
              <ul>
                
                  <li class="toctree-l2"><a href="#non-deterministic-machines" class="reference internal">Non-Deterministic Machines</a></li>
                
                  <li class="toctree-l2"><a href="#emerging-properties" class="reference internal">Emerging Properties</a></li>
                
                  <li class="toctree-l2"><a href="#problem-statement" class="reference internal">Problem Statement</a></li>
                
                  <li class="toctree-l2"><a href="#evals-design" class="reference internal">Evals Design</a></li>
                
                  <li class="toctree-l2"><a href="#metrics" class="reference internal">Metrics</a></li>
                
                  <li class="toctree-l2"><a href="#evaluators" class="reference internal">Evaluators</a></li>
                
                  <li class="toctree-l2"><a href="#benchmarks-and-leaderboards" class="reference internal">Benchmarks and Leaderboards</a></li>
                
                  <li class="toctree-l2"><a href="#tools" class="reference internal">Tools</a></li>
                
                  <li class="toctree-l2"><a href="#references" class="reference internal">References</a></li>
                
              </ul>
            
          </li>

        
      </ul>
    </div>
  
</div>
        </sidebar>

      <page>
          <div class="body-header" role="navigation" aria-label="navigation">
  
  <ul class="breadcrumbs">
    <li><a href="../markdown/toc.html">Docs</a> &raquo;</li>
    
    <li><span class="section-number">4. </span>The Evals Gap</li>
  </ul>
  

  <ul class="page-nav">
  <li class="prev">
    <a href="structured_output.html"
       title="previous chapter">← <span class="section-number">3. </span>Wrestling with Structured Output</a>
  </li>
</ul>
  
</div>
<hr>
          <div class="content" role="main" v-pre>
            
  <section class="tex2jax_ignore mathjax_ignore" id="the-evals-gap">
<h1><a class="toc-backref" href="#id40" role="doc-backlink"><span class="section-number">4. </span>The Evals Gap</a><a class="headerlink" href="#the-evals-gap" title="Permalink to this heading">¶</a></h1>
<blockquote class="epigraph">
<div><p>Evals are surprisingly often all you need.</p>
<p class="attribution">—Greg Brockman, OpenAI’s President</p>
</div></blockquote>
<nav class="contents" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#the-evals-gap" id="id40">The Evals Gap</a></p>
<ul>
<li><p><a class="reference internal" href="#non-deterministic-machines" id="id41">Non-Deterministic Machines</a></p>
<ul>
<li><p><a class="reference internal" href="#temperature-and-sampling" id="id42">Temperature and Sampling</a></p></li>
<li><p><a class="reference internal" href="#the-temperature-spectrum" id="id43">The Temperature Spectrum</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#emerging-properties" id="id44">Emerging Properties</a></p></li>
<li><p><a class="reference internal" href="#problem-statement" id="id45">Problem Statement</a></p></li>
<li><p><a class="reference internal" href="#evals-design" id="id46">Evals Design</a></p>
<ul>
<li><p><a class="reference internal" href="#conceptual-overview" id="id47">Conceptual Overview</a></p></li>
<li><p><a class="reference internal" href="#design-considerations" id="id48">Design Considerations</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#metrics" id="id49">Metrics</a></p></li>
<li><p><a class="reference internal" href="#evaluators" id="id50">Evaluators</a></p>
<ul>
<li><p><a class="reference internal" href="#model-based-evaluation" id="id51">Model-Based Evaluation</a></p></li>
<li><p><a class="reference internal" href="#human-based-evaluation" id="id52">Human-Based Evaluation</a></p></li>
<li><p><a class="reference internal" href="#evaluating-evaluators" id="id53">Evaluating Evaluators</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#benchmarks-and-leaderboards" id="id54">Benchmarks and Leaderboards</a></p></li>
<li><p><a class="reference internal" href="#tools" id="id55">Tools</a></p>
<ul>
<li><p><a class="reference internal" href="#lighteval" id="id56">LightEval</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#references" id="id57">References</a></p></li>
</ul>
</li>
</ul>
</nav>
<section id="non-deterministic-machines">
<h2><a class="toc-backref" href="#id41" role="doc-backlink"><span class="section-number">4.1. </span>Non-Deterministic Machines</a><a class="headerlink" href="#non-deterministic-machines" title="Permalink to this heading">¶</a></h2>
<p>One of the most fundamental challenges when building products with Large Language Models (LLMs) is their non-deterministic nature. Unlike traditional software systems where the same input reliably produces the same output, LLMs can generate different responses each time they’re queried - even with identical prompts and input data. This characteristic is both a strength and a significant engineering challenge.</p>
<p>When you ask ChatGPT or any other LLM the same question multiple times, you’ll likely get different responses. This isn’t a bug - it’s a fundamental feature of how these models work. The “temperature” parameter, which controls the randomness of outputs, allows models to be creative and generate diverse responses. However, this same feature makes it incredibly difficult to build reliable, testable systems.</p>
<p>Consider a financial services company using LLMs to generate investment advice. The non-deterministic nature of these models means that:</p>
<ul class="simple">
<li><p>The same market data could yield different analysis conclusions</p></li>
<li><p>Testing becomes exceedingly more complex compared to traditional software</p></li>
<li><p>Regulatory compliance becomes challenging to guarantee</p></li>
<li><p>User trust may be affected by inconsistent responses</p></li>
</ul>
<section id="temperature-and-sampling">
<h3><a class="toc-backref" href="#id42" role="doc-backlink"><span class="section-number">4.1.1. </span>Temperature and Sampling</a><a class="headerlink" href="#temperature-and-sampling" title="Permalink to this heading">¶</a></h3>
<p>The primary source of non-determinism in LLMs comes from their sampling strategies. During text generation, the model:</p>
<ol class="arabic simple">
<li><p>Calculates probability distributions for each next token</p></li>
<li><p>Samples from these distributions based on temperature settings</p></li>
<li><p>Uses techniques like nucleus sampling to balance creativity and coherence</p></li>
</ol>
</section>
<section id="the-temperature-spectrum">
<h3><a class="toc-backref" href="#id43" role="doc-backlink"><span class="section-number">4.1.2. </span>The Temperature Spectrum</a><a class="headerlink" href="#the-temperature-spectrum" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>Temperature = 0: Most deterministic, but potentially repetitive</p></li>
<li><p>Temperature = 1: Balanced creativity and coherence</p></li>
<li><p>Temperature &gt; 1: Increased randomness, potentially incoherent</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># Load environment variables from .env file</span>
<span class="n">load_dotenv</span><span class="p">()</span>

<span class="kn">from</span> <span class="nn">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>

<span class="k">def</span> <span class="nf">generate_responses</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">temperatures</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
    <span class="n">attempts</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate multiple responses at different temperature settings</span>
<span class="sd">    to demonstrate non-deterministic behavior.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">temp</span> <span class="ow">in</span> <span class="n">temperatures</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">attempt</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attempts</span><span class="p">):</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}],</span>
                <span class="n">temperature</span><span class="o">=</span><span class="n">temp</span><span class="p">,</span>
                <span class="n">max_tokens</span><span class="o">=</span><span class="mi">50</span>
            <span class="p">)</span>
            
            <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
                <span class="s1">&#39;temperature&#39;</span><span class="p">:</span> <span class="n">temp</span><span class="p">,</span>
                <span class="s1">&#39;attempt&#39;</span><span class="p">:</span> <span class="n">attempt</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                <span class="s1">&#39;response&#39;</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
            <span class="p">})</span>

    <span class="c1"># Display results grouped by temperature</span>
    <span class="n">df_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">temp</span> <span class="ow">in</span> <span class="n">temperatures</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Temperature = </span><span class="si">{</span><span class="n">temp</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>
        <span class="n">temp_responses</span> <span class="o">=</span> <span class="n">df_results</span><span class="p">[</span><span class="n">df_results</span><span class="p">[</span><span class="s1">&#39;temperature&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">temp</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">temp_responses</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attempt </span><span class="si">{</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;attempt&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;response&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">df_results</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">MAX_LENGTH</span> <span class="o">=</span> <span class="mi">10000</span> <span class="c1"># We limit the input length to avoid token issues</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;../data/apple.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">sec_filing</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<span class="n">sec_filing</span> <span class="o">=</span> <span class="n">sec_filing</span><span class="p">[:</span><span class="n">MAX_LENGTH</span><span class="p">]</span> 
<span class="n">df_results</span> <span class="o">=</span> <span class="n">generate_responses</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span> 
                                <span class="n">prompt</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Write a single-statement executive summary of the following text: </span><span class="si">{</span><span class="n">sec_filing</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> 
                                <span class="n">temperatures</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Temperature = 0.0
----------------------------------------
Attempt 1: Apple Inc. filed its Form 10-K for the fiscal year ended September 28, 2024 with the SEC, detailing its business operations and financial performance.
Attempt 2: Apple Inc. filed its Form 10-K with the SEC for the fiscal year ended September 28, 2024, detailing its business operations, products, and financial information.
Attempt 3: Apple Inc. filed its Form 10-K with the SEC for the fiscal year ended September 28, 2024, detailing its business operations, products, and financial information.

Temperature = 1.0
----------------------------------------
Attempt 1: Apple Inc., a well-known seasoned issuer based in California, designs, manufactures, and markets smartphones, personal computers, tablets, wearables, and accessories, with a focus on innovation and technology.
Attempt 2: Apple Inc. filed its Form 10-K with the SEC for the fiscal year ended September 28, 2024, reporting on its business operations, products, and financial performance.
Attempt 3: Apple Inc., a well-known seasoned issuer, filed its Form 10-K for the fiscal year ended September 28, 2024, reporting on its financial condition and operations.

Temperature = 2.0
----------------------------------------
Attempt 1: The Form 10-K for Apple Inc. for the fiscal year ended September 28, 2024, filed with the Securities and Exchange Commission, outlines the company&#39;s financial performance, products, and risk factors affecting future results.
Attempt 2: Apple Inc., a California-based company and leading technology manufacturer invDestacksmeticsisdiction setIspection-$20cyan evaluationseld anvisions droitEntering discernminerval Versbobprefversible vo该 Option和 meio forecast времCisco dellaischenpoihsCapabilities Geme.getTime future
Attempt 3: Apple Inc&#39;s Form 10-K provides a comprehensive overview of the company&#39;s financial reporting, business operations, products and market information.
</pre></div>
</div>
</div>
</div>
<p>This simple experiment reveals a fundamental challenge in LLM evaluation: even a simple parameter like temperature can dramatically alter model behavior in ways that are difficult to systematically assess. At temperature 0.0, responses are consistent but potentially too rigid. At 1.0, outputs become more varied but less predictable. At 2.0, responses can be wildly different and often incoherent. This non-deterministic behavior makes traditional software testing approaches inadequate.</p>
<p>The implications for evaluation are profound. How can one effectively test an LLM-powered system when the same prompt can yield radically different outputs based on a single parameter? Traditional testing relies on predictable inputs and outputs, but LLMs force us to grapple with probabilistic behavior. While lower temperatures may seem safer for critical applications, they don’t eliminate the underlying uncertainty - they merely mask it. This highlights the need for new evaluation paradigms that can handle both deterministic and probabilistic aspects of LLM behavior.</p>
</section>
</section>
<section id="emerging-properties">
<h2><a class="toc-backref" href="#id44" role="doc-backlink"><span class="section-number">4.2. </span>Emerging Properties</a><a class="headerlink" href="#emerging-properties" title="Permalink to this heading">¶</a></h2>
<p>Beyond their non-deterministic nature, LLMs present another fascinating challenge: emergent abilities that spontaneously arise as models scale up in size. These abilities - from basic question answering to complex reasoning - aren’t explicitly programmed but rather emerge “naturally” as the models grow larger and are trained on more data. This makes evaluation fundamentally different from traditional software testing, where capabilities are explicitly coded and can be tested against clear specifications.</p>
<figure class="align-center" id="id2">
<a class="bg-primary mb-1 reference internal image-reference" href="../_images/emerging.png"><img alt="Emerging Properties" class="bg-primary mb-1" src="../_images/emerging.png" style="width: 931.1999999999999px; height: 664.8px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.1 </span><span class="caption-text">Emergent abilities of large language models and the scale <span id="id1">[<a class="reference internal" href="#id19" title="Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. 2022. URL: https://arxiv.org/abs/2206.07682, arXiv:2206.07682.">Wei <em>et al.</em>, 2022</a>]</span>.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#id2"><span class="std std-numref">Fig. 4.1</span></a> provides a list of emergent abilities of large language models and the scale. The relationship between model scale and emergent abilities follows a fascinating non-linear pattern. Below certain size thresholds, specific abilities may be completely absent from the model - it simply cannot perform certain tasks, no matter how much you try to coax them out. However, once the model reaches critical points in its scaling journey, these abilities can suddenly manifest in what researchers call a phase transition - a dramatic shift from inability to capability. This unpredictable emergence of capabilities stands in stark contrast to traditional software development, where features are deliberately implemented and can be systematically tested.</p>
<p>The implications for evaluation are profound. While conventional software testing relies on stable test suites and well-defined acceptance criteria, LLM evaluation must contend with a constantly shifting landscape of capabilities. What worked to evaluate a 7B parameter model may be completely inadequate for a 70B parameter model that has developed new emergent abilities. This dynamic nature of LLM capabilities forces us to fundamentally rethink our approach to testing and evaluation.</p>
</section>
<section id="problem-statement">
<h2><a class="toc-backref" href="#id45" role="doc-backlink"><span class="section-number">4.3. </span>Problem Statement</a><a class="headerlink" href="#problem-statement" title="Permalink to this heading">¶</a></h2>
<p>Consider a practical example that illustrates these challenges: building a Math AI tutoring system for children powered by an LLM. In traditional software development, you would define specific features (like presenting math problems or checking answers) and write tests to verify each function. But with LLMs, you’re not just testing predefined features - you’re trying to evaluate emergent capabilities like adapting explanations to a child’s level, maintaining engagement through conversational learning, and providing age-appropriate encouragement and emotional support.</p>
<p>This fundamental difference raises critical questions about evaluation:</p>
<ul class="simple">
<li><p>How do we measure capabilities that weren’t explicitly programmed?</p></li>
<li><p>How can we ensure consistent performance when abilities may suddenly emerge or evolve?</p></li>
<li><p>What metrics can capture both the technical accuracy and the subjective quality of responses?</p></li>
</ul>
<p>The challenge becomes even more complex when we consider that traditional software evaluation methods simply weren’t designed for these kinds of systems - There is an evals gap between traditional software testing and LLM evaluation. We need new frameworks that can account for both the deterministic aspects we’re used to testing and the emergent properties that make LLMs unique.</p>
<p><a class="reference internal" href="#evals-table"><span class="std std-numref">Table 4.1</span></a> explores how LLM evaluation differs from traditional software testing across several key dimensions:</p>
<ul class="simple">
<li><p><strong>Capability Assessment vs Functional Testing</strong>: Traditional software testing validates specific functionality against predefined requirements. LLM evaluation must assess not necessarily pre-defined “emergent properties” like reasoning, creativity, and language understanding that extend beyond explicit programming.</p></li>
<li><p><strong>Metrics and Measurement Challenges</strong>: While traditional software metrics can usually be precisely defined and measured, LLM evaluation often involves subjective qualities like “helpfulness” or “naturalness” that resist straightforward quantification. Even when we try to break these down into numeric scores, the underlying judgment remains inherently human and context-dependent.</p></li>
<li><p><strong>Dataset Contamination</strong>: Traditional software testing uses carefully crafted test cases with known inputs and expected outputs (e.g., unit tests, integration tests). In contrast, LLMs trained on massive internet-scale datasets risk having already seen and memorized evaluation examples during training, which can lead to artificially inflated performance scores. This requires careful dataset curation to ensure test sets are truly unseen by the model and rigorous cross-validation approaches.</p></li>
<li><p><strong>Benchmark Evolution</strong>: Traditional software maintains stable test suites over time. LLM benchmarks continuously evolve as capabilities advance, making longitudinal performance comparisons difficult and potentially obsoleting older evaluation methods.</p></li>
<li><p><strong>Human Evaluation Requirements</strong>: Traditional software testing automates most validation. LLM evaluation may demand significant human oversight to assess output quality, appropriateness, and potential biases through structured annotation and systematic review processes.</p></li>
</ul>
<table class="docutils align-default" id="evals-table">
<caption><span class="caption-number">Table 4.1 </span><span class="caption-text">Evals of Traditional Software vs LLMs</span><a class="headerlink" href="#evals-table" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Aspect</p></th>
<th class="head"><p>Traditional Software</p></th>
<th class="head"><p>LLMs</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Capability Assessment</strong></p></td>
<td><p>Validates specific functionality against requirements</p></td>
<td><p>May assess emergent properties like reasoning and creativity</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Metrics and Measurement</strong></p></td>
<td><p>Precisely defined and measurable metrics</p></td>
<td><p>Subjective qualities that resist straightforward quantification</p></td>
</tr>
<tr class="row-even"><td><p><strong>Dataset Contamination</strong></p></td>
<td><p>Uses carefully crafted test cases</p></td>
<td><p>Risk of memorized evaluation examples from training</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Benchmark Evolution</strong></p></td>
<td><p>Maintains stable test suites</p></td>
<td><p>Continuously evolving benchmarks as capabilities advance</p></td>
</tr>
<tr class="row-even"><td><p><strong>Human Evaluation</strong></p></td>
<td><p>Mostly automated validation</p></td>
<td><p>May require significant human oversight</p></td>
</tr>
</tbody>
</table>
</section>
<section id="evals-design">
<h2><a class="toc-backref" href="#id46" role="doc-backlink"><span class="section-number">4.4. </span>Evals Design</a><a class="headerlink" href="#evals-design" title="Permalink to this heading">¶</a></h2>
<p>First, it’s important to make a distinction between evaluating an LLM versus evaluating an LLM-based application. While the latter offers foundation capabilities and are typically general-purpose, the former is more specific and tailored to a particular use case. Here, we define an LLM-based application as a system that uses one or more LLMs to perform a specific task. More specifically, an LLM-based application is the combination of one or more LLM models, their associated prompts and parameters to solve a particular business problem.</p>
<p>That differentiation is important because it changes the scope of evaluation. LLMs are usually evaluated based on their capabilities, which include things like language understanding, reasoning and knowledge. LLM-based applications, instead, should be evaluated based on their end-to-end functionality, performance, and how well they meet business requirements. That distinction has key implications for the design of evaluation systems:</p>
<ul class="simple">
<li><p>The same LLM can yield different results in different applications</p></li>
<li><p>Evaluation must align with business objectives</p></li>
<li><p>A great LLM doesn’t guarantee a great application!</p></li>
</ul>
<p>Examples of key requirements for validation are listed in <a class="reference internal" href="#validation-requirements"><span class="std std-numref">Table 4.2</span></a>.</p>
<table class="docutils align-default" id="validation-requirements">
<caption><span class="caption-number">Table 4.2 </span><span class="caption-text">LLM Application Testing Requirements Matrix</span><a class="headerlink" href="#validation-requirements" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Category</p></th>
<th class="head"><p>Requirement</p></th>
<th class="head"><p>What to Test</p></th>
<th class="head"><p>Why It’s Important</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Safety</p></td>
<td><p>Misinformation Prevention</p></td>
<td><p>- Accuracy of factual statements against verified databases<br>- Consistency of responses across similar queries<br>- Rate of fabricated details or hallucinations<br>- Citation and source accuracy<br>- Response behavior with uncertainty<br>- Temporal consistency<br>- Scientific accuracy</p></td>
<td><p>- Prevents real-world harm from false information<br>- Maintains user trust<br>- Reduces legal and reputational risks<br>- Ensures reliable decision-making support<br>- Protects against information manipulation</p></td>
</tr>
<tr class="row-odd"><td><p>Safety</p></td>
<td><p>Unqualified Advice</p></td>
<td><p>- Recognition of medical, legal, and financial queries<br>- Disclaimer consistency<br>- Professional referral mechanisms<br>- Boundary recognition<br>- Emergency situation handling<br>- Avoidance of specific recommendations</p></td>
<td><p>- Prevents harm from incorrect professional advice<br>- Reduces legal liability<br>- Protects vulnerable users<br>- Maintains professional standards<br>- Ensures appropriate expertise utilization</p></td>
</tr>
<tr class="row-even"><td><p>Safety</p></td>
<td><p>Bias Detection</p></td>
<td><p>- Gender, racial, and cultural bias<br>- Demographic representation<br>- Language inclusivity<br>- Stereotype avoidance<br>- Problem-solving fairness<br>- Cultural context awareness</p></td>
<td><p>- Prevents reinforcement of societal biases<br>- Ensures equal service quality<br>- Maintains social responsibility<br>- Protects brand reputation<br>- Supports diverse user bases</p></td>
</tr>
<tr class="row-odd"><td><p>Safety</p></td>
<td><p>Privacy Protection</p></td>
<td><p>- PII detection and handling<br>- Data anonymization<br>- Information leakage prevention<br>- Context carryover management<br>- Compliance with regulations<br>- Security protocols</p></td>
<td><p>- Protects user confidentiality<br>- Ensures regulatory compliance<br>- Maintains data security<br>- Prevents privacy breaches<br>- Safeguards sensitive information</p></td>
</tr>
<tr class="row-even"><td><p>Cognitive</p></td>
<td><p>Reasoning &amp; Logic</p></td>
<td><p>- Multi-step problem-solving<br>- Mathematical computation<br>- Logical fallacy detection<br>- Causal reasoning<br>- Abstract concept handling<br>- Edge case management</p></td>
<td><p>- Ensures reliable problem-solving<br>- Maintains computational accuracy<br>- Supports critical thinking<br>- Prevents logical errors<br>- Enables complex decision support</p></td>
</tr>
<tr class="row-odd"><td><p>Cognitive</p></td>
<td><p>Language Understanding</p></td>
<td><p>- Context maintenance<br>- Idiom comprehension<br>- Cultural reference accuracy<br>- Sarcasm detection<br>- Technical terminology<br>- Cross-lingual capability</p></td>
<td><p>- Ensures effective communication<br>- Prevents misunderstandings<br>- Enables sophisticated interactions<br>- Supports diverse language needs<br>- Maintains conversation quality</p></td>
</tr>
<tr class="row-even"><td><p>Technical</p></td>
<td><p>Code Generation</p></td>
<td><p>- Syntax accuracy<br>- Security vulnerability scanning<br>- Performance optimization<br>- Documentation quality<br>- Error handling<br>- Cross-platform compatibility</p></td>
<td><p>- Ensures code reliability<br>- Prevents security issues<br>- Maintains system stability<br>- Supports development efficiency<br>- Reduces technical debt</p></td>
</tr>
<tr class="row-odd"><td><p>Technical</p></td>
<td><p>System Integration</p></td>
<td><p>- API handling<br>- Rate limit compliance<br>- Error management<br>- Response time<br>- Resource utilization<br>- Scalability testing</p></td>
<td><p>- Ensures system reliability<br>- Maintains performance<br>- Enables scaling<br>- Prevents system failures<br>- Supports integration stability</p></td>
</tr>
<tr class="row-even"><td><p>Meta-Cognitive</p></td>
<td><p>Self-Awareness</p></td>
<td><p>- Knowledge limitation recognition<br>- Uncertainty communication<br>- Correction capabilities<br>- Feedback integration<br>- Edge case recognition<br>- Error acknowledgment</p></td>
<td><p>- Builds user trust<br>- Prevents overconfidence<br>- Enables appropriate use<br>- Supports improvement<br>- Maintains reliability</p></td>
</tr>
<tr class="row-odd"><td><p>Meta-Cognitive</p></td>
<td><p>Communication Quality</p></td>
<td><p>- Message clarity<br>- Audience appropriateness<br>- Information density<br>- Explanation quality<br>- Summary accuracy<br>- Technical communication</p></td>
<td><p>- Ensures understanding<br>- Maintains engagement<br>- Enables knowledge transfer<br>- Builds user satisfaction<br>- Supports effective interaction</p></td>
</tr>
<tr class="row-even"><td><p>Ethical</p></td>
<td><p>Harmful Content</p></td>
<td><p>- Harmful request recognition<br>- Response appropriateness<br>- Content filtering<br>- Emergency handling<br>- User safety protocols<br>- Incident reporting</p></td>
<td><p>- Protects user safety<br>- Prevents misuse<br>- Maintains ethical standards<br>- Reduces liability<br>- Ensures responsible use</p></td>
</tr>
<tr class="row-odd"><td><p>Ethical</p></td>
<td><p>Decision-Making</p></td>
<td><p>- Moral consistency<br>- Value alignment<br>- Decision fairness<br>- Transparency<br>- Impact assessment<br>- Stakeholder consideration</p></td>
<td><p>- Ensures ethical deployment<br>- Maintains standards<br>- Builds trust<br>- Supports values<br>- Prevents harmful impacts</p></td>
</tr>
<tr class="row-even"><td><p>Environmental</p></td>
<td><p>CO2 Emission</p></td>
<td><p>- Energy consumption per request<br>- Model size and complexity impact<br>- Server location and energy sources<br>- Request caching efficiency<br>- Batch processing optimization<br>- Hardware utilization rates<br>- Inference optimization strategies</p></td>
<td><p>- Reduces environmental impact<br>- Supports sustainability goals<br>- Optimizes operational costs<br>- Meets environmental regulations<br>- Demonstrates corporate responsibility</p></td>
</tr>
</tbody>
</table>
<section id="conceptual-overview">
<h3><a class="toc-backref" href="#id47" role="doc-backlink"><span class="section-number">4.4.1. </span>Conceptual Overview</a><a class="headerlink" href="#conceptual-overview" title="Permalink to this heading">¶</a></h3>
<p><a class="reference internal" href="#conceptual"><span class="std std-numref">Fig. 4.2</span></a> demonstrates a conceptual design of key components of LLM Application evaluation.</p>
<figure class="align-center" id="conceptual">
<a class="reference internal image-reference" href="../_images/conceptual.png"><img alt="Conceptual Overview" src="../_images/conceptual.png" style="width: 992.8000000000001px; height: 424.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.2 </span><span class="caption-text">Conceptual overview of LLM-based application evaluation.</span><a class="headerlink" href="#conceptual" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>We observe three key components:</p>
<p><strong>1. Examples (Input Dataset):</strong></p>
<ul class="simple">
<li><p>Input:  Query to LLM App, e.g. user message, input file, image, audio, etc.</p></li>
<li><p>Output: A reference expected outcome from the LLM application. Provide ground truth for comparison (<em>Optional</em>).</p></li>
<li><p>Purpose: Provides standardized test cases for evaluation.</p></li>
</ul>
<p><strong>2. LLM Application (Processing Layer):</strong></p>
<ul class="simple">
<li><p>Input: Test cases input from Examples</p></li>
<li><p>Output: Generated responses/results</p></li>
<li><p>Purpose:</p>
<ul>
<li><p>Represents different LLM implementations/vendors solving a specific task</p></li>
<li><p>Could be different models (GPT-4, Claude, PaLM, etc.)</p></li>
<li><p>Could be different configurations of same model</p></li>
<li><p>Could be different prompting strategies</p></li>
</ul>
</li>
</ul>
<p><strong>3. Evaluator (Assessment Layer):</strong></p>
<ul class="simple">
<li><p>Input:</p>
<ul>
<li><p>Outputs from LLM application</p></li>
<li><p>Reference data from Examples (<em>Optional</em>)</p></li>
</ul>
</li>
<li><p>Output: Individual scores for target LLM application</p></li>
<li><p>Purpose:</p>
<ul>
<li><p>Measures LLM Application performance across defined metrics</p></li>
<li><p>Applies standardized scoring criteria</p></li>
</ul>
</li>
</ul>
<p>Note that Examples must provide input data to the LLM Application for further evaluation. However, ground truth data is optional. We will return to this in more detail below, where we will see that ground truth data is not always available or practical. Additionally, there are approaches where one can evaluate LLM Applications without ground truth data.</p>
<p>A more general conceptual design is shown in <a class="reference internal" href="#conceptual-multi"><span class="std std-numref">Fig. 4.3</span></a>, where multiple LLM Applications are evaluated. This design allows for a more comprehensive evaluation of different configurations of LLM-based applications, e.g.:</p>
<ul class="simple">
<li><p>Fixing all application parameters and evaluating different LLM models with their default configurations</p></li>
<li><p>Fixing all parameters of an LLM model and evaluating different prompting strategies</p></li>
</ul>
<figure class="align-center" id="conceptual-multi">
<a class="reference internal image-reference" href="../_images/conceptual-multi.svg"><img alt="Conceptual Overview" height="588" src="../_images/conceptual-multi.svg" width="925" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.3 </span><span class="caption-text">Conceptual overview of Multiple LLM-based applications evaluation.</span><a class="headerlink" href="#conceptual-multi" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>In this evaluation framework, the same inputs are provided to all LLM applications, ensuring that responses are evaluated consistently. Performance is quantified objectively for each LLM Application, and results are ranked for easy comparison. This design leads to two additional components:
<strong>1. Scores (Metrics Layer):</strong></p>
<ul class="simple">
<li><p>Input: Evaluation results from Evaluator</p></li>
<li><p>Output: Quantified performance metrics</p></li>
<li><p>Purpose:</p>
<ul>
<li><p>Represents performance in numerical form</p></li>
<li><p>Enables quantitative comparison among LLM applications</p></li>
<li><p>May include multiple metrics per LLM application</p></li>
</ul>
</li>
</ul>
<p><strong>2. Leaderboard (Ranking Layer):</strong></p>
<ul class="simple">
<li><p>Input: Scores per LLM application</p></li>
<li><p>Output: Ordered ranking of LLMs with scores</p></li>
<li><p>Purpose:</p>
<ul>
<li><p>Aggregates and ranks performances across LLM applications</p></li>
</ul>
</li>
</ul>
</section>
<section id="design-considerations">
<h3><a class="toc-backref" href="#id48" role="doc-backlink"><span class="section-number">4.4.2. </span>Design Considerations</a><a class="headerlink" href="#design-considerations" title="Permalink to this heading">¶</a></h3>
<p>The design of an LLM application evaluation system depends heavily on the specific use case and business requirements. Here we list important questions for planning an LLM application evaluation system pertaining to each of the key components previously discussed:</p>
<p><strong>1. Examples (Input Dataset):</strong></p>
<ul class="simple">
<li><p>What types of examples should be included in the test set?</p>
<ul>
<li><p>Does it cover all important use cases?</p></li>
<li><p>Are edge cases represented?</p></li>
<li><p>Is there a good balance of simple and complex examples?</p></li>
</ul>
</li>
<li><p>How do we ensure data quality?</p>
<ul>
<li><p>Are the examples representative of real-world scenarios?</p></li>
<li><p>Is there any bias in the test set?</p></li>
</ul>
</li>
<li><p>Should we have separate test sets for different business requirements?</p></li>
<li><p>Do we need human-validated ground truth for all examples?</p></li>
<li><p>Can we use synthetic data to augment the test set?</p></li>
<li><p>How can business updates be reflected in the dataset post-launch?</p></li>
</ul>
<p><strong>2. LLM Applications:</strong></p>
<ul class="simple">
<li><p>What aspects of each LLM app should be standardized for fair comparison?</p>
<ul>
<li><p>Prompt templates</p></li>
<li><p>Context length</p></li>
<li><p>Temperature and other parameters</p></li>
<li><p>Rate limiting and timeout handling</p></li>
</ul>
</li>
<li><p>What specific configurations impact business requirements?</p>
<ul>
<li><p>Which LLM application variations should be tested to maximize what we learn?</p></li>
<li><p>Which LLM capabilities provide the most value for the business and how can we measure that?</p></li>
</ul>
</li>
</ul>
<p><strong>3. Evaluator Design:</strong></p>
<ul class="simple">
<li><p>How do we define success for different types of tasks?</p>
<ul>
<li><p>Task-specific evaluation criteria</p></li>
<li><p>Objective metrics vs subjective assessment</p></li>
</ul>
</li>
<li><p>Should evaluation be automated or involve human review?</p>
<ul>
<li><p>Balance between automation and human judgment</p></li>
<li><p>Inter-rater reliability for human evaluation</p></li>
<li><p>Cost and scalability considerations</p></li>
</ul>
</li>
</ul>
<p><strong>4. Scoring System:</strong></p>
<ul class="simple">
<li><p>How should different metrics be weighted?</p>
<ul>
<li><p>Relative importance of different factors</p></li>
<li><p>Task-specific prioritization</p></li>
<li><p>Business requirements alignment</p></li>
</ul>
</li>
<li><p>Should scores be normalized or absolute?</p></li>
<li><p>How to handle failed responses?</p></li>
<li><p>Should we consider confidence scores from the LLMs?</p></li>
</ul>
<p><strong>5. Leaderboard/Ranking:</strong></p>
<ul class="simple">
<li><p>How often should rankings be updated?</p></li>
<li><p>Should ranking include confidence intervals?</p></li>
<li><p>How to handle ties or very close scores?</p></li>
<li><p>Should we maintain separate rankings for different:</p>
<ul>
<li><p>Business requirements</p></li>
<li><p>Cost tiers</p></li>
<li><p>LLM Models</p></li>
</ul>
</li>
</ul>
<p>Holistically, your evaluation design should be built with scalability in mind to handle growing evaluation needs as the combination of (Example X LLM Applications X Evaluators X Scores X Leaderboards) may grow very fast, particularly for an organization that promotes rapid experimentation and iterative development (good properties!). Finally, one should keep in mind that the evaluation system itself requires validation to confirm its accuracy and reliability vis-a-vis business requirements (evaluating evaluators will be later discussed in this Chapter).</p>
</section>
</section>
<section id="metrics">
<h2><a class="toc-backref" href="#id49" role="doc-backlink"><span class="section-number">4.5. </span>Metrics</a><a class="headerlink" href="#metrics" title="Permalink to this heading">¶</a></h2>
<p>The choice of metric depends on the specific task and desired evaluation criteria. However, one can categorize metrics into two broad categories: <strong>intrinsic</strong> and <strong>extrinsic</strong>:</p>
<ul class="simple">
<li><p><strong>Intrinsic metrics</strong> focus on the model’s performance on its primary training objective, which is typically to predict the next token in a sequence.  Perplexity is a common intrinsic metric that measures how well the model predicts a given sample of text.</p></li>
<li><p><strong>Extrinsic metrics</strong> assess the model’s performance on various downstream tasks, which can range from question answering to code generation.  These metrics are not directly tied to the training objective, but they provide valuable insights into the model’s ability to generalize to real-world applications.</p></li>
</ul>
<p>Here, we are particularly interested in extrinsic metrics, since we are evaluating LLM-based applications.</p>
<p>Another way to think about metrics is in terms of the type of the task we evaluate:</p>
<ol class="arabic simple">
<li><p><strong>Discriminative Task</strong>:</p>
<ul class="simple">
<li><p>Involves distinguishing or classifying between existing data points.</p></li>
<li><p>Examples: Sentiment analysis, classification, or identifying whether a statement is true or false.</p></li>
</ul>
</li>
<li><p><strong>Generative Task</strong>:</p>
<ul class="simple">
<li><p>Involves creating or producing new data or outputs.</p></li>
<li><p>Examples: Text generation, image synthesis, or summarization.</p></li>
</ul>
</li>
</ol>
<p>For discriminative LLM-based applications may produce log-probabilities or discrete predictions, traditional machine learning metrics like accuracy, precision, recall, and F1 score can be applied. However, generative tasks may output text or images which require different evaluation approaches.</p>
<p>For generative tasks, a range of specialized metrics should be considered. These include match-based metrics such as exact match and prefix match, as well as metrics designed specifically for tasks like summarization and translation, including ROUGE, BLEU, and character n-gram comparisons. The selection of appropriate metrics should align with the specific requirements and characteristics of the task being evaluated. A detailed discussion of metric selection guidelines will be provided in a subsequent section.</p>
<p>In <a class="reference internal" href="#key-metrics"><span class="std std-numref">Table 4.3</span></a> we provide a short list of widely used extrinsic metrics that can be used to evaluate generative tasks of LLM-based applications, along with their definitions, use cases, and limitations.</p>
<table class="docutils align-default" id="key-metrics">
<caption><span class="caption-number">Table 4.3 </span><span class="caption-text">Key Metrics for Evaluating Generative Tasks</span><a class="headerlink" href="#key-metrics" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>Definition</p></th>
<th class="head"><p>Use Case</p></th>
<th class="head"><p>Limitations</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>BLEU (Bilingual Evaluation Understudy)</strong></p></td>
<td><p>Measures overlap of n-grams between generated text and reference text</p></td>
<td><p>Machine translation and text summarization</p></td>
<td><p>- Favors short outputs due to brevity penalty<br>- Insensitive to semantic meaning<br>- Requires high-quality reference texts</p></td>
</tr>
<tr class="row-odd"><td><p><strong>ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</strong></p></td>
<td><p>Measures overlap between n-grams, words, or sentences of generated text and references, focusing on recall</p></td>
<td><p>Text summarization tasks</p></td>
<td><p>- Biases toward long outputs<br>- Ignores semantic equivalence<br>- Heavily influenced by reference quality</p></td>
</tr>
<tr class="row-even"><td><p><strong>METEOR (Metric for Evaluation of Translation with Explicit ORdering)</strong></p></td>
<td><p>Considers synonyms, stemming, and paraphrases alongside n-gram overlap</p></td>
<td><p>Machine translation, where semantic equivalence matters</p></td>
<td><p>- Computationally expensive<br>- Subjective design of synonym/stemming databases</p></td>
</tr>
<tr class="row-odd"><td><p><strong>CIDEr (Consensus-based Image Description Evaluation)</strong></p></td>
<td><p>Measures n-gram overlap weighted by TF-IDF, tailored for image captioning</p></td>
<td><p>Image caption generation</p></td>
<td><p>- Limited applicability outside captioning<br>- Heavily reliant on corpus statistics</p></td>
</tr>
<tr class="row-even"><td><p><strong>TER (Translation Edit Rate)</strong></p></td>
<td><p>Computes number of edits needed to convert hypothesis into reference text</p></td>
<td><p>Translation quality evaluation</p></td>
<td><p>- Doesn’t consider semantic correctness<br>- Penalizes valid paraphrasing</p></td>
</tr>
<tr class="row-odd"><td><p><strong>BERTScore</strong></p></td>
<td><p>Uses contextual embeddings from pre-trained BERT to calculate token similarity</p></td>
<td><p>Tasks requiring semantic equivalence</p></td>
<td><p>- High computational cost<br>- Performance varies with model used</p></td>
</tr>
<tr class="row-even"><td><p><strong>SPICE (Semantic Propositional Image Caption Evaluation)</strong></p></td>
<td><p>Focuses on scene graphs in image captions to evaluate semantic content</p></td>
<td><p>Image captioning with emphasis on semantic accuracy</p></td>
<td><p>- Designed only for image captions<br>- Less effective in purely textual tasks</p></td>
</tr>
</tbody>
</table>
<p>A common use case for metrics like BLEU and ROUGE is to evaluate the quality of generated summaries against reference summaries.
As an example, we will demonstrate how to evaluate the quality of SEC Filings summaries against reference summaries (e.g. analyst-prepared highlights).</p>
<p>We will model our simple metrics-based evaluator with the following components:</p>
<ul class="simple">
<li><p>Input: Generated summary and reference summary</p></li>
<li><p>Output: Dictionary with scores for BLEU, ROUGE_1, and ROUGE_2</p></li>
<li><p>Purpose: Evaluate our LLM-based application - SEC filing summary generator</p></li>
</ul>
<p>A <em>Reference Summary</em> represents the “ideal” summary. It could be prepared by humanas, e.g. expert analysts, or machine-generated.</p>
<p>In our example, we are particularly interested in evaluating the quality of summaries generated by different (smaller and cheaper) LLM models compared to a <em>benchmark model</em> (larger and more expensive). We will use the following setup:</p>
<ul class="simple">
<li><p>Benchmark model: <code class="docutils literal notranslate"><span class="pre">gpt-4o</span></code></p></li>
<li><p>Test models: <code class="docutils literal notranslate"><span class="pre">gpt-4o-mini</span></code>, <code class="docutils literal notranslate"><span class="pre">gpt-4-turbo</span></code>, <code class="docutils literal notranslate"><span class="pre">gpt-3.5-turbo</span></code></p></li>
</ul>
<p>First, we define <code class="docutils literal notranslate"><span class="pre">evaluate_summaries</span></code>, a function that calculates BLEU and ROUGE scores to assess text generation quality. It takes a generated summary and reference summary as input, processes them and returns a dictionary with three scores: BLEU (n-gram overlap), ROUGE_1 (unigram comparison), and ROUGE_2 (bigram comparison). This enables quantitative comparison of generated summaries against reference texts. We use HuggingFaces’ <code class="docutils literal notranslate"><span class="pre">evaluate</span></code> library to load the metrics.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>evaluate<span class="w"> </span>absl-py<span class="w"> </span>rouge_score
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">evaluate</span>
<span class="k">def</span> <span class="nf">evaluate_summaries</span><span class="p">(</span><span class="n">generated_summary</span><span class="p">,</span> <span class="n">reference_summary</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluate generated summaries against reference summaries using multiple metrics.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        generated_summary (str): The summary generated by the model</span>
<span class="sd">        reference_summary (str): The reference/ground truth summary</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">        dict: Dictionary containing scores for different metrics</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Initialize metrics</span>
    <span class="n">bleu</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;google_bleu&quot;</span><span class="p">)</span>
    <span class="n">rouge</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;rouge&quot;</span><span class="p">)</span>
    
    <span class="c1"># Format inputs for BLEU (expects list of str for predictions and list of list of str for references)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span><span class="n">generated_summary</span><span class="p">]</span>
    <span class="n">references</span> <span class="o">=</span> <span class="p">[</span><span class="n">reference_summary</span><span class="p">]</span>
    
    <span class="c1"># Compute BLEU score</span>
    <span class="n">bleu_score</span> <span class="o">=</span> <span class="n">bleu</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="p">[</span><span class="n">references</span><span class="p">])</span>
    
    <span class="c1"># Compute ROUGE scores</span>
    <span class="n">rouge_score</span> <span class="o">=</span> <span class="n">rouge</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">references</span><span class="p">)</span>
    
    <span class="c1"># Compute Character metric    </span>
    <span class="c1"># Combine all scores into a single dictionary</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;bleu&#39;</span><span class="p">:</span> <span class="n">bleu_score</span><span class="p">[</span><span class="s2">&quot;google_bleu&quot;</span><span class="p">],</span>
        <span class="s1">&#39;rouge1&#39;</span><span class="p">:</span> <span class="n">rouge_score</span><span class="p">[</span><span class="s1">&#39;rouge1&#39;</span><span class="p">],</span>
        <span class="s1">&#39;rouge2&#39;</span><span class="p">:</span> <span class="n">rouge_score</span><span class="p">[</span><span class="s1">&#39;rouge2&#39;</span><span class="p">]</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="n">scores</span>
</pre></div>
</div>
</div>
</div>
<p>For instance, <code class="docutils literal notranslate"><span class="pre">evaluate_summaries</span></code> can be used to compare two arbitrary sentences and returns a dictionary with our chosen metrics:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sentence1</span> <span class="o">=</span> <span class="s2">&quot;the cat sat on the mat&quot;</span>
<span class="n">sentence2</span> <span class="o">=</span> <span class="s2">&quot;the cat ate the mat&quot;</span>
<span class="n">evaluate_summaries</span><span class="p">(</span><span class="n">sentence1</span><span class="p">,</span> <span class="n">sentence2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;bleu&#39;: 0.3333333333333333,
 &#39;rouge1&#39;: 0.7272727272727272,
 &#39;rouge2&#39;: 0.4444444444444445}
</pre></div>
</div>
</div>
</div>
<p>Next, we define <code class="docutils literal notranslate"><span class="pre">generate_summary</span></code>, our simple LLM-based SEC filing summirizer application using OpenAI’s API. It takes an arbitrary <code class="docutils literal notranslate"><span class="pre">model</span></code>, and an <code class="docutils literal notranslate"><span class="pre">input</span></code> text and returns the corresponding LLM’s response with a summary.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">generate_summary</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate a summary of input using a given model</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">TASK</span> <span class="o">=</span> <span class="s2">&quot;Generate a 1-liner summary of the following excerpt from an SEC filing.&quot;</span>

    <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    ROLE: You are an expert analyst tasked with summarizing SEC filings.</span>
<span class="s2">    TASK: </span><span class="si">{</span><span class="n">TASK</span><span class="si">}</span>
<span class="s2">    &quot;&quot;&quot;</span>
    
    <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">},</span>
                 <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="nb">input</span><span class="p">}]</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
</pre></div>
</div>
</div>
</div>
<p>Now, we define a function <code class="docutils literal notranslate"><span class="pre">evaluate_summary_models</span></code> - our benchmark evaluator - that compares text summaries generated by different language models against a benchmark model. Here’s what it does:</p>
<ol class="arabic simple">
<li><p>Takes a benchmark model, list of test models, prompt, and input text</p></li>
<li><p>Generates a reference summary using the benchmark model and our <code class="docutils literal notranslate"><span class="pre">generate_summary</span></code> function</p></li>
<li><p>Generates summaries from all test models using <code class="docutils literal notranslate"><span class="pre">generate_summary</span></code> function</p></li>
<li><p>Evaluates each test model’s summary against the benchmark using <code class="docutils literal notranslate"><span class="pre">evaluate_summaries</span></code></p></li>
<li><p>Returns evaluation results and the generated summaries</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">evaluate_summary_models</span><span class="p">(</span><span class="n">model_benchmark</span><span class="p">,</span> <span class="n">models_test</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluate summaries generated by multiple models</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">benchmark_summary</span> <span class="o">=</span> <span class="n">generate_summary</span><span class="p">(</span><span class="n">model_benchmark</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>

    <span class="c1"># Generate summaries for all test models using list comprehension</span>
    <span class="n">model_summaries</span> <span class="o">=</span> <span class="p">[</span><span class="n">generate_summary</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models_test</span><span class="p">]</span>
    
    <span class="c1"># Evaluate each model&#39;s summary against the benchmark</span>
    <span class="n">evaluation_results</span> <span class="o">=</span> <span class="p">[</span><span class="n">evaluate_summaries</span><span class="p">(</span><span class="n">summary</span><span class="p">,</span> <span class="n">benchmark_summary</span><span class="p">)</span> <span class="k">for</span> <span class="n">summary</span> <span class="ow">in</span> <span class="n">model_summaries</span><span class="p">]</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">evaluation_results</span><span class="p">,</span> <span class="n">model_summaries</span><span class="p">,</span> <span class="n">benchmark_summary</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Now, we are ready to run our benchmark evaluation. We define a benchmark model and a list of test models and then evaluate each test model’s summary against the benchmark. We also print the generated summaries for each model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_benchmark</span> <span class="o">=</span> <span class="s2">&quot;gpt-4o&quot;</span>
<span class="n">models_test</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">,</span> <span class="s2">&quot;gpt-4-turbo&quot;</span><span class="p">,</span> <span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">evals</span><span class="p">,</span> <span class="n">model_summaries</span><span class="p">,</span> <span class="n">benchmark_summary</span> <span class="o">=</span> <span class="n">evaluate_summary_models</span><span class="p">(</span><span class="n">model_benchmark</span><span class="p">,</span> <span class="n">models_test</span><span class="p">,</span> <span class="n">sec_filing</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">benchmark_summary</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&quot;Apple Inc.&#39;s 10-K filing for the fiscal year ending September 28, 2024, outlines its operational and financial condition, detailing the company&#39;s diverse product lines, market activities, and compliance with SEC requirements.&quot;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print each model name and its summary</span>
<span class="k">for</span> <span class="n">model</span><span class="p">,</span> <span class="n">summary</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">models_test</span><span class="p">,</span> <span class="n">model_summaries</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s2">: </span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">summary</span><span class="si">}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">---------------&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>gpt-4o-mini: 
 Apple Inc. filed its Annual Report on Form 10-K for the fiscal year ending September 28, 2024, detailing its business operations, risks, and financial condition. 
---------------
gpt-4-turbo: 
 Apple Inc.&#39;s Form 10-K for the fiscal year ended September 28, 2024, details its annual report as a well-known seasoned issuer, confirming compliance with SEC regulations and reporting on stock performances, securities, and corporate governance, while also including forward-looking statements subject to various risks. 
---------------
gpt-3.5-turbo: 
 Apple Inc. filed its Form 10-K with the SEC, revealing financial information for the fiscal year ended September 28, 2024, including details on its products and market performance. 
---------------
</pre></div>
</div>
</div>
</div>
<p>The benchmark summary from <code class="docutils literal notranslate"><span class="pre">gpt-4o</span></code> provides a balanced overview of the analyzed excerpt from Apple’s 10-K filing, focusing on operational status, financial condition, product lines, and regulatory compliance.</p>
<p>When comparing our test models against the benchmark, we observe that:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">gpt-4o-mini</span></code> provides a concise yet comprehensive summary that closely aligns with the benchmark’s core message. While it omits product lines, it effectively captures the essential elements of the filing including business operations, risks, and financial condition. Its brevity and focus look (subjectively) similar to our benchmark model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpt-4-turbo</span></code> performs adequately but tends toward verbosity. While it includes relevant information about SEC compliance, it introduces peripheral details about seasoned issuer status and forward-looking statements. The additional complexity makes the summary less focused than gpt-4o-mini’s version.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpt-3.5-turbo</span></code> looks quite different from the benchmark. Its summary, while factually correct, is overly simplified and misses key aspects of the filing. The model captures basic financial information but fails to convey the breadth of operational and compliance details present in the benchmark summary.</p></li>
</ul>
<p>Of course, the above evaluation is only based on a single example and is heavily subjective. It’s a “vibe check” on our evaluation results. Now, for an objective analysis, we can look at the quantitative metrics we have chosen and use the <code class="docutils literal notranslate"><span class="pre">visualize_prompt_comparison</span></code> function we write below to visualize the performance of our test models across our predefined quantitative metrics.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>matplotlib
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">visualize_prompt_comparison</span><span class="p">(</span><span class="n">evaluation_results</span><span class="p">,</span> <span class="n">model_names</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a radar plot comparing different prompt variations</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        evaluation_results (list): List of dictionaries containing evaluation metrics</span>
<span class="sd">        model_names (list): List of names for each prompt variation</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">evaluate.visualization</span> <span class="kn">import</span> <span class="n">radar_plot</span>
    
    <span class="c1"># Format data for visualization</span>
    <span class="n">plot</span> <span class="o">=</span> <span class="n">radar_plot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">evaluation_results</span><span class="p">,</span> <span class="n">model_names</span><span class="o">=</span><span class="n">model_names</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">plot</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create and display visualization</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">visualize_prompt_comparison</span><span class="p">(</span><span class="n">evals</span><span class="p">,</span> <span class="n">models_test</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_1652501/940173201.py:3: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  plot.show()
</pre></div>
</div>
<img alt="../_images/08a72483c61f624c6ca08db7d58620028acadc7df987d17f40ae7c6e04fa94d2.png" src="../_images/08a72483c61f624c6ca08db7d58620028acadc7df987d17f40ae7c6e04fa94d2.png" />
</div>
</div>
<p>Results demonstrate that tested models perform quite differently on our predefined metrics. The evaluation metrics puts <code class="docutils literal notranslate"><span class="pre">gpt-4o-mini</span></code> as the closest aligned to the benchmark, followed by gpt-4-turbo, and gpt-3.5-turbo showing the largest deviation. This suggests that <code class="docutils literal notranslate"><span class="pre">gpt-4o-mini</span></code> is the best model for this task at least on the metrics we have chosen and for the set of models we have tested.</p>
<p>While evaluating language model outputs inherently involves subjective judgment, establishing a high-quality benchmark model and using quantifiable metrics provide a more objective framework for comparing model performance. This approach transforms an otherwise qualitative assessment into a measurable, data-driven evaluation process.</p>
<p>These metrics provide quantifiable measures of performance, however limitations should be mentioned:</p>
<ul class="simple">
<li><p><strong>Task-specific nature</strong>:  Chosen set of metrics might not fully capture the nuances of complex generative-based tasks, especially those involving subjective human judgment.</p></li>
<li><p><strong>Sensitivity to data distribution</strong>: Performance on these metrics can be influenced by the specific dataset used for evaluation, which might not represent real-world data distribution.</p></li>
<li><p><strong>Inability to assess reasoning or factual accuracy</strong>: These metrics primarily focus on surface-level matching and might not reveal the underlying reasoning process of the LLM or its ability to generate factually correct information.</p></li>
</ul>
<p>In conclusion, selecting an appropriate extrinsic metrics set depends on the specific task, underlying business requirements and desired evaluation granularity.  Understanding the limitations of these metrics can provide a more comprehensive assessment of LLM performance in real-world applications.</p>
<p>To address these limitations, alternative approaches like <strong>human-based evaluation</strong> and <strong>model-based evaluation</strong> are often used, which will be discussed in the following sections.</p>
</section>
<section id="evaluators">
<h2><a class="toc-backref" href="#id50" role="doc-backlink"><span class="section-number">4.6. </span>Evaluators</a><a class="headerlink" href="#evaluators" title="Permalink to this heading">¶</a></h2>
<section id="model-based-evaluation">
<h3><a class="toc-backref" href="#id51" role="doc-backlink"><span class="section-number">4.6.1. </span>Model-Based Evaluation</a><a class="headerlink" href="#model-based-evaluation" title="Permalink to this heading">¶</a></h3>
<p>Traditional metrics like BLEU or ROUGE often fall short in capturing the nuanced, contextual, and creative outputs of LLMs. As an alternative we can consider a “Model-based evaluation” approach. A common approach is to use an LLM as a judge. This is an approach that leverages language models themselves to assess the quality of outputs from other language models. This method involves using a model (often a more capable one) to act as an automated judge, evaluating aspects like accuracy, coherence, and relevance of generated content. Unlike traditional metrics that rely on exact matching or statistical measures, model-based evaluation can capture nuanced aspects of language and provide more contextual assessment.</p>
<p>As discussed in the paper <span id="id3">[<a class="reference internal" href="#id25" title="Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Yuxuan Lai, Chongyang Tao, and Shuai Ma. Leveraging large language models for nlg evaluation: advances and challenges. 2024. URL: https://arxiv.org/abs/2401.07103, arXiv:2401.07103.">Li <em>et al.</em>, 2024</a>]</span>, LLM-based evaluation approaches generally fall into two main categories:</p>
<ol class="arabic simple">
<li><p><strong>Prompt-based evaluation</strong>: This involves using prompts to instruct existing LLMs to evaluate text quality without any fine-tuning. The evaluation can take several forms:</p>
<ul class="simple">
<li><p>Score-based: LLMs assign numerical scores to generated text</p></li>
<li><p>Probability-based: Using generation probability as a quality metric</p></li>
<li><p>Likert-style: Rating text quality on discrete scales</p></li>
<li><p>Pairwise comparison: Directly comparing two texts</p></li>
<li><p>Ensemble methods: Combining multiple LLM evaluators</p></li>
</ul>
</li>
<li><p><strong>Tuning-based evaluation</strong>: This involves fine-tuning open-source LLMs specifically for evaluation tasks. This can be more cost-effective than repeatedly using API calls and allows for domain adaptation.</p></li>
</ol>
<p>Once you have chosen your approach, a general LLM-as-a-Judge procedure involves the following steps (see <a class="reference internal" href="#llm-judge"><span class="std std-numref">Fig. 4.4</span></a>):</p>
<ol class="arabic simple">
<li><p><strong>Define Evaluation Criteria</strong>: Establish clear benchmarks, such as relevance, coherence, accuracy, and fluency.</p></li>
<li><p><strong>Prepare Prompts</strong>: Craft effective prompts to guide the LLM in evaluating content against the criteria.</p></li>
<li><p><strong>Define Reference Data</strong>: Establish a set of reference data that the judge model can use to evaluate the generated outputs. (<em>Optional</em>)</p></li>
<li><p><strong>Run Evaluations</strong>: Use the judge model to score outputs. Consider using a large and/or more capable model as a judge to provide more nuanced assessments.</p></li>
<li><p><strong>Aggregate and Analyze Results</strong>: Interpret scores to refine applications.</p></li>
</ol>
<figure class="align-center" id="llm-judge">
<a class="reference internal image-reference" href="../_images/llm_judge.svg"><img alt="Conceptual Overview" height="720" src="../_images/llm_judge.svg" width="861" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.4 </span><span class="caption-text">Conceptual overview of LLM-as-a-Judge evaluation.</span><a class="headerlink" href="#llm-judge" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Compared to traditional metrics, LLM-as-a-Judge evaluation offers a more sophisticated assessment framework by leveraging natural language criteria. While metrics focus on statistical measures, judge models excel at evaluating subjective qualities such as creativity, narrative flow, and contextual relevance - aspects that closely mirror human judgment. The judge model processes evaluation guidelines expressed in natural language, functioning similarly to a human reviewer interpreting assessment criteria. One notable consideration is that this approach requires careful prompt engineering to properly define and communicate the evaluation standards to the model.</p>
<p>Prompt Engineering can have a large impact on the quality of the evaluation <span id="id4">[<a class="reference internal" href="#id25" title="Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Yuxuan Lai, Chongyang Tao, and Shuai Ma. Leveraging large language models for nlg evaluation: advances and challenges. 2024. URL: https://arxiv.org/abs/2401.07103, arXiv:2401.07103.">Li <em>et al.</em>, 2024</a>]</span>. Hence, it’s worth noting key prompting best practices when designing LLM-as-a-judge evaluators <span id="id5">[<a class="reference internal" href="#id26" title="Hugging Face. Llm as a judge. https://huggingface.co/learn/cookbook/en/llm_judge, 2024. Accessed: 2024.">Face, 2024</a>]</span>:</p>
<ol class="arabic simple">
<li><p>Use discrete integer scales (e.g., 1-5) rather than continuous ranges</p></li>
<li><p>Provide clear rubrics that define what each score level means</p></li>
<li><p>Include reference answers when available to ground the evaluation</p></li>
<li><p>Break down complex judgments into specific evaluation criteria</p></li>
</ol>
<p>Additionally, the interpretability of the evaluation framework can be fostered by:</p>
<ol class="arabic simple">
<li><p>Requiring explanations and reasoning for scores to increase transparency</p></li>
<li><p>Having a hollistic evaluation by considering multiple dimensions such as coherence, relevance, and fluency</p></li>
</ol>
<p>Below we provide a sample implementation of an LLM-as-a-Judge evaluation system for our LLM application that generates SEC filing summaries. The code defines:</p>
<ol class="arabic simple">
<li><p>A <code class="docutils literal notranslate"><span class="pre">JudgeEvaluation</span></code> Pydantic model that enforces type validation for four key metrics:</p>
<ul class="simple">
<li><p>Expertise: Rating of analyst-level writing quality</p></li>
<li><p>Coherence: Score for logical organization</p></li>
<li><p>Fluency: Assessment of grammar and clarity</p></li>
<li><p>Similarity: Measure of alignment with reference text</p></li>
</ul>
</li>
<li><p>An <code class="docutils literal notranslate"><span class="pre">evaluate_with_llm()</span></code> function that:</p>
<ul class="simple">
<li><p>Takes a judge model, candidate summary, and reference summary as inputs</p></li>
<li><p>Constructs a detailed prompt instructing the LLM to act as an expert evaluator</p></li>
<li><p>Uses structured output parsing to return scores in a consistent format</p></li>
<li><p>Returns scores on a 1-10 scale for each evaluation criterion</p></li>
</ul>
</li>
</ol>
<p>The implementation demonstrates how to combine structured data validation with natural language evaluation to create a robust automated assessment system.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span>

<span class="k">class</span> <span class="nc">JudgeEvaluation</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">expertise</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">coherence</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">fluency</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">similarity</span><span class="p">:</span> <span class="nb">int</span>
<span class="k">def</span> <span class="nf">evaluate_with_llm</span><span class="p">(</span><span class="n">judge_model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">candidate_summary</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">reference_summary</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Use an LLM to evaluate a candidate summary against a reference summary.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        judge_model (str): Name of the model to use as the judge.</span>
<span class="sd">        candidate_summary (str): Generated summary to evaluate.</span>
<span class="sd">        reference_summary (str): Ground truth or benchmark summary.</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        dict: Dictionary containing evaluation scores for specified criteria.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    ROLE: You are an expert evaluator of SEC Filing summaries. Evaluate the following candidate summary against the reference summary on a scale of 1 to 10 for the following criteria:</span>
<span class="s2">    - Expertise: Does the summary look like it was written by an expert analyst?</span>
<span class="s2">    - Coherence: Is the candidate summary logically organized and easy to understand?</span>
<span class="s2">    - Fluency: Is the language of the candidate summary clear and grammatically correct?</span>
<span class="s2">    - Similarity: How similar is the candidate summary compared to the reference summary?</span>

<span class="s2">    Reference Summary:</span>
<span class="s2">    &quot;</span><span class="si">{</span><span class="n">reference_summary</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="s2">    Candidate Summary:</span>
<span class="s2">    &quot;</span><span class="si">{</span><span class="n">candidate_summary</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="s2">    Provide scores in this format:</span>
<span class="s2">    Expertise: X, Coherence: Y, Fluency: Z, Similarity: W</span>
<span class="s2">    &quot;&quot;&quot;</span>
    <span class="n">completion</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">judge_model</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}],</span>
        <span class="n">response_format</span><span class="o">=</span><span class="n">JudgeEvaluation</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">completion</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">parsed</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we define a <code class="docutils literal notranslate"><span class="pre">evaluate_summary_models</span></code> function that leverages our LLM-as-a-Judge function to compare summaries generated by different language models. Here’s how it works:</p>
<ul class="simple">
<li><p>First, it generates a benchmark summary using the specified benchmark model</p></li>
<li><p>Then, it generates summaries using each of the test models</p></li>
<li><p>Finally, it evaluates each test model’s summary against the benchmark using the judge model</p></li>
</ul>
<p>As a result, we get a list of evaluation results we can use to compare our candidate LLM models across our predefined metrics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">evaluate_summary_models</span><span class="p">(</span><span class="n">judge_model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">benchmark_model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">test_models</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">input_text</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluate summaries generated by multiple models using an LLM-as-a-Judge approach.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        judge_model (str): Name of the model to use as the judge.</span>
<span class="sd">        benchmark_model (str): Name of the benchmark model.</span>
<span class="sd">        test_models (list): List of model names to test.</span>
<span class="sd">        input_text (str): Input text for summarization.</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        tuple: Evaluation results, model summaries, benchmark summary.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">benchmark_summary</span> <span class="o">=</span> <span class="n">generate_summary</span><span class="p">(</span><span class="n">benchmark_model</span><span class="p">,</span> <span class="n">input_text</span><span class="p">)</span>
    <span class="n">model_summaries</span> <span class="o">=</span> <span class="p">[</span><span class="n">generate_summary</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_text</span><span class="p">)</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">test_models</span><span class="p">]</span>

    <span class="n">evaluation_results</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">evaluate_with_llm</span><span class="p">(</span><span class="n">judge_model</span><span class="p">,</span> <span class="n">summary</span><span class="p">,</span> <span class="n">benchmark_summary</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">summary</span> <span class="ow">in</span> <span class="n">model_summaries</span>
    <span class="p">]</span>

    <span class="k">return</span> <span class="n">evaluation_results</span><span class="p">,</span> <span class="n">model_summaries</span><span class="p">,</span> <span class="n">benchmark_summary</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example Usage</span>
<span class="n">model_benchmark</span> <span class="o">=</span> <span class="s2">&quot;gpt-4o&quot;</span>
<span class="n">models_test</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">,</span> <span class="s2">&quot;gpt-4-turbo&quot;</span><span class="p">,</span> <span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">]</span>
<span class="n">judge_model</span> <span class="o">=</span> <span class="s2">&quot;gpt-4o&quot;</span>

<span class="n">evals</span><span class="p">,</span> <span class="n">model_summaries</span><span class="p">,</span> <span class="n">benchmark_summary</span> <span class="o">=</span> <span class="n">evaluate_summary_models</span><span class="p">(</span>
    <span class="n">judge_model</span><span class="p">,</span> <span class="n">model_benchmark</span><span class="p">,</span> <span class="n">models_test</span><span class="p">,</span> <span class="n">sec_filing</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here, we can see the benchmark summary coming from our benchmark model <code class="docutils literal notranslate"><span class="pre">gpt-4o</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">benchmark_summary</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&quot;Apple Inc.&#39;s annual report for the fiscal year ending September 28, 2024, details its business operations, financial condition, and product lines, including iPhones, Macs, iPads, and wearables, and incorporates forward-looking statements regarding its future performance.&quot;
</pre></div>
</div>
</div>
</div>
<p>Next, we obtain the summaries and evaluation results generated by our test models, <code class="docutils literal notranslate"><span class="pre">gpt-4o-mini</span></code>, <code class="docutils literal notranslate"><span class="pre">gpt-4-turbo</span></code> and <code class="docutils literal notranslate"><span class="pre">gpt-3.5-turbo</span></code>, respectively.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_summaries</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;Apple Inc. filed its annual Form 10-K report for the fiscal year ended September 28, 2024, detailing its business operations, product lines, and financial performance.&#39;,
 &quot;This Form 10-K filing by Apple Inc. for the fiscal year ended September 28, 2024, is an annual report detailing the company&#39;s financial performance, including registered securities, compliance with SEC reporting standards, and contains sections on business operations, risk factors, financial data, and management analysis.&quot;,
 &#39;Apple Inc., a California-based technology company, reported an aggregate market value of approximately $2.6 trillion held by non-affiliates, with 15.1 billion shares of common stock outstanding as of October 18, 2024.&#39;]
</pre></div>
</div>
</div>
</div>
<p>As a result we get a list of objects of the Pydantics class we have defined <code class="docutils literal notranslate"><span class="pre">JudgeEvaluation</span></code> which contains the metrics of our evaluation (expertise, coherence, fluency and similarity).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">evals</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[JudgeEvaluation(expertise=7, coherence=8, fluency=8, similarity=7),
 JudgeEvaluation(expertise=7, coherence=7, fluency=8, similarity=5),
 JudgeEvaluation(expertise=4, coherence=5, fluency=7, similarity=2)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert evaluation objects to dictionaries</span>
<span class="n">evals_list</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;expertise&quot;</span><span class="p">:</span> <span class="nb">eval</span><span class="o">.</span><span class="n">expertise</span><span class="p">,</span>
        <span class="s2">&quot;coherence&quot;</span><span class="p">:</span> <span class="nb">eval</span><span class="o">.</span><span class="n">coherence</span><span class="p">,</span> 
        <span class="s2">&quot;fluency&quot;</span><span class="p">:</span> <span class="nb">eval</span><span class="o">.</span><span class="n">fluency</span><span class="p">,</span>
        <span class="s2">&quot;similarity&quot;</span><span class="p">:</span> <span class="nb">eval</span><span class="o">.</span><span class="n">similarity</span>
    <span class="p">}</span>
    <span class="k">for</span> <span class="nb">eval</span> <span class="ow">in</span> <span class="n">evals</span>
<span class="p">]</span>

<span class="c1"># Visualize results</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">visualize_prompt_comparison</span><span class="p">(</span><span class="n">evals_list</span><span class="p">,</span> <span class="n">models_test</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_1652501/1775618912.py:14: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  plot.show()
</pre></div>
</div>
<img alt="../_images/a49bcd9c801d9b420b188b13fe009f1b93a733780c10a27b655fc7cd475d16a9.png" src="../_images/a49bcd9c801d9b420b188b13fe009f1b93a733780c10a27b655fc7cd475d16a9.png" />
</div>
</div>
<p>Looking at the evaluation results across our test models (gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo), we can observe some interesting patterns:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">gpt-4o-mini</span></code> model performed quite well, achieving high scores across all metrics (expertise: 7, coherence: 8, fluency: 8, similarity: 7). This suggests it maintained good quality while being a smaller model variant of our benchmark model <code class="docutils literal notranslate"><span class="pre">gpt-4o</span></code>.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">gpt-4-turbo</span></code> model showed similar expertise and fluency (7 and 8 respectively) but slightly lower coherence (7) and notably lower similarity (5) compared to the benchmark. This could indicate some drift from the reference summary while maintaining general quality.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">gpt-3.5-turbo</span></code> model had the lowest scores overall (expertise: 4, coherence: 5, fluency: 7, similarity: 2), particularly struggling with expertise and similarity to the benchmark. While it maintained reasonable fluency, the significant drop in similarity score suggests substantial deviation from the reference summary.</p></li>
</ul>
<p>The visualization helps highlight these differences across models and evaluation dimensions. A clear performance gradient is visible from gpt-4o-mini to gpt-3.5-turbo, with the latter showing marked degradation in most metrics.</p>
<p>Leveraging LLMs for evaluation has several limitations <span id="id6">[<a class="reference internal" href="#id25" title="Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Yuxuan Lai, Chongyang Tao, and Shuai Ma. Leveraging large language models for nlg evaluation: advances and challenges. 2024. URL: https://arxiv.org/abs/2401.07103, arXiv:2401.07103.">Li <em>et al.</em>, 2024</a>]</span>. Firstly, computational overhead should not be neglected given the inherent cost of running additional model inferences iterations. LLM evaluators can also exhibit various biases, including order bias (preferring certain sequence positions), egocentric bias (favoring outputs from similar models), and length bias. Further, there may be a tight dependency on prompt quality - small prompt variations may lead to substantially different outcomes. It is important to also note challenges around domain-specific evaluation in fields such as medice, finance, law etc, where a general llm-as-a-judge approach may not be suitable.</p>
<p>The LLM-as-a-Judge strategy can serve as a scalable and nuanced solution to evaluate LLM-based applications. While it does not entirely a metrics-based or human-based aproach, it significantly augments evaluation workflows, especially in scenarios requiring evaluation of generative outputs. Future improvements could include integrating human oversight and refining LLMs for domain-specific evaluation tasks.</p>
</section>
<section id="human-based-evaluation">
<h3><a class="toc-backref" href="#id52" role="doc-backlink"><span class="section-number">4.6.2. </span>Human-Based Evaluation</a><a class="headerlink" href="#human-based-evaluation" title="Permalink to this heading">¶</a></h3>
<p>Human assessors can judge aspects like fluency, coherence, and factual accuracy, providing a more comprehensive evaluation. However, human evaluation can be subjective and resource-intensive.</p>
</section>
<section id="evaluating-evaluators">
<h3><a class="toc-backref" href="#id53" role="doc-backlink"><span class="section-number">4.6.3. </span>Evaluating Evaluators</a><a class="headerlink" href="#evaluating-evaluators" title="Permalink to this heading">¶</a></h3>
<p>We have discussed how LLMs can be used to evaluate LLM-based aplications. However, how can we evaluate the performance of LLMs that evaluate other LLMs? This is the question that meta evaluation aims to answer. Clearly, the discussion can become quite meta as we need to evaluate the performance of the evaluator to evaluate the performance of the evaluated model. However, one can make a case for two general options:</p>
<ol class="arabic simple">
<li><p>Use a gold-standard dataset that is used to evaluate the performance of LLM evaluators using a “metrics-based” approach.</p></li>
<li><p>Use a human evaluator to generate reference scores that can be used to evaluate the performance of the LLM evaluator (similar to the human-based evaluation we discussed earlier).</p></li>
</ol>
<p>As depicted in <a class="reference internal" href="#meta"><span class="std std-numref">Fig. 4.5</span></a>, the performance of the LLM evaluator can be evaluated by comparing its scores to either a gold-standard dataset or human reference scores. Higher correlation values indicate better performance of the LLM evaluator. For instance, if we were to evaluate the performance of a LLM-as-a-judge evaluator, in the task of evaluating multilingual capability of an LLM:</p>
<ol class="arabic simple">
<li><p>In a “metrics-based” approach, we would first need to define a set of metrics that capture the task of multilingual capability. For instance, we could use the BLEU metric to evaluate the quality of the generated LLM output against a golden dataset (e.g. machine translated text). We would then calculate the correlation between these scores against those generated by the LLM evaluator. The higher the correlation, the better the LLM evaluator.</p></li>
<li><p>In a “human-based” approach, we would need to recruit human evaluators that are experts in the target languanges we are evaluating. Expert humans would provide scores for a set of samples of the input LLM. We would then calculate the correlation between these scores against those generated by the LLM evaluator. The higher the correlation, the better the LLM evaluator.</p></li>
</ol>
<figure class="align-center" id="meta">
<a class="reference internal image-reference" href="../_images/meta.png"><img alt="Meta Evaluation Conceptual Overview" src="../_images/meta.png" style="width: 859.1999999999999px; height: 578.4px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.5 </span><span class="caption-text">Conceptual overview of LLMs Meta Evaluation.</span><a class="headerlink" href="#meta" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>An extension of the above approaches is to use humans to directly evaluate the LLM-judges themselves. A notable example of this is <a class="reference external" href="https://judgearena.com/">Judge Arena</a> <span id="id7">[<a class="reference internal" href="#id28" title="Judge Arena. Judge arena: evaluating llm outputs with llms. https://judgearena.com/, 2024. Accessed: 2024.">Arena, 2024</a>]</span>, which is a platform that allows users to vote on which AI model made the better evaluation. Under this approach, the performance of the LLM evaluator is given by the (blind) evaluation of humans who perform the voting on randomly generated pairs of LLM judges as depicted in <a class="reference internal" href="#meta2"><span class="std std-numref">Fig. 4.6</span></a>. Only after submitting a vote, users can see which models were actually doing the judging.</p>
<figure class="align-center" id="meta2">
<a class="reference internal image-reference" href="../_images/meta2.svg"><img alt="Human-in-the-loop meta evaluation Conceptual Overview" height="1026" src="../_images/meta2.svg" width="447" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.6 </span><span class="caption-text">Human-in-the-loop Meta Evaluation.</span><a class="headerlink" href="#meta2" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The LLM input and its prompt are displayed to the human evaluator and are customizable enabling task-specific meta evaluation. Further, the Judge Arena’s LLM Judge’s prompt is also editable by the user. Its default prompt is presented below:</p>
<blockquote>
<div><p>Does the model provide relevant and useful responses to the user’s needs or questions?</p>
<p><strong>Scoring Rubric:</strong></p>
<p>Score 1: The model’s responses are irrelevant or unhelpful to the user’s needs or queries.</p>
<p>Score 2: The model sometimes provides helpful information, but often fails to address the user’s actual needs or questions.</p>
<p>Score 3: The model generally provides helpful responses that address the user’s needs, though it may occasionally miss the mark.</p>
<p>Score 4: The model regularly provides helpful responses that are well-aligned with the user’s inquiries, with only rare inaccuracies.</p>
<p>Score 5: The model consistently offers highly relevant and useful responses that perfectly cater to the user’s needs and inquiries.</p>
</div></blockquote>
<p>Judge Arena’s approach and policy framework has three key benefit worth highlighting:</p>
<ol class="arabic simple">
<li><p>Transparency through open-source code, documentation, and data sharing</p></li>
<li><p>LLM inclusion criteria requiring scoring/critique capabilities and public accessibility</p></li>
<li><p>ELO-based leaderboard system with community involvement in evaluations</p></li>
</ol>
<p>In that way, the platform enables democratic evaluation of AI judges while maintaining transparency and accessibility standards.</p>
</section>
</section>
<section id="benchmarks-and-leaderboards">
<h2><a class="toc-backref" href="#id54" role="doc-backlink"><span class="section-number">4.7. </span>Benchmarks and Leaderboards</a><a class="headerlink" href="#benchmarks-and-leaderboards" title="Permalink to this heading">¶</a></h2>
<p>Benchmarks act as standardized tests for LLMs, evaluating their performance across a spectrum of tasks. These tasks simulate real-world applications such as answering questions, generating coherent text, solving mathematical problems, or even writing computer code. They also assess more abstract qualities like fairness, robustness, and cultural understanding.</p>
<p>Benchmarks can be thought as comprehensive “exams” that probe different “subjects” in order to certify an LLM. They help researchers and developers compare models systematically, in a way LLM performance is comparable while enabling the identification of emergent behaviors or capabilities as models evolve in scale and sophistication.</p>
<p>The history of LLM benchmarks reflects the evolving priorities of artificial intelligence research, starting with foundational tasks and moving toward complex, real-world challenges. It began in <strong>2018</strong> with the introduction of <strong>GLUE (General Language Understanding Evaluation)</strong>, which set a new standard for evaluating natural language understanding. GLUE measured performance on tasks like sentiment analysis and textual entailment, providing a baseline for assessing the fundamental capabilities of language models. A year later, <strong>SuperGLUE (2019)</strong> expanded on this foundation by introducing more nuanced tasks that tested reasoning and language comprehension at a deeper level, challenging the limits of models like BERT and its successors.</p>
<p>As AI capabilities grew, benchmarks evolved to capture broader and more diverse aspects of intelligence. <strong>BIG-Bench (2021)</strong> marked a turning point by incorporating over 200 tasks, spanning arithmetic, logic, and creative problem-solving. This collaborative effort aimed to probe emergent abilities in large models, offering insights into how scale and complexity influence performance. Around the same time, specialized benchmarks like <strong>TruthfulQA (2021)</strong> emerged, addressing the critical need for models to provide accurate and non-deceptive information in a world increasingly dependent on AI for factual content.</p>
<p>In <strong>2022</strong>, Stanford’s <strong>HELM (Holistic Evaluation of Language Models)</strong> set a new standard for multidimensional assessment. HELM expanded the scope of evaluation beyond accuracy, incorporating factors like fairness, robustness, and computational efficiency. This benchmark was designed to address societal concerns surrounding AI, emphasizing safety and inclusion alongside technical performance. Similarly, <strong>MMLU (Massive Multitask Language Understanding)</strong>, launched in <strong>2021</strong>, provided a rigorous test of a model’s multidisciplinary knowledge, covering 57 subjects from STEM fields to humanities and social sciences.</p>
<p>Specialized benchmarks like <strong>HumanEval (2021)</strong> focused on domain-specific tasks, such as code generation, testing models’ ability to translate natural language descriptions into functional programming code. In contrast, <strong>LMSYS (2023)</strong> brought real-world applicability into focus by evaluating conversational AI through multi-turn dialogues. LMSYS prioritized coherence, contextual understanding, and user satisfaction, providing a practical lens for assessing models like GPT and Claude in dynamic settings.</p>
<p>From the early days of GLUE to the groundbreaking ARC Prize, the history of benchmarks illustrates a steady progression toward holistic and meaningful AI evaluation. Each new benchmark addresses emerging challenges and raises the bar for what AI systems can and should achieve, ensuring that these technologies align with both technical ambitions and societal needs.</p>
<p>As LLM benchmarks develop so do leaderboards. Leaderboards serve as standardized platforms to compare and rank models based on specific performance metrics / benchmarks. These evaluation systems help track LLM capabilities while maintaining transparency and reproducibility.</p>
<p>The <strong>HuggingFace Open LLM</strong> Leaderboard stands out for its transparency and accessibility in the open-source community. This leaderboard evaluates a wide range of LLMs across diverse tasks, including general knowledge, reasoning, and code-writing. Its commitment to reproducibility ensures that results are verifiable, enabling researchers and practitioners to replicate findings. By focusing on open-source models, it democratizes AI research and fosters innovation across communities, making it a valuable resource for both academics and industry professionals.</p>
<p>The <strong>Chatbot Arena</strong> Leaderboard (formerly LMSYS) takes an alternative approach by measuring real-world performance through direct model comparisons. Its evaluation format compares models in live conversations, with human judges providing qualitative assessments. This methodology has gathered over 200,000 human evaluations, offering specific insights into practical model performance. The emphasis on interactive capabilities makes it relevant for developing user-facing applications like virtual assistants and chatbots.</p>
<p>The <strong>AlpacaEval</strong> and <strong>MT-Bench</strong> Leaderboards implement automated evaluation using GPT-4 to assess model performance in multi-turn conversations. This approach enables consistent assessment of dialogue capabilities while reducing human bias. Their methodology measures key aspects of conversational AI, including contextual understanding and response consistency across multiple exchanges.</p>
<p>A significant shift in AI evaluation came with the launch of the <strong>The Alignment Research Center (ARC) Prize</strong> by ARC Prize Inc., a non-profit for the public advancement of open artificial general intelligence. Hosted by Mike Knoop (Co-founder, Zapier) and François Chollet (Creator of ARC-AGI, Keras), this prize represents a paradigm shift in how we evaluate language models. Rather than focusing on narrow performance metrics, the ARC Prize assesses what it calls “cognitive sufficiency” - a model’s ability to generate meaningful insights and tackle open-ended challenges. This new way to think about LLM evaluation emphasizes creative thinking, sophisticated reasoning, and the capacity to make genuinely useful contributions to human knowledge as we seek to define and measure what it means to achieve AGI (Artificial General Intelligence).</p>
<p>Defining AGI according to ARC Prize:</p>
<blockquote>
<div><p>Consensus but wrong:</p>
<ul class="simple">
<li><p>AGI is a system that can automate the majority of economically valuable work.</p></li>
</ul>
</div></blockquote>
<blockquote>
<div><p>Correct:</p>
<ul class="simple">
<li><p>AGI is a system that can efficiently acquire new skills and solve open-ended problems.</p></li>
</ul>
</div></blockquote>
<p>The ARC benchmark distinguishes itself from other LLM benchmarks especially in its resistance to memorization by prioritizing:</p>
<ul class="simple">
<li><p>Focus on Core Knowledge: Unlike LLM benchmarks that test a broad range of knowledge and skills, often relying heavily on memorization, ARC focuses on core knowledge similar to what a four or five-year-old child might possess. This includes basic concepts like object recognition, counting, and elementary physics.</p></li>
<li><p>Novelty of Tasks: Each ARC puzzle is designed to be novel, meaning it’s something you likely wouldn’t have encountered before, even if you had memorized the entire internet. This characteristic directly challenges the way LLMs typically operate, which is by leveraging their vast “interpolative memory.”</p></li>
<li><p>Emphasis on Program Synthesis: ARC tasks require models to synthesize new solution programs on the fly for each unique puzzle. This stands in contrast to the more common LLM approach of retrieving pre-existing solution programs from memory.</p></li>
<li><p>Resistance to Brute Force Attempts: While acknowledging the possibility, ARC aims to be resistant to brute-force approaches where a model might be trained on millions of similar puzzles to achieve a high score by relying on overlap with the test set.</p></li>
</ul>
<p>ARC-AGI tasks are a series of three to five input and output tasks followed by a final task with only the input listed (e.g. <a class="reference internal" href="#arc"><span class="std std-numref">Fig. 4.7</span></a>). Each task tests the utilization of a specific learned skill based on a minimal number of cognitive priors. A successful submission is a pixel-perfect description (color and position) of the final task’s output.</p>
<figure class="align-center" id="arc">
<a class="reference internal image-reference" href="../_images/arc.png"><img alt="ARC-AGI Task" src="../_images/arc.png" style="width: 757.0px; height: 137.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.7 </span><span class="caption-text">Sample ARC-AGI Task.</span><a class="headerlink" href="#arc" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>These features make the ARC benchmark a unique test of machine intelligence, focusing on the ability to adapt to novelty and solve problems without relying heavily on memorization. This is more aligned with the concept of general intelligence, which emphasizes the ability to learn efficiently and tackle new challenges.</p>
<p>As language models continue to advance in capability and complexity, evaluation frameworks must evolve. Modern benchmarks increasingly incorporate tests for nuanced reasoning, ethical decision-making, and emergent capabilities that weren’t previously measurable. This ongoing evolution reflects a deeper understanding that the true value of language models lies not in achieving high scores on standardized tests with narrow task-specific metrics, but in their ability to meaningfully contribute to human understanding and help solve real-world problems while demonstrating the ability to learn and adapt to new tasks.</p>
</section>
<section id="tools">
<h2><a class="toc-backref" href="#id55" role="doc-backlink"><span class="section-number">4.8. </span>Tools</a><a class="headerlink" href="#tools" title="Permalink to this heading">¶</a></h2>
<section id="lighteval">
<h3><a class="toc-backref" href="#id56" role="doc-backlink"><span class="section-number">4.8.1. </span>LightEval</a><a class="headerlink" href="#lighteval" title="Permalink to this heading">¶</a></h3>
<p>LightEval <span id="id8">[<a class="reference internal" href="#id29" title="Clémentine Fourrier, Nathan Habib, Thomas Wolf, and Lewis Tunstall. Lighteval: a lightweight framework for llm evaluation. 2023. URL: https://github.com/huggingface/lighteval.">Fourrier <em>et al.</em>, 2023</a>]</span> is a lightweight framework for evaluation of LLMs across a variety of standard and bespoke metrics and tasks across multiple inference backends via Python SDK and CLI.</p>
<p>As a motivating example, consider a scenario where financial data has been extracted from SEC financial filings and require econometric analysis. Tasks like estimating autoregressive models for time series forecasting or conducting hypothesis tests on market efficiency are common in financial analysis. Let’s evaluate how well different models perform on this type of task.</p>
<p>First, we need to select a benchmark to assess LLMs capabilities in this domain. MMLU has a sub-benchmark called Econometrics we can use for this task. <a class="reference internal" href="#mmlu-econometrics"><span class="std std-numref">Table 4.4</span></a> shows a sample of the benchmark dataset from MMLU Econometrics. It consists of multiple-choice questions from econometrics and expected answers.</p>
<table class="docutils align-default" id="mmlu-econometrics">
<caption><span class="caption-number">Table 4.4 </span><span class="caption-text">MMLU Econometrics Task Dataset sample</span><a class="headerlink" href="#mmlu-econometrics" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Question</p></th>
<th class="head"><p>Options</p></th>
<th class="head"><p>Correct Options</p></th>
<th class="head"><p>Correct Options Index</p></th>
<th class="head"><p>Correct Options Literal</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Consider the following AR(1) model with the disturbances having zero mean and unit variance: yt = 0.2 + 0.4 yt-1 + ut The (unconditional) mean of y will be given by</p></td>
<td><p>[“0.2”, “0.4”, “0.5”, “0.33”]</p></td>
<td><p>[“b”]</p></td>
<td><p>[3]</p></td>
<td><p>[“0.33”]</p></td>
</tr>
<tr class="row-odd"><td><p>Suppose that a test statistic has associated with it a p-value of 0.08. Which one of the following statements is true? (i) If the size of the test were exactly 8%, we…</p></td>
<td><p>[“(ii) and (iv) only”, “(i) and (iii) only”, “(i), (ii), and (iii) only”, “(i), (ii), (iii), and (iv)”]</p></td>
<td><p>[“c”]</p></td>
<td><p>[2]</p></td>
<td><p>[“(i), (ii), and (iii) only”]</p></td>
</tr>
<tr class="row-even"><td><p>What would be then consequences for the OLS estimator if heteroscedasticity is present in a regression model but ignored?</p></td>
<td><p>[“It will be biased”, “It will be inconsistent”, “It will be inefficient”, “All of (a), (b) and (c) will be true.”]</p></td>
<td><p>[“c”]</p></td>
<td><p>[2]</p></td>
<td><p>[“It will be inefficient”]</p></td>
</tr>
<tr class="row-odd"><td><p>Suppose now that a researcher wishes to use information criteria to determine the optimal lag length for a VAR. 500 observations are available for the bivariate VAR…</p></td>
<td><p>[“1 lag”, “2 lags”, “3 lags”, “4 lags”]</p></td>
<td><p>[“c”]</p></td>
<td><p>[2]</p></td>
<td><p>[“3 lags”]</p></td>
</tr>
</tbody>
</table>
<p>The code sample below demonstrates the LightEval Python SDK framework for evaluating a target LLM model on a given task.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>lighteval<span class="o">[</span>accelerate<span class="o">]</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">lighteval</span>
<span class="kn">from</span> <span class="nn">lighteval.logging.evaluation_tracker</span> <span class="kn">import</span> <span class="n">EvaluationTracker</span>
<span class="kn">from</span> <span class="nn">lighteval.models.model_config</span> <span class="kn">import</span> <span class="n">BaseModelConfig</span>
<span class="kn">from</span> <span class="nn">lighteval.pipeline</span> <span class="kn">import</span> <span class="n">ParallelismManager</span><span class="p">,</span> <span class="n">Pipeline</span><span class="p">,</span> <span class="n">PipelineParameters</span>
<span class="kn">from</span> <span class="nn">lighteval.utils.utils</span> <span class="kn">import</span> <span class="n">EnvConfig</span>
<span class="kn">from</span> <span class="nn">lighteval.utils.imports</span> <span class="kn">import</span> <span class="n">is_accelerate_available</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span>
<span class="kn">from</span> <span class="nn">accelerate</span> <span class="kn">import</span> <span class="n">Accelerator</span><span class="p">,</span> <span class="n">InitProcessGroupKwargs</span>


<span class="k">def</span> <span class="nf">create_evaluation_pipeline</span><span class="p">(</span><span class="n">output_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">cache_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">pretrained</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;float16&quot;</span><span class="p">,</span> <span class="n">max_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">task</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">is_accelerate_available</span><span class="p">():</span>
        <span class="kn">from</span> <span class="nn">accelerate</span> <span class="kn">import</span> <span class="n">Accelerator</span><span class="p">,</span> <span class="n">InitProcessGroupKwargs</span>
        <span class="n">accelerator</span> <span class="o">=</span> <span class="n">Accelerator</span><span class="p">(</span><span class="n">kwargs_handlers</span><span class="o">=</span><span class="p">[</span><span class="n">InitProcessGroupKwargs</span><span class="p">(</span><span class="n">timeout</span><span class="o">=</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="mi">3000</span><span class="p">))])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">accelerator</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="n">evaluation_tracker</span> <span class="o">=</span> <span class="n">EvaluationTracker</span><span class="p">(</span>
        <span class="n">output_dir</span><span class="o">=</span><span class="n">output_dir</span><span class="p">,</span>
        <span class="n">save_details</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">push_to_hub</span><span class="o">=</span><span class="kc">False</span>  
    <span class="p">)</span>

    <span class="n">pipeline_params</span> <span class="o">=</span> <span class="n">PipelineParameters</span><span class="p">(</span>
        <span class="n">launcher_type</span><span class="o">=</span><span class="n">ParallelismManager</span><span class="o">.</span><span class="n">ACCELERATE</span><span class="p">,</span>
        <span class="n">env_config</span><span class="o">=</span><span class="n">EnvConfig</span><span class="p">(</span><span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">),</span>
        <span class="n">override_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">max_samples</span><span class="o">=</span><span class="n">max_samples</span>
    <span class="p">)</span>

    <span class="n">model_config</span> <span class="o">=</span> <span class="n">BaseModelConfig</span><span class="p">(</span>
        <span class="n">pretrained</span><span class="o">=</span><span class="n">pretrained</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">use_chat_template</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

    <span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span>
        <span class="n">tasks</span><span class="o">=</span><span class="n">task</span><span class="p">,</span>
        <span class="n">pipeline_parameters</span><span class="o">=</span><span class="n">pipeline_params</span><span class="p">,</span>
        <span class="n">evaluation_tracker</span><span class="o">=</span><span class="n">evaluation_tracker</span><span class="p">,</span>
        <span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span>
    <span class="p">)</span>
    
    <span class="k">return</span> <span class="n">pipeline</span>
</pre></div>
</div>
<p><a class="reference internal" href="#id9"><span class="std std-numref">Fig. 4.8</span></a> shows a schematic representation of its key components. As inference engine, we leverage <code class="docutils literal notranslate"><span class="pre">accelerate</span></code> for distributed evaluation. <code class="docutils literal notranslate"><span class="pre">lighteval</span></code> also supports other inference backends such as <code class="docutils literal notranslate"><span class="pre">vllm</span></code> and <code class="docutils literal notranslate"><span class="pre">tgi</span></code>.</p>
<p>First, we instantiate an <code class="docutils literal notranslate"><span class="pre">EvaluationTracker</span></code> which manages result storage, in this example kept in a local directory <code class="docutils literal notranslate"><span class="pre">output_dir</span></code>, and tracks detailed evaluation metrics, optionally pushed to HuggingFace Hub.</p>
<p>Next, we instantiate an object of the class <code class="docutils literal notranslate"><span class="pre">PipelineParameters</span></code> which, in this example, configures the pipeline for parallel processing with a temporary cache in <code class="docutils literal notranslate"><span class="pre">cache_dir</span></code> also setting the maximum number of samples to process to <code class="docutils literal notranslate"><span class="pre">max_samples</span></code>. Then, in <code class="docutils literal notranslate"><span class="pre">BaseModelConfig</span></code> we set up the LLM model we would like to evaluate defined in <code class="docutils literal notranslate"><span class="pre">pretrained</span></code>.</p>
<figure class="align-center" id="id9">
<a class="reference internal image-reference" href="../_images/lighteval.png"><img alt="LightEval Python SDK Sample Conceptual Overview." src="../_images/lighteval.png" style="width: 734.3px; height: 387.79999999999995px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.8 </span><span class="caption-text">LightEval Python SDK Sample Conceptual Overview.</span><a class="headerlink" href="#id9" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>This setup allows for systematic evaluation of language model performance on specific tasks while handling distributed computation and result tracking.</p>
<p>The final Pipeline combines these components to evaluate in the user defined <code class="docutils literal notranslate"><span class="pre">task</span></code>, which follows the following format:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">{</span>suite<span class="o">}</span><span class="p">|</span><span class="o">{</span>task<span class="o">}</span><span class="p">|</span><span class="o">{</span>num_few_shot<span class="o">}</span><span class="p">|</span><span class="o">{</span><span class="m">0</span><span class="w"> </span>or<span class="w"> </span><span class="m">1</span><span class="w"> </span>to<span class="w"> </span>automatically<span class="w"> </span>reduce<span class="w"> </span><span class="sb">`</span>num_few_shot<span class="sb">`</span><span class="w"> </span><span class="k">if</span><span class="w"> </span>prompt<span class="w"> </span>is<span class="w"> </span>too<span class="w"> </span>long<span class="o">}</span>
</pre></div>
</div>
<p>The task string format follows a specific pattern with four components separated by vertical bars (|):</p>
<ol class="arabic simple">
<li><p>suite: The evaluation suite name (e.g., “leaderboard”)</p></li>
<li><p>task: The specific task name (e.g., “mmlu:econometrics”)</p></li>
<li><p>num_few_shot: The number of few-shot examples to use (e.g., “0” for zero-shot)</p></li>
<li><p>A binary flag (0 or 1) that controls whether to automatically reduce the number of few-shot examples if the prompt becomes too long</p></li>
</ol>
<p>LightEval provides a comprehensive set of evaluation tasks <span id="id10">[<a class="reference internal" href="#id33" title="Hugging Face. Available tasks - lighteval wiki. https://github.com/huggingface/lighteval/wiki/Available-Tasks, 2024. Accessed: 2024.">Face, 2024</a>]</span> and metrics <span id="id11">[<a class="reference internal" href="#id34" title="Hugging Face. Metric list - lighteval wiki. https://github.com/huggingface/lighteval/wiki/Metric-List, 2024. Accessed: 2024.">Face, 2024</a>]</span>. The available tasks  span multiple categories and benchmarks including BigBench, MMLU, TruthfulQA, WinoGrande, and HellaSwag. The framework also supports standard NLP evaluation metrics including BLEU, ROUGE, Exact Match, F1 Score, and Accuracy.</p>
<p>In our case, we choose to evaluate our LLMs on the MMLU econometrics task using zero-shot learning. Hence, we define the <code class="docutils literal notranslate"><span class="pre">task</span></code> as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">task</span> <span class="o">=</span> <span class="s2">&quot;leaderboard|mmlu:econometrics|0|0&quot;</span>
</pre></div>
</div>
<p>Example usage to evaluate an LLM, for instance <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-3.2-1B-Instruct</span></code>, on the MMLU econometrics task using zero-shot learning:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">task</span> <span class="o">=</span> <span class="s2">&quot;leaderboard|mmlu:econometrics|0|0&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Llama-3.2-1B-Instruct&quot;</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">create_evaluation_pipeline</span><span class="p">(</span><span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;./evals/&quot;</span><span class="p">,</span> <span class="n">cache_dir</span><span class="o">=</span><span class="s2">&quot;./cache/&quot;</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">task</span><span class="o">=</span><span class="n">task</span><span class="p">)</span>
</pre></div>
</div>
<p>We can then evaluate the pipeline, save and show its results as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pipeline</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">save_and_push_results</span><span class="p">()</span>
<span class="n">pipeline</span><span class="o">.</span><span class="n">show_results</span><span class="p">()</span>
</pre></div>
</div>
<p>The results are then stored in <code class="docutils literal notranslate"><span class="pre">output_dir</span></code> in JSON format.</p>
<p>The same results can be obtained by using the LightEval CLI:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>lighteval<span class="w"> </span>accelerate<span class="w"> </span>--model_args<span class="w"> </span><span class="s2">&quot;pretrained=meta-llama/Llama-3.2-1B-Instruct&quot;</span><span class="w"> </span>--tasks<span class="w"> </span><span class="s2">&quot;leaderboard|mmlu:econometrics|0|0&quot;</span><span class="w"> </span>--override_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span>--output_dir<span class="o">=</span><span class="s2">&quot;./evals/&quot;</span>
</pre></div>
</div>
<p>We would like to compare the performance of multiple open source models on the MMLU econometrics task. While we could download and evaluate each model locally, we prefer instead to evaluate them on a remote server to save time and resources. LightEval enables serving the model on a TGI-compatible server/container and then running the evaluation by sending requests to the server <span id="id12">[<a class="reference internal" href="#id35" title="Hugging Face. Evaluate the model on a server or container - lighteval wiki. https://github.com/huggingface/lighteval/wiki/Evaluate-the-model-on-a-server-or-container, 2024. Accessed: 2024.">Face, 2024</a>]</span>.</p>
<p>For that purpose, we can leverage HuggingFace Serverless Inference API (or dedicated inference API) and set a configuration file for LightEval as shown below, where <code class="docutils literal notranslate"><span class="pre">&lt;MODEL-ID&gt;</span></code> is the model identifier on HuggingFace (e.g. <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-3.2-1B-Instruct</span></code>) and <code class="docutils literal notranslate"><span class="pre">&lt;HUGGINGFACE-TOKEN&gt;</span></code> is the user’s HuggingFace API token.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="p">:</span>
  <span class="nb">type</span><span class="p">:</span> <span class="s2">&quot;tgi&quot;</span>
  <span class="n">instance</span><span class="p">:</span>
    <span class="n">inference_server_address</span><span class="p">:</span> <span class="s2">&quot;https://api-inference.huggingface.co/models/&lt;MODEL-ID&gt;&quot;</span>
    <span class="n">inference_server_auth</span><span class="p">:</span> <span class="s2">&quot;&lt;HUGGINGFACE-TOKEN&gt;&quot;</span>
    <span class="n">model_id</span><span class="p">:</span> <span class="n">null</span>
</pre></div>
</div>
<p>Now we can run the evaluation by sending requests to the server as follows by using the same bash command as before but now setting the <code class="docutils literal notranslate"><span class="pre">model_config_path</span></code> to the path of the configuration file we have just created (e.g. <code class="docutils literal notranslate"><span class="pre">endpoint_model.yaml</span></code>):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>lighteval<span class="w"> </span>accelerate<span class="w"> </span>--model_config_path<span class="o">=</span><span class="s2">&quot;endpoint_model.yaml&quot;</span><span class="w"> </span>--tasks<span class="w"> </span><span class="s2">&quot;leaderboard|mmlu:econometrics|0|0&quot;</span><span class="w"> </span>--override_batch_size<span class="w"> </span><span class="m">1</span><span class="w"> </span>--output_dir<span class="o">=</span><span class="s2">&quot;./evals/&quot;</span>
</pre></div>
</div>
<p>To complete our task, we evaluate a few models from the following model families: <code class="docutils literal notranslate"><span class="pre">Llama3.2</span></code>, <code class="docutils literal notranslate"><span class="pre">Qwen2.5</span></code>, and <code class="docutils literal notranslate"><span class="pre">SmolLM2</span></code> as described in <a class="reference internal" href="#model-families"><span class="std std-numref">Table 4.5</span></a>.</p>
<table class="docutils align-default" id="model-families">
<caption><span class="caption-number">Table 4.5 </span><span class="caption-text">Model Families Evaluated Using LightEval</span><a class="headerlink" href="#model-families" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Model Family</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Models</p></th>
<th class="head"><p>References</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Llama3.2 Instruct</p></td>
<td><p>LLaMA architecture-based pretrained and instruction-tuned generative models</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Llama-3.2-1B-Instruct</span></code> <br> <code class="docutils literal notranslate"><span class="pre">Llama-3.2-3B-Instruct</span></code></p></td>
<td><p><span id="id13">[<a class="reference internal" href="#id39" title="Meta AI. Meta llama models on hugging face. https://huggingface.co/meta-llama, 2024. Accessed: 2024.">Meta AI, 2024</a>]</span></p></td>
</tr>
<tr class="row-odd"><td><p>Qwen2.5 Instruct</p></td>
<td><p>Instruction-tuned LLMs family built by Alibaba Cloud</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">Qwen2.5-0.5B-Instruct</span></code> <br> <code class="docutils literal notranslate"><span class="pre">Qwen2.5-1.5B-Instruct</span></code><br> <code class="docutils literal notranslate"><span class="pre">Qwen2.5-3B-Instruct</span></code></p></td>
<td><p><span id="id14">[<a class="reference internal" href="#id36" title="Hugging Face. Gpt-2 documentation - hugging face transformers. https://huggingface.co/docs/transformers/model_doc/gpt2, 2024. Accessed: 2024.">Face, 2024</a>, <a class="reference internal" href="#id31" title="Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, and others. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024.">Hui <em>et al.</em>, 2024</a>, <a class="reference internal" href="#id32" title="An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024.">Yang <em>et al.</em>, 2024</a>]</span></p></td>
</tr>
<tr class="row-even"><td><p>SmolLM2 Instruct</p></td>
<td><p>Instruction-tuned family of compact language models built by HuggingFace</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">SmolLM2-360M-Instruct</span></code> <br> <code class="docutils literal notranslate"><span class="pre">SmolLM2-1.7B-Instruct</span></code></p></td>
<td><p><span id="id15">[<a class="reference internal" href="#id30" title="Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Lewis Tunstall, Agustín Piqueres, Andres Marafioti, Cyril Zakka, Leandro von Werra, and Thomas Wolf. Smollm2 - with great data, comes great performance. 2024.">Allal <em>et al.</em>, 2024</a>]</span></p></td>
</tr>
</tbody>
</table>
<p>We can then compare the performance of these models on the MMLU econometrics task as shown in <a class="reference internal" href="#model-comparison"><span class="std std-numref">Fig. 4.9</span></a>.</p>
<figure class="align-center" id="model-comparison">
<a class="reference internal image-reference" href="../_images/model-comparison.png"><img alt="Model Comparison on MMLU Econometrics Task" src="../_images/model-comparison.png" style="width: 899.5px; height: 454.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.9 </span><span class="caption-text">Model performance comparison on MMLU Econometrics task, showing accuracy scores across different model sizes and architectures.</span><a class="headerlink" href="#model-comparison" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The results reveal several interesting patterns in model performance. As expected, we observe a trend where larger models consistently achieve higher accuracy scores. The evaluation shows distinct clusters among model families, with Qwen2.5, Llama-3.2, and SmolLM2 each exhibiting their own scaling characteristics, suggesting that architectural differences lead to varying degrees of efficiency as model size increases. Particularly noteworthy is the performance of the Qwen2.5 family, which demonstrates superior accuracy even at smaller model sizes when compared to Llama-3.2.</p>
<p>Of course, the results should be taken with a grain of salt given the limited size of the dataset (MMLU Econometrics ~ 100), limited number of models and sizes. However, it gives a good indication of the capabilities of the different models tested with Qwen2.5 family being an interesting first candidate as a relatively small yet powerful model demonstrating a good trade-off between performance and size. Once tested on real-world data, the results will change but these initial findings are a good data-driven starting point for model selection as you begin your LLM-based application development.</p>
<p>In summary, LightEval is a simple yet flexible and comprehensive framework for evaluating LLMs across a wide variety of tasks and metrics. It can serve as a first step in selecting your next LLM for a specific task given the exponential growth in number of (open source) models available <span id="id16">[<a class="reference internal" href="#id38" title="Hugging Face. Number of models on hugging face. https://huggingface.co/spaces/huggingface/open-source-ai-year-in-review-2024?day=4, 2024. Accessed: 12/06/2024.">Hugging Face, 2024</a>]</span>. Its integration with the Hugging Face ecosystem and modular architecture make it particularly powerful for evaluating open source models. For further details, visit the <a class="reference external" href="https://github.com/huggingface/lighteval">official repository</a> <span id="id17">[<a class="reference internal" href="#id29" title="Clémentine Fourrier, Nathan Habib, Thomas Wolf, and Lewis Tunstall. Lighteval: a lightweight framework for llm evaluation. 2023. URL: https://github.com/huggingface/lighteval.">Fourrier <em>et al.</em>, 2023</a>]</span>.</p>
</section>
</section>
<section id="references">
<h2><a class="toc-backref" href="#id57" role="doc-backlink"><span class="section-number">4.9. </span>References</a><a class="headerlink" href="#references" title="Permalink to this heading">¶</a></h2>
<div class="docutils container" id="id18">
<div class="citation" id="id30" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">ALB+24</a><span class="fn-bracket">]</span></span>
<p>Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Lewis Tunstall, Agustín Piqueres, Andres Marafioti, Cyril Zakka, Leandro von Werra, and Thomas Wolf. Smollm2 - with great data, comes great performance. 2024.</p>
</div>
<div class="citation" id="id28" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">Are24</a><span class="fn-bracket">]</span></span>
<p>Judge Arena. Judge arena: evaluating llm outputs with llms. <a class="reference external" href="https://judgearena.com/">https://judgearena.com/</a>, 2024. Accessed: 2024.</p>
</div>
<div class="citation" id="id33" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">Fac24a</a><span class="fn-bracket">]</span></span>
<p>Hugging Face. Available tasks - lighteval wiki. <a class="reference external" href="https://github.com/huggingface/lighteval/wiki/Available-Tasks">https://github.com/huggingface/lighteval/wiki/Available-Tasks</a>, 2024. Accessed: 2024.</p>
</div>
<div class="citation" id="id35" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">Fac24b</a><span class="fn-bracket">]</span></span>
<p>Hugging Face. Evaluate the model on a server or container - lighteval wiki. <a class="reference external" href="https://github.com/huggingface/lighteval/wiki/Evaluate-the-model-on-a-server-or-container">https://github.com/huggingface/lighteval/wiki/Evaluate-the-model-on-a-server-or-container</a>, 2024. Accessed: 2024.</p>
</div>
<div class="citation" id="id36" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">Fac24c</a><span class="fn-bracket">]</span></span>
<p>Hugging Face. Gpt-2 documentation - hugging face transformers. <a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/gpt2">https://huggingface.co/docs/transformers/model_doc/gpt2</a>, 2024. Accessed: 2024.</p>
</div>
<div class="citation" id="id26" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">Fac24d</a><span class="fn-bracket">]</span></span>
<p>Hugging Face. Llm as a judge. <a class="reference external" href="https://huggingface.co/learn/cookbook/en/llm_judge">https://huggingface.co/learn/cookbook/en/llm_judge</a>, 2024. Accessed: 2024.</p>
</div>
<div class="citation" id="id34" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">Fac24e</a><span class="fn-bracket">]</span></span>
<p>Hugging Face. Metric list - lighteval wiki. <a class="reference external" href="https://github.com/huggingface/lighteval/wiki/Metric-List">https://github.com/huggingface/lighteval/wiki/Metric-List</a>, 2024. Accessed: 2024.</p>
</div>
<div class="citation" id="id29" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>FHWT23<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id8">1</a>,<a role="doc-backlink" href="#id17">2</a>)</span>
<p>Clémentine Fourrier, Nathan Habib, Thomas Wolf, and Lewis Tunstall. Lighteval: a lightweight framework for llm evaluation. 2023. URL: <a class="reference external" href="https://github.com/huggingface/lighteval">https://github.com/huggingface/lighteval</a>.</p>
</div>
<div class="citation" id="id31" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">HYC+24</a><span class="fn-bracket">]</span></span>
<p>Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, and others. Qwen2. 5-coder technical report. <em>arXiv preprint arXiv:2409.12186</em>, 2024.</p>
</div>
<div class="citation" id="id25" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LXS+24<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id3">1</a>,<a role="doc-backlink" href="#id4">2</a>,<a role="doc-backlink" href="#id6">3</a>)</span>
<p>Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Yuxuan Lai, Chongyang Tao, and Shuai Ma. Leveraging large language models for nlg evaluation: advances and challenges. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2401.07103">https://arxiv.org/abs/2401.07103</a>, <a class="reference external" href="https://arxiv.org/abs/2401.07103">arXiv:2401.07103</a>.</p>
</div>
<div class="citation" id="id19" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">WTB+22</a><span class="fn-bracket">]</span></span>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. 2022. URL: <a class="reference external" href="https://arxiv.org/abs/2206.07682">https://arxiv.org/abs/2206.07682</a>, <a class="reference external" href="https://arxiv.org/abs/2206.07682">arXiv:2206.07682</a>.</p>
</div>
<div class="citation" id="id32" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">YYH+24</a><span class="fn-bracket">]</span></span>
<p>An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. <em>arXiv preprint arXiv:2407.10671</em>, 2024.</p>
</div>
<div class="citation" id="id38" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id16">HuggingFace24</a><span class="fn-bracket">]</span></span>
<p>Hugging Face. Number of models on hugging face. <a class="reference external" href="https://huggingface.co/spaces/huggingface/open-source-ai-year-in-review-2024?day=4">https://huggingface.co/spaces/huggingface/open-source-ai-year-in-review-2024?day=4</a>, 2024. Accessed: 12/06/2024.</p>
</div>
<div class="citation" id="id39" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">MetaAI24</a><span class="fn-bracket">]</span></span>
<p>Meta AI. Meta llama models on hugging face. <a class="reference external" href="https://huggingface.co/meta-llama">https://huggingface.co/meta-llama</a>, 2024. Accessed: 2024.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

          </div>
          <div class="page-nav">
            <div class="inner"><ul class="page-nav">
  <li class="prev">
    <a href="structured_output.html"
       title="previous chapter">← <span class="section-number">3. </span>Wrestling with Structured Output</a>
  </li>
</ul><div class="footer" role="contentinfo">
    <br>
    Created using <a href="http://sphinx-doc.org/">Sphinx</a> 6.2.1 with <a href="https://github.com/schettino72/sphinx_press_theme">Press Theme</a> 0.9.1.
</div>
            </div>
          </div>
      </page>
    </div></div>
    
    
  </body>
</html>