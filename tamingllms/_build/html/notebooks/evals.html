<!DOCTYPE html>
<html  lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1"><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

      <title>4. Challenges of Evaluating LLM-based Applications</title>
    
          <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
          <link rel="stylesheet" href="../_static/theme.css " type="text/css" />
          <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
          <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
          <link rel="stylesheet" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
          <link rel="stylesheet" href="../_static/sphinx-thebe.css" type="text/css" />
          <link rel="stylesheet" href="../_static/sphinx-design.4cbf315f70debaebd550c87a6162cf0f.min.css" type="text/css" />
      
      <!-- sphinx script_files -->
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script src="../_static/scripts/sphinx-book-theme.js"></script>
        <script>let toggleHintShow = 'Click to show';</script>
        <script>let toggleHintHide = 'Click to hide';</script>
        <script>let toggleOpenOnPrint = 'true';</script>
        <script src="../_static/togglebutton.js"></script>
        <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
        <script src="../_static/design-tabs.js"></script>
        <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
        <script async="async" src="../_static/sphinx-thebe.js"></script>

      
      <!-- bundled in js (rollup iife) -->
      <!-- <script src="../_static/theme-vendors.js"></script> -->
      <script src="../_static/theme.js" defer></script>
      <link rel="canonical" href="https://souzatharsis.github.io/tamingllms/notebooks/evals.html" />
    
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="prev" title="3. Wrestling with Structured Output" href="structured_output.html" /> 
  </head>

  <body>
    <div id="app">
    <div class="theme-container" :class="pageClasses"><navbar @toggle-sidebar="toggleSidebar">
  <router-link to="../markdown/toc.html" class="home-link">
    
      <span class="site-name">tamingLLMs</span>
    
  </router-link>

  <div class="links">
    <navlinks class="can-hide">



  
    <div class="nav-item">
      <a href="https://github.com/souzatharsis/tamingllms"
        class="nav-link external">
          Github <outboundlink></outboundlink>
      </a>
    </div>
  

    </navlinks>
  </div>
</navbar>

      
      <div class="sidebar-mask" @click="toggleSidebar(false)">
      </div>
        <sidebar @toggle-sidebar="toggleSidebar">
          
          <navlinks>
            



  
    <div class="nav-item">
      <a href="https://github.com/souzatharsis/tamingllms"
        class="nav-link external">
          Github <outboundlink></outboundlink>
      </a>
    </div>
  

            
          </navlinks><div id="searchbox" class="searchbox" role="search">
  <div class="caption"><span class="caption-text">Quick search</span>
    <div class="searchformwrapper">
      <form class="search" action="../search.html" method="get">
        <input type="text" name="q" />
        <input type="submit" value="Search" />
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
      </form>
    </div>
  </div>
</div><div class="sidebar-links" role="navigation" aria-label="main navigation">
  
    <div class="sidebar-group">
      <p class="caption">
        <span class="caption-text"><a href="../markdown/toc.html#taming-large-language-models">taming large language models</a></span>
      </p>
      <ul class="current">
        
          <li class="toctree-l1 ">
            
              <a href="../markdown/intro.html" class="reference internal ">Introduction</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="output_size_limit.html" class="reference internal ">Output Size Limitations</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="structured_output.html" class="reference internal ">Wrestling with Structured Output</a>
            

            
          </li>

        
          <li class="toctree-l1 current">
            
              <a href="#" class="reference internal current">Challenges of Evaluating LLM-based Applications</a>
            

            
              <ul>
                
                  <li class="toctree-l2"><a href="#non-deterministic-machines" class="reference internal">Non-Deterministic Machines</a></li>
                
                  <li class="toctree-l2"><a href="#emerging-properties" class="reference internal">Emerging Properties</a></li>
                
                  <li class="toctree-l2"><a href="#problem-statement" class="reference internal">Problem Statement</a></li>
                
                  <li class="toctree-l2"><a href="#evals-design" class="reference internal">Evals Design</a></li>
                
                  <li class="toctree-l2"><a href="#key-components" class="reference internal">Key Components</a></li>
                
                  <li class="toctree-l2"><a href="#tools" class="reference internal">Tools</a></li>
                
                  <li class="toctree-l2"><a href="#references" class="reference internal">References</a></li>
                
              </ul>
            
          </li>

        
      </ul>
    </div>
  
</div>
        </sidebar>

      <page>
          <div class="body-header" role="navigation" aria-label="navigation">
  
  <ul class="breadcrumbs">
    <li><a href="../markdown/toc.html">Docs</a> &raquo;</li>
    
    <li><span class="section-number">4. </span>Challenges of Evaluating LLM-based Applications</li>
  </ul>
  

  <ul class="page-nav">
  <li class="prev">
    <a href="structured_output.html"
       title="previous chapter">← <span class="section-number">3. </span>Wrestling with Structured Output</a>
  </li>
</ul>
  
</div>
<hr>
          <div class="content" role="main" v-pre>
            
  <section class="tex2jax_ignore mathjax_ignore" id="challenges-of-evaluating-llm-based-applications">
<h1><a class="toc-backref" href="#id10" role="doc-backlink"><span class="section-number">4. </span>Challenges of Evaluating LLM-based Applications</a><a class="headerlink" href="#challenges-of-evaluating-llm-based-applications" title="Permalink to this heading">¶</a></h1>
<blockquote class="epigraph">
<div><p>Evals are surprisingly often all you need.</p>
<p class="attribution">—Greg Brockman, OpenAI’s President</p>
</div></blockquote>
<nav class="contents" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#challenges-of-evaluating-llm-based-applications" id="id10">Challenges of Evaluating LLM-based Applications</a></p>
<ul>
<li><p><a class="reference internal" href="#non-deterministic-machines" id="id11">Non-Deterministic Machines</a></p>
<ul>
<li><p><a class="reference internal" href="#temperature-and-sampling" id="id12">Temperature and Sampling</a></p></li>
<li><p><a class="reference internal" href="#the-temperature-spectrum" id="id13">The Temperature Spectrum</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#emerging-properties" id="id14">Emerging Properties</a></p></li>
<li><p><a class="reference internal" href="#problem-statement" id="id15">Problem Statement</a></p></li>
<li><p><a class="reference internal" href="#evals-design" id="id16">Evals Design</a></p>
<ul>
<li><p><a class="reference internal" href="#conceptual-overview" id="id17">Conceptual Overview</a></p></li>
<li><p><a class="reference internal" href="#design-considerations" id="id18">Design Considerations</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#key-components" id="id19">Key Components</a></p>
<ul>
<li><p><a class="reference internal" href="#metrics" id="id20">Metrics</a></p>
<ul>
<li><p><a class="reference internal" href="#example-bleu-and-rouge-for-sec-filing-summarization" id="id21">Example: BLEU and ROUGE for SEC Filing Summarization</a></p></li>
<li><p><a class="reference internal" href="#considerations" id="id22">Considerations</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#evaluators" id="id23">Evaluators</a></p>
<ul>
<li><p><a class="reference internal" href="#model-based-evaluation" id="id24">Model-Based Evaluation</a></p></li>
<li><p><a class="reference internal" href="#human-based-evaluation" id="id25">Human-Based Evaluation</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#leaderboard" id="id26">Leaderboard</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#tools" id="id27">Tools</a></p></li>
<li><p><a class="reference internal" href="#references" id="id28">References</a></p></li>
</ul>
</li>
</ul>
</nav>
<section id="non-deterministic-machines">
<h2><a class="toc-backref" href="#id11" role="doc-backlink"><span class="section-number">4.1. </span>Non-Deterministic Machines</a><a class="headerlink" href="#non-deterministic-machines" title="Permalink to this heading">¶</a></h2>
<p>One of the most fundamental challenges when building products with Large Language Models (LLMs) is their non-deterministic nature. Unlike traditional software systems where the same input reliably produces the same output, LLMs can generate different responses each time they’re queried - even with identical prompts and input data. This characteristic is both a strength and a significant engineering challenge.</p>
<p>When you ask ChatGPT or any other LLM the same question multiple times, you’ll likely get different responses. This isn’t a bug - it’s a fundamental feature of how these models work. The “temperature” parameter, which controls the randomness of outputs, allows models to be creative and generate diverse responses. However, this same feature makes it incredibly difficult to build reliable, testable systems.</p>
<p>Consider a financial services company using LLMs to generate investment advice. The non-deterministic nature of these models means that:</p>
<ul class="simple">
<li><p>The same market data could yield different analysis conclusions</p></li>
<li><p>Testing becomes exceedingly more complex compared to traditional software</p></li>
<li><p>Regulatory compliance becomes challenging to guarantee</p></li>
<li><p>User trust may be affected by inconsistent responses</p></li>
</ul>
<section id="temperature-and-sampling">
<h3><a class="toc-backref" href="#id12" role="doc-backlink"><span class="section-number">4.1.1. </span>Temperature and Sampling</a><a class="headerlink" href="#temperature-and-sampling" title="Permalink to this heading">¶</a></h3>
<p>The primary source of non-determinism in LLMs comes from their sampling strategies. During text generation, the model:</p>
<ol class="arabic simple">
<li><p>Calculates probability distributions for each next token</p></li>
<li><p>Samples from these distributions based on temperature settings</p></li>
<li><p>Uses techniques like nucleus sampling to balance creativity and coherence</p></li>
</ol>
</section>
<section id="the-temperature-spectrum">
<h3><a class="toc-backref" href="#id13" role="doc-backlink"><span class="section-number">4.1.2. </span>The Temperature Spectrum</a><a class="headerlink" href="#the-temperature-spectrum" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>Temperature = 0: Most deterministic, but potentially repetitive</p></li>
<li><p>Temperature = 1: Balanced creativity and coherence</p></li>
<li><p>Temperature &gt; 1: Increased randomness, potentially incoherent</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># Load environment variables from .env file</span>
<span class="n">load_dotenv</span><span class="p">()</span>

<span class="kn">from</span> <span class="nn">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span>

<span class="k">def</span> <span class="nf">generate_responses</span><span class="p">(</span>
    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">temperatures</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span>
    <span class="n">attempts</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate multiple responses at different temperature settings</span>
<span class="sd">    to demonstrate non-deterministic behavior.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">temp</span> <span class="ow">in</span> <span class="n">temperatures</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">attempt</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">attempts</span><span class="p">):</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}],</span>
                <span class="n">temperature</span><span class="o">=</span><span class="n">temp</span><span class="p">,</span>
                <span class="n">max_tokens</span><span class="o">=</span><span class="mi">50</span>
            <span class="p">)</span>
            
            <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
                <span class="s1">&#39;temperature&#39;</span><span class="p">:</span> <span class="n">temp</span><span class="p">,</span>
                <span class="s1">&#39;attempt&#39;</span><span class="p">:</span> <span class="n">attempt</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
                <span class="s1">&#39;response&#39;</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
            <span class="p">})</span>

    <span class="c1"># Display results grouped by temperature</span>
    <span class="n">df_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">temp</span> <span class="ow">in</span> <span class="n">temperatures</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Temperature = </span><span class="si">{</span><span class="n">temp</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">40</span><span class="p">)</span>
        <span class="n">temp_responses</span> <span class="o">=</span> <span class="n">df_results</span><span class="p">[</span><span class="n">df_results</span><span class="p">[</span><span class="s1">&#39;temperature&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">temp</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">temp_responses</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Attempt </span><span class="si">{</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;attempt&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">row</span><span class="p">[</span><span class="s1">&#39;response&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">df_results</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">MAX_LENGTH</span> <span class="o">=</span> <span class="mi">10000</span> <span class="c1"># We limit the input length to avoid token issues</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;../data/apple.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">sec_filing</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<span class="n">sec_filing</span> <span class="o">=</span> <span class="n">sec_filing</span><span class="p">[:</span><span class="n">MAX_LENGTH</span><span class="p">]</span> 
<span class="n">df_results</span> <span class="o">=</span> <span class="n">generate_responses</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span> 
                                <span class="n">prompt</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Write a single-statement executive summary of the following text: </span><span class="si">{</span><span class="n">sec_filing</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> 
                                <span class="n">temperatures</span><span class="o">=</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Temperature = 0.0
----------------------------------------
Attempt 1: Apple Inc. filed its Form 10-K for the fiscal year ended September 28, 2024 with the SEC, detailing its business operations and financial performance.
Attempt 2: Apple Inc. filed its Form 10-K with the SEC for the fiscal year ended September 28, 2024, detailing its business operations, products, and financial information.
Attempt 3: Apple Inc. filed its Form 10-K with the SEC for the fiscal year ended September 28, 2024, detailing its business operations, products, and financial information.

Temperature = 1.0
----------------------------------------
Attempt 1: Apple Inc., a well-known seasoned issuer based in California, designs, manufactures, and markets smartphones, personal computers, tablets, wearables, and accessories, with a focus on innovation and technology.
Attempt 2: Apple Inc. filed its Form 10-K with the SEC for the fiscal year ended September 28, 2024, reporting on its business operations, products, and financial performance.
Attempt 3: Apple Inc., a well-known seasoned issuer, filed its Form 10-K for the fiscal year ended September 28, 2024, reporting on its financial condition and operations.

Temperature = 2.0
----------------------------------------
Attempt 1: The Form 10-K for Apple Inc. for the fiscal year ended September 28, 2024, filed with the Securities and Exchange Commission, outlines the company&#39;s financial performance, products, and risk factors affecting future results.
Attempt 2: Apple Inc., a California-based company and leading technology manufacturer invDestacksmeticsisdiction setIspection-$20cyan evaluationseld anvisions droitEntering discernminerval Versbobprefversible vo该 Option和 meio forecast времCisco dellaischenpoihsCapabilities Geme.getTime future
Attempt 3: Apple Inc&#39;s Form 10-K provides a comprehensive overview of the company&#39;s financial reporting, business operations, products and market information.
</pre></div>
</div>
</div>
</div>
<p>This simple experiment reveals a fundamental challenge in LLM evaluation: even a simple parameter like temperature can dramatically alter model behavior in ways that are difficult to systematically assess. At temperature 0.0, responses are consistent but potentially too rigid. At 1.0, outputs become more varied but less predictable. At 2.0, responses can be wildly different and often incoherent. This non-deterministic behavior makes traditional software testing approaches inadequate.</p>
<p>The implications for evaluation are profound. How can one effectively test an LLM-powered system when the same prompt can yield radically different outputs based on a single parameter? Traditional testing relies on predictable inputs and outputs, but LLMs force us to grapple with probabilistic behavior. While lower temperatures may seem safer for critical applications, they don’t eliminate the underlying uncertainty - they merely mask it. This highlights the need for new evaluation paradigms that can handle both deterministic and probabilistic aspects of LLM behavior.</p>
</section>
</section>
<section id="emerging-properties">
<h2><a class="toc-backref" href="#id14" role="doc-backlink"><span class="section-number">4.2. </span>Emerging Properties</a><a class="headerlink" href="#emerging-properties" title="Permalink to this heading">¶</a></h2>
<p>Beyond their non-deterministic nature, LLMs present another fascinating challenge: emergent abilities that spontaneously arise as models scale up in size. These abilities - from basic question answering to complex reasoning - aren’t explicitly programmed but rather emerge “naturally” as the models grow larger and are trained on more data. This makes evaluation fundamentally different from traditional software testing, where capabilities are explicitly coded and can be tested against clear specifications.</p>
<figure class="align-center" id="id2">
<a class="bg-primary mb-1 reference internal image-reference" href="../_images/emerging.png"><img alt="Emerging Properties" class="bg-primary mb-1" src="../_images/emerging.png" style="width: 931.1999999999999px; height: 664.8px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.1 </span><span class="caption-text">Emergent abilities of large language models and the scale <span id="id1">[<a class="reference internal" href="#id4" title="Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. 2022. URL: https://arxiv.org/abs/2206.07682, arXiv:2206.07682.">WTB+22</a>]</span>.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#id2"><span class="std std-numref">Fig. 4.1</span></a> provides a list of emergent abilities of large language models and the scale. The relationship between model scale and emergent abilities follows a fascinating non-linear pattern. Below certain size thresholds, specific abilities may be completely absent from the model - it simply cannot perform certain tasks, no matter how much you try to coax them out. However, once the model reaches critical points in its scaling journey, these abilities can suddenly manifest in what researchers call a phase transition - a dramatic shift from inability to capability. This unpredictable emergence of capabilities stands in stark contrast to traditional software development, where features are deliberately implemented and can be systematically tested.</p>
<p>The implications for evaluation are profound. While conventional software testing relies on stable test suites and well-defined acceptance criteria, LLM evaluation must contend with a constantly shifting landscape of capabilities. What worked to evaluate a 7B parameter model may be completely inadequate for a 70B parameter model that has developed new emergent abilities. This dynamic nature of LLM capabilities forces us to fundamentally rethink our approach to testing and evaluation.</p>
</section>
<section id="problem-statement">
<h2><a class="toc-backref" href="#id15" role="doc-backlink"><span class="section-number">4.3. </span>Problem Statement</a><a class="headerlink" href="#problem-statement" title="Permalink to this heading">¶</a></h2>
<p>Consider a practical example that illustrates these challenges: building a customer support chatbot powered by an LLM. In traditional software development, you would define specific features (like handling refund requests or tracking orders) and write tests to verify each function. But with LLMs, you’re not just testing predefined features - you’re trying to evaluate emergent capabilities like understanding context, maintaining conversation coherence, and generating appropriate emotional responses.</p>
<p>This fundamental difference raises critical questions about evaluation:</p>
<ul class="simple">
<li><p>How do we measure capabilities that weren’t explicitly programmed?</p></li>
<li><p>How can we ensure consistent performance when abilities may suddenly emerge or evolve?</p></li>
<li><p>What metrics can capture both the technical accuracy and the subjective quality of responses?</p></li>
</ul>
<p>The challenge becomes even more complex when we consider that traditional software evaluation methods simply weren’t designed for these kinds of systems. We need new frameworks that can account for both the deterministic aspects we’re used to testing and the emergent properties that make LLMs unique. Let’s explore how LLM evaluation differs from traditional software testing across several key dimensions:</p>
<ul class="simple">
<li><p><strong>Capability Assessment vs Functional Testing</strong>: Traditional software testing validates specific functionality against predefined requirements. LLM evaluation must assess not necessiraly pre-defined “emergent properties” like reasoning, creativity, and language understanding that extend beyond explicit programming.</p></li>
<li><p><strong>Metrics and Measurement Challenges</strong>: While traditional software metrics can usually be precisely defined and measured, LLM evaluation often involves subjective qualities like “helpfulness” or “naturalness” that resist straightforward quantification. Even when we try to break these down into numeric scores, the underlying judgment remains inherently human and context-dependent.</p></li>
<li><p><strong>Dataset Contamination</strong>: Traditional software testing uses carefully crafted test cases with known inputs and expected outputs (e.g., unit tests, integration tests). In contrast, LLMs trained on massive internet-scale datasets risk having already seen and memorized evaluation examples during training, which can lead to artificially inflated performance scores. This requires careful dataset curation to ensure test sets are truly unseen by the model and rigorous cross-validation approaches.</p></li>
<li><p><strong>Benchmark Evolution</strong>: Traditional software maintains stable test suites over time. LLM benchmarks continuously evolve as capabilities advance, making longitudinal performance comparisons difficult and potentially obsoleting older evaluation methods.</p></li>
<li><p><strong>Human Evaluation Requirements</strong>: Traditional software testing automates most validation. LLM evaluation may demand significant human oversight to assess output quality, appropriateness, and potential biases through structured annotation and systematic review processes.</p></li>
</ul>
<table class="docutils align-default" id="evals-table">
<caption><span class="caption-number">Table 4.1 </span><span class="caption-text">Evals of Traditional Software vs LLMs</span><a class="headerlink" href="#evals-table" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Aspect</p></th>
<th class="head"><p>Traditional Software</p></th>
<th class="head"><p>LLMs</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Capability Assessment</strong></p></td>
<td><p>Validates specific functionality against requirements</p></td>
<td><p>May assess emergent properties like reasoning and creativity</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Metrics and Measurement</strong></p></td>
<td><p>Precisely defined and measurable metrics</p></td>
<td><p>Subjective qualities that resist straightforward quantification</p></td>
</tr>
<tr class="row-even"><td><p><strong>Dataset Contamination</strong></p></td>
<td><p>Uses carefully crafted test cases</p></td>
<td><p>Risk of memorized evaluation examples from training</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Benchmark Evolution</strong></p></td>
<td><p>Maintains stable test suites</p></td>
<td><p>Continuously evolving benchmarks as capabilities advance</p></td>
</tr>
<tr class="row-even"><td><p><strong>Human Evaluation</strong></p></td>
<td><p>Mostly automated validation</p></td>
<td><p>May require significant human oversight</p></td>
</tr>
</tbody>
</table>
</section>
<section id="evals-design">
<h2><a class="toc-backref" href="#id16" role="doc-backlink"><span class="section-number">4.4. </span>Evals Design</a><a class="headerlink" href="#evals-design" title="Permalink to this heading">¶</a></h2>
<p>First, it’s important to make a distinction between evaluating an LLM versus evaluating an LLM-based application (our focus). While the latter offers foundation capabilities and are typically general-purpose, the former is more specific and tailored to a particular use case. Here, we define an LLM-based application as a system that uses one or more LLMs to perform a specific task. More specifically, an LLM-based application is the combination of one or more LLM models, their associated prompts and parameters to solve a particular business problem.</p>
<p>That differentiation is important because it changes the scope of evaluation. LLMs are usually evaluated based on their capabilities, which include things like language understanding, reasoning and knowledge. LLM-based applications are evaluated based on their end-to-end functionality, performance, and how well they meet business requirements. That distinction has key implications for the design of evaluation systems:</p>
<ol class="arabic simple">
<li><p>Application requirements are closely tied to LLM evaluations</p></li>
<li><p>The same LLM can yield different results in different applications</p></li>
<li><p>Evaluation must align with business objectives</p></li>
<li><p>A great LLM doesn’t guarantee a great application!</p></li>
</ol>
<section id="conceptual-overview">
<h3><a class="toc-backref" href="#id17" role="doc-backlink"><span class="section-number">4.4.1. </span>Conceptual Overview</a><a class="headerlink" href="#conceptual-overview" title="Permalink to this heading">¶</a></h3>
<p><a class="reference internal" href="#conceptual"><span class="std std-numref">Fig. 4.2</span></a> demonstrates a conceptual design of key components of LLM Application evaluation.</p>
<figure class="align-center" id="conceptual">
<a class="reference internal image-reference" href="../_images/conceptual.png"><img alt="Conceptual Overview" src="../_images/conceptual.png" style="width: 992.8000000000001px; height: 424.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.2 </span><span class="caption-text">Conceptual overview of LLM-based application evaluation.</span><a class="headerlink" href="#conceptual" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>We observe three key components:
<strong>1. Examples (Input Dataset):</strong></p>
<ul class="simple">
<li><p>Input:  Query to LLM App, e.g. user message, input file, image, audio, etc.</p></li>
<li><p>Output: A reference expected outcome from the LLM application. Provide ground truth for comparison (<em>Optional</em>).</p></li>
<li><p>Purpose: Provides standardized test cases for evaluation.</p></li>
</ul>
<p><strong>2. LLM Application (Processing Layer):</strong></p>
<ul class="simple">
<li><p>Input: Test cases input from Examples</p></li>
<li><p>Output: Generated responses/results</p></li>
<li><p>Purpose:</p>
<ul>
<li><p>Represents different LLM implementations/vendors solving a specific task</p></li>
<li><p>Could be different models (GPT-4, Claude, PaLM, etc.)</p></li>
<li><p>Could be different configurations of same model</p></li>
<li><p>Could be different prompting strategies</p></li>
</ul>
</li>
</ul>
<p><strong>3. Evaluator (Assessment Layer):</strong></p>
<ul class="simple">
<li><p>Input:</p>
<ul>
<li><p>Outputs from LLM application</p></li>
<li><p>Reference data from Examples (<em>Optional</em>)</p></li>
</ul>
</li>
<li><p>Output: Individual scores for target LLM application</p></li>
<li><p>Purpose:</p>
<ul>
<li><p>Measures LLM Application performance across defined metrics</p></li>
<li><p>Applies standardized scoring criteria</p></li>
</ul>
</li>
</ul>
<p>Note that Examples must provide input data to the LLM Application for further evaluation. However, ground truth data is optional. We will return to this in more detail below, where we will see that ground truth data is not always available or practical. Additionally, there are approaches where one can evaluate LLM Applications without ground truth data.</p>
<p>A more general conceptual design is shown in <a class="reference internal" href="#conceptual-multi"><span class="std std-numref">Fig. 4.3</span></a>, where multiple LLM Applications are evaluated. This design allows for a more comprehensive evaluation of different configurations of LLM-based applications, e.g.:</p>
<ul class="simple">
<li><p>Fixing all application parameters and evaluating different LLM models with their default configurations</p></li>
<li><p>Fixing all parameters of an LLM model and evaluating different prompting strategies</p></li>
</ul>
<figure class="align-center" id="conceptual-multi">
<a class="reference internal image-reference" href="../_images/conceptual-multi.svg"><img alt="Conceptual Overview" height="588" src="../_images/conceptual-multi.svg" width="925" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4.3 </span><span class="caption-text">Conceptual overview of Multiple LLM-based applications evaluation.</span><a class="headerlink" href="#conceptual-multi" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>In this evaluation framework, the same inputs are provided to all LLM applications, ensuring that responses are evaluated consistently. Performance is quantified objectively for each LLM Application, and results are ranked for easy comparison. This design leads to two additional components:
<strong>1. Scores (Metrics Layer):</strong></p>
<ul class="simple">
<li><p>Input: Evaluation results from Evaluator</p></li>
<li><p>Output: Quantified performance metrics</p></li>
<li><p>Purpose:</p>
<ul>
<li><p>Represents performance in numerical form</p></li>
<li><p>Enables quantitative comparison among LLM applications</p></li>
<li><p>May include multiple metrics per LLM application</p></li>
</ul>
</li>
</ul>
<p><strong>2. Leaderboard (Ranking Layer):</strong></p>
<ul class="simple">
<li><p>Input: Scores per LLM application</p></li>
<li><p>Output: Ordered ranking of LLMs with scores</p></li>
<li><p>Purpose:</p>
<ul>
<li><p>Aggregates and ranks performances across LLM applications</p></li>
</ul>
</li>
</ul>
</section>
<section id="design-considerations">
<h3><a class="toc-backref" href="#id18" role="doc-backlink"><span class="section-number">4.4.2. </span>Design Considerations</a><a class="headerlink" href="#design-considerations" title="Permalink to this heading">¶</a></h3>
<p>The design of an LLM application evaluation system depends heavily on the specific use case and business requirements. Here we list important questions for planning an LLM application evaluation system pertaining to each of the key components previously discussed:</p>
<p><strong>1. Examples (Input Dataset):</strong></p>
<ul class="simple">
<li><p>What types of examples should be included in the test set?</p>
<ul>
<li><p>Does it cover all important use cases?</p></li>
<li><p>Are edge cases represented?</p></li>
<li><p>Is there a good balance of simple and complex examples?</p></li>
</ul>
</li>
<li><p>How do we ensure data quality?</p>
<ul>
<li><p>Are the examples representative of real-world scenarios?</p></li>
<li><p>Is there any bias in the test set?</p></li>
</ul>
</li>
<li><p>Should we have separate test sets for different business requirements?</p></li>
<li><p>Do we need human-validated ground truth for all examples?</p></li>
<li><p>Can we use synthetic data to augment the test set?</p></li>
<li><p>How can business updates be reflected in the dataset post-launch?</p></li>
</ul>
<p><strong>2. LLM Applications:</strong></p>
<ul class="simple">
<li><p>What aspects of each LLM app should be standardized for fair comparison?</p>
<ul>
<li><p>Prompt templates</p></li>
<li><p>Context length</p></li>
<li><p>Temperature and other parameters</p></li>
<li><p>Rate limiting and timeout handling</p></li>
</ul>
</li>
<li><p>What specific configurations impact business requirements?</p>
<ul>
<li><p>Which LLM application variations should be tested to maximize what we learn?</p></li>
<li><p>Which LLM capabilities provide the most value for the business and how can we measure that?</p></li>
</ul>
</li>
</ul>
<p><strong>3. Evaluator Design:</strong></p>
<ul class="simple">
<li><p>How do we define success for different types of tasks?</p>
<ul>
<li><p>Task-specific evaluation criteria</p></li>
<li><p>Objective metrics vs subjective assessment</p></li>
</ul>
</li>
<li><p>Should evaluation be automated or involve human review?</p>
<ul>
<li><p>Balance between automation and human judgment</p></li>
<li><p>Inter-rater reliability for human evaluation</p></li>
<li><p>Cost and scalability considerations</p></li>
</ul>
</li>
</ul>
<p><strong>4. Scoring System:</strong></p>
<ul class="simple">
<li><p>How should different metrics be weighted?</p>
<ul>
<li><p>Relative importance of different factors</p></li>
<li><p>Task-specific prioritization</p></li>
<li><p>Business requirements alignment</p></li>
</ul>
</li>
<li><p>Should scores be normalized or absolute?</p></li>
<li><p>How to handle failed responses?</p></li>
<li><p>Should we consider confidence scores from the LLMs?</p></li>
</ul>
<p><strong>5. Leaderboard/Ranking:</strong></p>
<ul class="simple">
<li><p>How often should rankings be updated?</p></li>
<li><p>Should ranking include confidence intervals?</p></li>
<li><p>How to handle ties or very close scores?</p></li>
<li><p>Should we maintain separate rankings for different:</p>
<ul>
<li><p>Business requirements</p></li>
<li><p>Cost tiers</p></li>
<li><p>LLM Models</p></li>
</ul>
</li>
</ul>
<p>Hollistically, you evaluation design should be built with scalability in mind to handle growing evaluation needs as the combination of (Example X LLM Applications X Evaluators X Scores X Leaderboards) may grow very fast, particularly for an organization that promotes rapid experimentation and iterative development (good properties!). Finally, one should keep in mind that the evaluation system itself requires validation to confirm its accuracy and reliability vis-a-vis the business requirements!</p>
</section>
</section>
<section id="key-components">
<h2><a class="toc-backref" href="#id19" role="doc-backlink"><span class="section-number">4.5. </span>Key Components</a><a class="headerlink" href="#key-components" title="Permalink to this heading">¶</a></h2>
<section id="metrics">
<h3><a class="toc-backref" href="#id20" role="doc-backlink"><span class="section-number">4.5.1. </span>Metrics</a><a class="headerlink" href="#metrics" title="Permalink to this heading">¶</a></h3>
<p>The choice of metric depends on the specific task and desired evaluation criteria. However, one can categorize metrics into two broad categories: <strong>intrinsic</strong> and <strong>extrinsic</strong>:</p>
<ul class="simple">
<li><p><strong>Intrinsic metrics</strong> focus on the model’s performance on its primary training objective, which is typically to predict the next token in a sequence.  Perplexity is a common intrinsic metric that measures how well the model predicts a given sample of text.</p></li>
<li><p><strong>Extrinsic metrics</strong> assess the model’s performance on various downstream tasks, which can range from question answering to code generation.  These metrics are not directly tied to the training objective, but they provide valuable insights into the model’s ability to generalise to real-world applications.</p></li>
</ul>
<p>Here, we are particularly interested in extrinsic metrics, since we are evaluating LLM-based applications.</p>
<p>Another way to think about metrics is in terms of the type of the task we evaluate:</p>
<ol class="arabic simple">
<li><p><strong>Discriminative Task</strong>:</p>
<ul class="simple">
<li><p>Involves distinguishing or classifying between existing data points.</p></li>
<li><p>Examples: Sentiment analysis, classification, or identifying whether a statement is true or false.</p></li>
</ul>
</li>
<li><p><strong>Generative Task</strong>:</p>
<ul class="simple">
<li><p>Involves creating or producing new data or outputs.</p></li>
<li><p>Examples: Text generation, image synthesis, or summarization.</p></li>
</ul>
</li>
</ol>
<p>For discriminative LLM-based applications may produce log-probabilities or discrete predictions, traditional machine learning metrics like accuracy, precision, recall, and F1 score can be applied. However, generative tasks may output text or images which require different evaluation approaches.</p>
<p>For generative tasks, a range of specialized metrics should be considered. These include match-based metrics such as exact match and prefix match, as well as metrics designed specifically for tasks like summarization and translation, including ROUGE, BLEU, and character n-gram comparisons. The selection of appropriate metrics should align with the specific requirements and characteristics of the task being evaluated. A detailed discussion of metric selection guidelines will be provided in a subsequent section.</p>
<p>In <a class="reference internal" href="#key-metrics"><span class="std std-numref">Table 4.2</span></a> we provide a short list of widely used extrinsic metrics that can be used to evaluate generative tasks of LLM-based applications, along with their definitions, use cases, and limitations.</p>
<table class="docutils align-default" id="key-metrics">
<caption><span class="caption-number">Table 4.2 </span><span class="caption-text">Key Metrics for Evaluating Generative Tasks</span><a class="headerlink" href="#key-metrics" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>Definition</p></th>
<th class="head"><p>Use Case</p></th>
<th class="head"><p>Limitations</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>BLEU (Bilingual Evaluation Understudy)</strong></p></td>
<td><p>Measures overlap of n-grams between generated text and reference text</p></td>
<td><p>Machine translation and text summarization</p></td>
<td><p>- Favors short outputs due to brevity penalty<br>- Insensitive to semantic meaning<br>- Requires high-quality reference texts</p></td>
</tr>
<tr class="row-odd"><td><p><strong>ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</strong></p></td>
<td><p>Measures overlap between n-grams, words, or sentences of generated text and references, focusing on recall</p></td>
<td><p>Text summarization tasks</p></td>
<td><p>- Biases toward long outputs<br>- Ignores semantic equivalence<br>- Heavily influenced by reference quality</p></td>
</tr>
<tr class="row-even"><td><p><strong>METEOR (Metric for Evaluation of Translation with Explicit ORdering)</strong></p></td>
<td><p>Considers synonyms, stemming, and paraphrases alongside n-gram overlap</p></td>
<td><p>Machine translation, where semantic equivalence matters</p></td>
<td><p>- Computationally expensive<br>- Subjective design of synonym/stemming databases</p></td>
</tr>
<tr class="row-odd"><td><p><strong>CIDEr (Consensus-based Image Description Evaluation)</strong></p></td>
<td><p>Measures n-gram overlap weighted by TF-IDF, tailored for image captioning</p></td>
<td><p>Image caption generation</p></td>
<td><p>- Limited applicability outside captioning<br>- Heavily reliant on corpus statistics</p></td>
</tr>
<tr class="row-even"><td><p><strong>TER (Translation Edit Rate)</strong></p></td>
<td><p>Computes number of edits needed to convert hypothesis into reference text</p></td>
<td><p>Translation quality evaluation</p></td>
<td><p>- Doesn’t consider semantic correctness<br>- Penalizes valid paraphrasing</p></td>
</tr>
<tr class="row-odd"><td><p><strong>BERTScore</strong></p></td>
<td><p>Uses contextual embeddings from pre-trained BERT to calculate token similarity</p></td>
<td><p>Tasks requiring semantic equivalence</p></td>
<td><p>- High computational cost<br>- Performance varies with model used</p></td>
</tr>
<tr class="row-even"><td><p><strong>SPICE (Semantic Propositional Image Caption Evaluation)</strong></p></td>
<td><p>Focuses on scene graphs in image captions to evaluate semantic content</p></td>
<td><p>Image captioning with emphasis on semantic accuracy</p></td>
<td><p>- Designed only for image captions<br>- Less effective in purely textual tasks</p></td>
</tr>
</tbody>
</table>
<section id="example-bleu-and-rouge-for-sec-filing-summarization">
<h4><a class="toc-backref" href="#id21" role="doc-backlink"><span class="section-number">4.5.1.1. </span>Example: BLEU and ROUGE for SEC Filing Summarization</a><a class="headerlink" href="#example-bleu-and-rouge-for-sec-filing-summarization" title="Permalink to this heading">¶</a></h4>
<p>When working with SEC filings, you may want to evaluate the quality of summaries or extracted key sections against reference summaries (e.g. analyst-prepared highlights).</p>
<p>For that purpose, we can use BLEU and ROUGE scores to evaluate the quality of generated summaries against reference summaries.</p>
<p>We will model our simple metrics-based evaluator with the following components:</p>
<ul class="simple">
<li><p>Input: Generated summary and reference summary</p></li>
<li><p>Output: Dictionary with scores for BLEU, ROUGE_1, and ROUGE_2</p></li>
<li><p>Purpose: Evaluate our LLM-based application - SEC filing summary generator</p></li>
</ul>
<p>A <em>Reference Summary</em> represents the “ideal” summary. It could be prepared by humanas, e.g. expert analysts, or machine-generated.</p>
<p>In our example, we are particularly interested in evaluating the quality of summaries generated by different (smaller and cheaper) LLM models compared to a <em>benchmark model</em> (larger and more expensive). We will use the following setup:</p>
<ul class="simple">
<li><p>Benchmark model: <code class="docutils literal notranslate"><span class="pre">gpt-4o</span></code></p></li>
<li><p>Test models: <code class="docutils literal notranslate"><span class="pre">gpt-4o-mini</span></code>, <code class="docutils literal notranslate"><span class="pre">gpt-4-turbo</span></code>, <code class="docutils literal notranslate"><span class="pre">gpt-3.5-turbo</span></code></p></li>
</ul>
<p>First, we define <code class="docutils literal notranslate"><span class="pre">evaluate_summaries</span></code>, a function that calculates BLEU and ROUGE scores to assess text generation quality. It takes a generated summary and reference summary as input, processes them and returns a dictionary with three scores: BLEU (n-gram overlap), ROUGE_1 (unigram comparison), and ROUGE_2 (bigram comparison). This enables quantitative comparison of generated summaries against reference texts. We use HuggingFaces’ <code class="docutils literal notranslate"><span class="pre">evaluate</span></code> library to load the metrics.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>evaluate<span class="w"> </span>absl-py<span class="w"> </span>rouge_score
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">evaluate</span>
<span class="k">def</span> <span class="nf">evaluate_summaries</span><span class="p">(</span><span class="n">generated_summary</span><span class="p">,</span> <span class="n">reference_summary</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluate generated summaries against reference summaries using multiple metrics.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        generated_summary (str): The summary generated by the model</span>
<span class="sd">        reference_summary (str): The reference/ground truth summary</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">        dict: Dictionary containing scores for different metrics</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Initialize metrics</span>
    <span class="n">bleu</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;google_bleu&quot;</span><span class="p">)</span>
    <span class="n">rouge</span> <span class="o">=</span> <span class="n">evaluate</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;rouge&quot;</span><span class="p">)</span>
    
    <span class="c1"># Format inputs for BLEU (expects list of str for predictions and list of list of str for references)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span><span class="n">generated_summary</span><span class="p">]</span>
    <span class="n">references</span> <span class="o">=</span> <span class="p">[</span><span class="n">reference_summary</span><span class="p">]</span>
    
    <span class="c1"># Compute BLEU score</span>
    <span class="n">bleu_score</span> <span class="o">=</span> <span class="n">bleu</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="p">[</span><span class="n">references</span><span class="p">])</span>
    
    <span class="c1"># Compute ROUGE scores</span>
    <span class="n">rouge_score</span> <span class="o">=</span> <span class="n">rouge</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">predictions</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">references</span><span class="p">)</span>
    
    <span class="c1"># Compute Character metric    </span>
    <span class="c1"># Combine all scores into a single dictionary</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;bleu&#39;</span><span class="p">:</span> <span class="n">bleu_score</span><span class="p">[</span><span class="s2">&quot;google_bleu&quot;</span><span class="p">],</span>
        <span class="s1">&#39;rouge1&#39;</span><span class="p">:</span> <span class="n">rouge_score</span><span class="p">[</span><span class="s1">&#39;rouge1&#39;</span><span class="p">],</span>
        <span class="s1">&#39;rouge2&#39;</span><span class="p">:</span> <span class="n">rouge_score</span><span class="p">[</span><span class="s1">&#39;rouge2&#39;</span><span class="p">]</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="n">scores</span>
</pre></div>
</div>
</div>
</div>
<p>For instance, <code class="docutils literal notranslate"><span class="pre">evaluate_summaries</span></code> can be used to compare two arbitrary sentences and returns a dictionary with our chosen metrics:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sentence1</span> <span class="o">=</span> <span class="s2">&quot;the cat sat on the mat&quot;</span>
<span class="n">sentence2</span> <span class="o">=</span> <span class="s2">&quot;the cat ate the mat&quot;</span>
<span class="n">evaluate_summaries</span><span class="p">(</span><span class="n">sentence1</span><span class="p">,</span> <span class="n">sentence2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;bleu&#39;: 0.3333333333333333,
 &#39;rouge1&#39;: 0.7272727272727272,
 &#39;rouge2&#39;: 0.4444444444444445}
</pre></div>
</div>
</div>
</div>
<p>Next, we define <code class="docutils literal notranslate"><span class="pre">generate_summary</span></code>, our simple LLM-based SEC filing summirizer application using OpenAI’s API. It takes an arbitrary <code class="docutils literal notranslate"><span class="pre">model</span></code>, and an <code class="docutils literal notranslate"><span class="pre">input</span></code> text and returns the corresponding LLM’s response with a summary.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">generate_summary</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate a summary of input using a given model</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">TASK</span> <span class="o">=</span> <span class="s2">&quot;Generate a 1-liner summary of the following excerpt from an SEC filing.&quot;</span>

    <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    ROLE: You are an expert analyst tasked with summarizing SEC filings.</span>
<span class="s2">    TASK: </span><span class="si">{</span><span class="n">TASK</span><span class="si">}</span>
<span class="s2">    &quot;&quot;&quot;</span>
    
    <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">},</span>
                 <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="nb">input</span><span class="p">}]</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
</pre></div>
</div>
</div>
</div>
<p>Now, we define a function <code class="docutils literal notranslate"><span class="pre">evaluate_summary_models</span></code> - our benchmark evaluator - that compares text summaries generated by different language models against a benchmark model. Here’s what it does:</p>
<ol class="arabic simple">
<li><p>Takes a benchmark model, list of test models, prompt, and input text</p></li>
<li><p>Generates a reference summary using the benchmark model and our <code class="docutils literal notranslate"><span class="pre">generate_summary</span></code> function</p></li>
<li><p>Generates summaries from all test models using <code class="docutils literal notranslate"><span class="pre">generate_summary</span></code> function</p></li>
<li><p>Evaluates each test model’s summary against the benchmark using <code class="docutils literal notranslate"><span class="pre">evaluate_summaries</span></code></p></li>
<li><p>Returns evaluation results and the generated summaries</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">evaluate_summary_models</span><span class="p">(</span><span class="n">model_benchmark</span><span class="p">,</span> <span class="n">models_test</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluate summaries generated by multiple models</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">benchmark_summary</span> <span class="o">=</span> <span class="n">generate_summary</span><span class="p">(</span><span class="n">model_benchmark</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>

    <span class="c1"># Generate summaries for all test models using list comprehension</span>
    <span class="n">model_summaries</span> <span class="o">=</span> <span class="p">[</span><span class="n">generate_summary</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models_test</span><span class="p">]</span>
    
    <span class="c1"># Evaluate each model&#39;s summary against the benchmark</span>
    <span class="n">evaluation_results</span> <span class="o">=</span> <span class="p">[</span><span class="n">evaluate_summaries</span><span class="p">(</span><span class="n">summary</span><span class="p">,</span> <span class="n">benchmark_summary</span><span class="p">)</span> <span class="k">for</span> <span class="n">summary</span> <span class="ow">in</span> <span class="n">model_summaries</span><span class="p">]</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">evaluation_results</span><span class="p">,</span> <span class="n">model_summaries</span><span class="p">,</span> <span class="n">benchmark_summary</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Now, we are ready to run our benchmark evaluation. We define a benchmark model and a list of test models and then evaluate each test model’s summary against the benchmark. We also print the generated summaries for each model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_benchmark</span> <span class="o">=</span> <span class="s2">&quot;gpt-4o&quot;</span>
<span class="n">models_test</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">,</span> <span class="s2">&quot;gpt-4-turbo&quot;</span><span class="p">,</span> <span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">evals</span><span class="p">,</span> <span class="n">model_summaries</span><span class="p">,</span> <span class="n">benchmark_summary</span> <span class="o">=</span> <span class="n">evaluate_summary_models</span><span class="p">(</span><span class="n">model_benchmark</span><span class="p">,</span> <span class="n">models_test</span><span class="p">,</span> <span class="n">sec_filing</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">benchmark_summary</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&quot;Apple Inc.&#39;s 10-K filing for the fiscal year ending September 28, 2024, outlines its operational and financial condition, detailing the company&#39;s diverse product lines, market activities, and compliance with SEC requirements.&quot;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print each model name and its summary</span>
<span class="k">for</span> <span class="n">model</span><span class="p">,</span> <span class="n">summary</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">models_test</span><span class="p">,</span> <span class="n">model_summaries</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s2">: </span><span class="se">\n</span><span class="s2"> </span><span class="si">{</span><span class="n">summary</span><span class="si">}</span><span class="s2"> </span><span class="se">\n</span><span class="s2">---------------&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>gpt-4o-mini: 
 Apple Inc. filed its Annual Report on Form 10-K for the fiscal year ending September 28, 2024, detailing its business operations, risks, and financial condition. 
---------------
gpt-4-turbo: 
 Apple Inc.&#39;s Form 10-K for the fiscal year ended September 28, 2024, details its annual report as a well-known seasoned issuer, confirming compliance with SEC regulations and reporting on stock performances, securities, and corporate governance, while also including forward-looking statements subject to various risks. 
---------------
gpt-3.5-turbo: 
 Apple Inc. filed its Form 10-K with the SEC, revealing financial information for the fiscal year ended September 28, 2024, including details on its products and market performance. 
---------------
</pre></div>
</div>
</div>
</div>
<p>The benchmark summary from <code class="docutils literal notranslate"><span class="pre">gpt-4o</span></code> provides a balanced overview of the analyzed excerpt from Apple’s 10-K filing, focusing on operational status, financial condition, product lines, and regulatory compliance.</p>
<p>When comparing our test models against the benchmark, we observe that:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">gpt-4o-mini</span></code> provides a concise yet comprehensive summary that closely aligns with the benchmark’s core message. While it omits product lines, it effectively captures the essential elements of the filing including business operations, risks, and financial condition. Its brevity and focus look (subjectively) similar to our benchmark model.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpt-4-turbo</span></code> performs adequately but tends toward verbosity. While it includes relevant information about SEC compliance, it introduces peripheral details about seasoned issuer status and forward-looking statements. The additional complexity makes the summary less focused than gpt-4o-mini’s version.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpt-3.5-turbo</span></code> looks quite different from the benchmark. Its summary, while factually correct, is overly simplified and misses key aspects of the filing. The model captures basic financial information but fails to convey the breadth of operational and compliance details present in the benchmark summary.</p></li>
</ul>
<p>Of course, the above evaluation is only based on a single example and is heavily subjective. It’s a “vibe check” on our evaluation results. Now, for an objective analysis, we can look at the quantitative metrics we have chosen and use the <code class="docutils literal notranslate"><span class="pre">visualize_prompt_comparison</span></code> function we write below to visualize the performance of our test models across our predefined quantitative metrics.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>matplotlib
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">visualize_prompt_comparison</span><span class="p">(</span><span class="n">evaluation_results</span><span class="p">,</span> <span class="n">model_names</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a radar plot comparing different prompt variations</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        evaluation_results (list): List of dictionaries containing evaluation metrics</span>
<span class="sd">        model_names (list): List of names for each prompt variation</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">evaluate.visualization</span> <span class="kn">import</span> <span class="n">radar_plot</span>
    
    <span class="c1"># Format data for visualization</span>
    <span class="n">plot</span> <span class="o">=</span> <span class="n">radar_plot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">evaluation_results</span><span class="p">,</span> <span class="n">model_names</span><span class="o">=</span><span class="n">model_names</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">plot</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create and display visualization</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">visualize_prompt_comparison</span><span class="p">(</span><span class="n">evals</span><span class="p">,</span> <span class="n">models_test</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_1652501/940173201.py:3: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  plot.show()
</pre></div>
</div>
<img alt="../_images/08a72483c61f624c6ca08db7d58620028acadc7df987d17f40ae7c6e04fa94d2.png" src="../_images/08a72483c61f624c6ca08db7d58620028acadc7df987d17f40ae7c6e04fa94d2.png" />
</div>
</div>
<p>Results demonstrate that tested models perform quite differently on our predefined metrics. The evaluation metrics puts <code class="docutils literal notranslate"><span class="pre">gpt-4o-mini</span></code> as the closest aligned to the benchmark, followed by gpt-4-turbo, and gpt-3.5-turbo showing the largest deviation. This suggests that <code class="docutils literal notranslate"><span class="pre">gpt-4o-mini</span></code> is the best model for this task at least on the metrics we have chosen and for the set of models we have tested.</p>
<p>While evaluating language model outputs inherently involves subjective judgment, establishing a high-quality benchmark model and using quantifiable metrics provide a more objective framework for comparing model performance. This approach transforms an otherwise qualitative assessment into a measurable, data-driven evaluation process.</p>
</section>
<section id="considerations">
<h4><a class="toc-backref" href="#id22" role="doc-backlink"><span class="section-number">4.5.1.2. </span>Considerations</a><a class="headerlink" href="#considerations" title="Permalink to this heading">¶</a></h4>
<p>While these metrics provide quantifiable measures of performance, they have limitations:</p>
<ul class="simple">
<li><p><strong>Task-specific nature</strong>:  Chosen set of metrics might not fully capture the nuances of complex generative-based tasks, especially those involving subjective human judgment.</p></li>
<li><p><strong>Sensitivity to data distribution</strong>: Performance on these metrics can be influenced by the specific dataset used for evaluation, which might not represent real-world data distribution.</p></li>
<li><p><strong>Inability to assess reasoning or factual accuracy</strong>: These metrics primarily focus on surface-level matching and might not reveal the underlying reasoning process of the LLM or its ability to generate factually correct information.</p></li>
</ul>
<p>In conclusion, selecting an appropriate extrinsic metrics set depends on the specific task, underlying business requirements and desired evaluation granularity.  Understanding the limitations of these metrics can provide a more comprehensive assessment of LLM performance in real-world applications.</p>
<p>To address these limitations, alternative approaches like <strong>human-based evaluation</strong> and <strong>model-based evaluation</strong> are often used, which will be discussed in the following sections.</p>
</section>
</section>
<section id="evaluators">
<h3><a class="toc-backref" href="#id23" role="doc-backlink"><span class="section-number">4.5.2. </span>Evaluators</a><a class="headerlink" href="#evaluators" title="Permalink to this heading">¶</a></h3>
<section id="model-based-evaluation">
<h4><a class="toc-backref" href="#id24" role="doc-backlink"><span class="section-number">4.5.2.1. </span>Model-Based Evaluation</a><a class="headerlink" href="#model-based-evaluation" title="Permalink to this heading">¶</a></h4>
<p>Traditional metrics like BLEU or ROUGE often fall short in capturing the nuanced, contextual, and creative outputs of LLMs. As an alternative we can consider a “Model-based evaluation” approach, also known as <strong>LLM-as-a-Judge</strong>. This is an approach that leverages language models themselves to assess the quality of outputs from other language models. This method involves using a model (often a more capable one) to act as an automated judge, evaluating aspects like accuracy, coherence, and relevance of generated content. Unlike traditional metrics that rely on exact matching or statistical measures, model-based evaluation can capture nuanced aspects of language and provide more contextual assessment.</p>
<p>An LLM-as-a-Judge approach involves similar steps to the general evaluation design we have discussed:</p>
<ol class="arabic simple">
<li><p><strong>Define Evaluation Criteria</strong>: Establish clear benchmarks, such as relevance, coherence, accuracy, and fluency.</p></li>
<li><p><strong>Prepare Prompts</strong>: Craft effective prompts to guide the LLM in evaluating content against the criteria.</p></li>
<li><p><strong>Run Evaluations</strong>: Use the judge model to score outputs. Consider using a large and/or more capable model as a judge to provide more nuanced assessments.</p></li>
<li><p><strong>Aggregate and Analyze Results</strong>: Interpret scores to refine applications.</p></li>
</ol>
<p>The key difference compared to a metrics-based approach is that an LLM-as-a-Judge approach allows for more nuanced assessments by considering criteria defined in natural language. This enables evaluation of human-centric aspects like creativity, logical flow, and contextual appropriateness that are difficult to capture with traditional metrics. The judge model can understand and assess complex evaluation criteria expressed in plain language, similar to how a human evaluator would interpret and apply assessment guidelines. Additionally, LLM-as-a-Judge approaches has an added step of prompt engineering to define the evaluation criteria and prompts.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span>

<span class="k">class</span> <span class="nc">JudgeEvaluation</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">expertise</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">coherence</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">fluency</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">similarity</span><span class="p">:</span> <span class="nb">int</span>
<span class="k">def</span> <span class="nf">evaluate_with_llm</span><span class="p">(</span><span class="n">judge_model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">candidate_summary</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">reference_summary</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Use an LLM to evaluate a candidate summary against a reference summary.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        judge_model (str): Name of the model to use as the judge.</span>
<span class="sd">        candidate_summary (str): Generated summary to evaluate.</span>
<span class="sd">        reference_summary (str): Ground truth or benchmark summary.</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        dict: Dictionary containing evaluation scores for specified criteria.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    ROLE: You are an expert evaluator of SEC Filing summaries. Evaluate the following candidate summary against the reference summary on a scale of 1 to 10 for the following criteria:</span>
<span class="s2">    - Expertise: Does the summary look like it was written by an expert analyst?</span>
<span class="s2">    - Coherence: Is the candidate summary logically organized and easy to understand?</span>
<span class="s2">    - Fluency: Is the language of the candidate summary clear and grammatically correct?</span>
<span class="s2">    - Similarity: How similar is the candidate summary compared to the reference summary?</span>

<span class="s2">    Reference Summary:</span>
<span class="s2">    &quot;</span><span class="si">{</span><span class="n">reference_summary</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="s2">    Candidate Summary:</span>
<span class="s2">    &quot;</span><span class="si">{</span><span class="n">candidate_summary</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="s2">    Provide scores in this format:</span>
<span class="s2">    Expertise: X, Coherence: Y, Fluency: Z, Similarity: W</span>
<span class="s2">    &quot;&quot;&quot;</span>
    <span class="n">completion</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">judge_model</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}],</span>
        <span class="n">response_format</span><span class="o">=</span><span class="n">JudgeEvaluation</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">completion</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">parsed</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">evaluate_summary_models</span><span class="p">(</span><span class="n">judge_model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">benchmark_model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">test_models</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">input_text</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Evaluate summaries generated by multiple models using an LLM-as-a-Judge approach.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        judge_model (str): Name of the model to use as the judge.</span>
<span class="sd">        benchmark_model (str): Name of the benchmark model.</span>
<span class="sd">        test_models (list): List of model names to test.</span>
<span class="sd">        input_text (str): Input text for summarization.</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">        tuple: Evaluation results, model summaries, benchmark summary.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">benchmark_summary</span> <span class="o">=</span> <span class="n">generate_summary</span><span class="p">(</span><span class="n">benchmark_model</span><span class="p">,</span> <span class="n">input_text</span><span class="p">)</span>
    <span class="n">model_summaries</span> <span class="o">=</span> <span class="p">[</span><span class="n">generate_summary</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_text</span><span class="p">)</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">test_models</span><span class="p">]</span>

    <span class="n">evaluation_results</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">evaluate_with_llm</span><span class="p">(</span><span class="n">judge_model</span><span class="p">,</span> <span class="n">summary</span><span class="p">,</span> <span class="n">benchmark_summary</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">summary</span> <span class="ow">in</span> <span class="n">model_summaries</span>
    <span class="p">]</span>

    <span class="k">return</span> <span class="n">evaluation_results</span><span class="p">,</span> <span class="n">model_summaries</span><span class="p">,</span> <span class="n">benchmark_summary</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example Usage</span>
<span class="n">model_benchmark</span> <span class="o">=</span> <span class="s2">&quot;gpt-4o&quot;</span>
<span class="n">models_test</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">,</span> <span class="s2">&quot;gpt-4-turbo&quot;</span><span class="p">,</span> <span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">]</span>
<span class="n">judge_model</span> <span class="o">=</span> <span class="s2">&quot;gpt-4o&quot;</span>

<span class="n">evals</span><span class="p">,</span> <span class="n">model_summaries</span><span class="p">,</span> <span class="n">benchmark_summary</span> <span class="o">=</span> <span class="n">evaluate_summary_models</span><span class="p">(</span>
    <span class="n">judge_model</span><span class="p">,</span> <span class="n">model_benchmark</span><span class="p">,</span> <span class="n">models_test</span><span class="p">,</span> <span class="n">sec_filing</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">benchmark_summary</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&quot;Apple Inc.&#39;s annual report for the fiscal year ending September 28, 2024, details its business operations, financial condition, and product lines, including iPhones, Macs, iPads, and wearables, and incorporates forward-looking statements regarding its future performance.&quot;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_summaries</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;Apple Inc. filed its annual Form 10-K report for the fiscal year ended September 28, 2024, detailing its business operations, product lines, and financial performance.&#39;,
 &quot;This Form 10-K filing by Apple Inc. for the fiscal year ended September 28, 2024, is an annual report detailing the company&#39;s financial performance, including registered securities, compliance with SEC reporting standards, and contains sections on business operations, risk factors, financial data, and management analysis.&quot;,
 &#39;Apple Inc., a California-based technology company, reported an aggregate market value of approximately $2.6 trillion held by non-affiliates, with 15.1 billion shares of common stock outstanding as of October 18, 2024.&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">evals</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[JudgeEvaluation(expertise=7, coherence=8, fluency=8, similarity=7),
 JudgeEvaluation(expertise=7, coherence=7, fluency=8, similarity=5),
 JudgeEvaluation(expertise=4, coherence=5, fluency=7, similarity=2)]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert evaluation objects to dictionaries</span>
<span class="n">evals_list</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;expertise&quot;</span><span class="p">:</span> <span class="nb">eval</span><span class="o">.</span><span class="n">expertise</span><span class="p">,</span>
        <span class="s2">&quot;coherence&quot;</span><span class="p">:</span> <span class="nb">eval</span><span class="o">.</span><span class="n">coherence</span><span class="p">,</span> 
        <span class="s2">&quot;fluency&quot;</span><span class="p">:</span> <span class="nb">eval</span><span class="o">.</span><span class="n">fluency</span><span class="p">,</span>
        <span class="s2">&quot;similarity&quot;</span><span class="p">:</span> <span class="nb">eval</span><span class="o">.</span><span class="n">similarity</span>
    <span class="p">}</span>
    <span class="k">for</span> <span class="nb">eval</span> <span class="ow">in</span> <span class="n">evals</span>
<span class="p">]</span>

<span class="c1"># Visualize results</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">visualize_prompt_comparison</span><span class="p">(</span><span class="n">evals_list</span><span class="p">,</span> <span class="n">models_test</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_1652501/1775618912.py:14: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  plot.show()
</pre></div>
</div>
<img alt="../_images/a49bcd9c801d9b420b188b13fe009f1b93a733780c10a27b655fc7cd475d16a9.png" src="../_images/a49bcd9c801d9b420b188b13fe009f1b93a733780c10a27b655fc7cd475d16a9.png" />
</div>
</div>
<p>Limitations</p>
<ol class="arabic simple">
<li><p>Potential for the evaluator model to share biases with the model being evaluated</p></li>
<li><p>Difficulty in validating the evaluation model’s own judgment</p></li>
<li><p>Computational overhead of running additional model inference</p></li>
</ol>
<ul class="simple">
<li><p><strong>Bias and Limitations of LLMs</strong>: If the judge model has biases, these might affect evaluations.</p></li>
<li><p><strong>Interpretability</strong>: LLMs may not provide clear rationales for their scores.</p></li>
<li><p><strong>Dependency on Prompt Quality</strong>: Results heavily depend on well-crafted prompts.</p></li>
</ul>
<p>The LLM-as-a-Judge strategy offers a scalable, nuanced, and efficient solution to evaluate LLM-based applications. While it does not entirely replace human judgment, it significantly augments evaluation workflows, especially in scenarios requiring consistency and scalability. Future improvements could include integrating human oversight and refining LLMs for domain-specific evaluation tasks.</p>
</section>
<section id="human-based-evaluation">
<h4><a class="toc-backref" href="#id25" role="doc-backlink"><span class="section-number">4.5.2.2. </span>Human-Based Evaluation</a><a class="headerlink" href="#human-based-evaluation" title="Permalink to this heading">¶</a></h4>
<p>Human assessors can judge aspects like fluency, coherence, and factual accuracy, providing a more comprehensive evaluation. However, human evaluation can be subjective and resource-intensive.</p>
</section>
</section>
<section id="leaderboard">
<h3><a class="toc-backref" href="#id26" role="doc-backlink"><span class="section-number">4.5.3. </span>Leaderboard</a><a class="headerlink" href="#leaderboard" title="Permalink to this heading">¶</a></h3>
<p>Benchmark datasets provide a standardised way to evaluate the performance of LLMs on a variety of downstream tasks. Some popular benchmark datasets include:</p>
<ul class="simple">
<li><p><strong>GLUE (General Language Understanding Evaluation)</strong>: A collection of diverse natural language understanding tasks, such as sentiment analysis, question answering, and textual entailment. GLUE scores provide a holistic view of a model’s language understanding capabilities.</p></li>
<li><p><strong>SuperGLUE</strong>: A successor to GLUE with more challenging tasks designed to push the boundaries of language understanding.</p></li>
<li><p><strong>BIG-bench</strong>: A collaborative benchmark with a wide range of tasks, encompassing language modeling, reasoning, and common sense.</p></li>
<li><p><strong>HELM (Holistic Evaluation of Language Models)</strong>: A comprehensive evaluation framework that considers multiple factors, including accuracy, robustness, fairness, and efficiency.</p></li>
<li><p><strong>FLASK</strong>: A benchmark focused on evaluating the factual accuracy and consistency of language models.</p></li>
<li><p><strong>Massive Multitask Language Understanding (MMLU):</strong> This benchmark tests a model’s knowledge and reasoning abilities across 57 subjects, including STEM, humanities, and social sciences.</p></li>
<li><p><strong>HumanEval:</strong> This dataset evaluates code generation capabilities by asking a model to write code based on a given description.</p></li>
<li><p><strong>LiveBench:</strong>  This benchmark attempts to mitigate the problem of data contamination by releasing new questions monthly.</p></li>
</ul>
<p>However, benchmark datasets have limitations.  It can be difficult to prevent data from the benchmarks leaking into the training data, which can inflate the model’s performance.  Additionally, some benchmarks may contain incorrect answers or ambiguous questions.  It’s also important to remember that benchmark performance does not always translate directly to real-world performance.</p>
</section>
</section>
<section id="tools">
<h2><a class="toc-backref" href="#id27" role="doc-backlink"><span class="section-number">4.6. </span>Tools</a><a class="headerlink" href="#tools" title="Permalink to this heading">¶</a></h2>
</section>
<section id="references">
<h2><a class="toc-backref" href="#id28" role="doc-backlink"><span class="section-number">4.7. </span>References</a><a class="headerlink" href="#references" title="Permalink to this heading">¶</a></h2>
<div class="docutils container" id="id3">
<div class="citation" id="id4" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">WTB+22</a><span class="fn-bracket">]</span></span>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. 2022. URL: <a class="reference external" href="https://arxiv.org/abs/2206.07682">https://arxiv.org/abs/2206.07682</a>, <a class="reference external" href="https://arxiv.org/abs/2206.07682">arXiv:2206.07682</a>.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

          </div>
          <div class="page-nav">
            <div class="inner"><ul class="page-nav">
  <li class="prev">
    <a href="structured_output.html"
       title="previous chapter">← <span class="section-number">3. </span>Wrestling with Structured Output</a>
  </li>
</ul><div class="footer" role="contentinfo">
    <br>
    Created using <a href="http://sphinx-doc.org/">Sphinx</a> 6.2.1 with <a href="https://github.com/schettino72/sphinx_press_theme">Press Theme</a> 0.9.1.
</div>
            </div>
          </div>
      </page>
    </div></div>
    
    
  </body>
</html>