<!DOCTYPE html>
<html  lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1"><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

      <title>3. Wrestling with Structured Output</title>
    
          <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
          <link rel="stylesheet" href="../_static/theme.css " type="text/css" />
          <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
          <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
          <link rel="stylesheet" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
          <link rel="stylesheet" href="../_static/sphinx-thebe.css" type="text/css" />
          <link rel="stylesheet" href="../_static/sphinx-design.4cbf315f70debaebd550c87a6162cf0f.min.css" type="text/css" />
      
      <!-- sphinx script_files -->
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script src="../_static/scripts/sphinx-book-theme.js"></script>
        <script>let toggleHintShow = 'Click to show';</script>
        <script>let toggleHintHide = 'Click to hide';</script>
        <script>let toggleOpenOnPrint = 'true';</script>
        <script src="../_static/togglebutton.js"></script>
        <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
        <script src="../_static/design-tabs.js"></script>
        <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
        <script async="async" src="../_static/sphinx-thebe.js"></script>

      
      <!-- bundled in js (rollup iife) -->
      <!-- <script src="../_static/theme-vendors.js"></script> -->
      <script src="../_static/theme.js" defer></script>
      <link rel="canonical" href="https://souzatharsis.github.io/tamingllms/notebooks/structured_output.html" />
    
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="next" title="4. Challenges of Evaluating LLM-based Applications" href="evals.html" />
  <link rel="prev" title="2. Output Size Limitations" href="output_size_limit.html" /> 
  </head>

  <body>
    <div id="app">
    <div class="theme-container" :class="pageClasses"><navbar @toggle-sidebar="toggleSidebar">
  <router-link to="../markdown/toc.html" class="home-link">
    
      <span class="site-name">tamingLLMs</span>
    
  </router-link>

  <div class="links">
    <navlinks class="can-hide">



  
    <div class="nav-item">
      <a href="https://github.com/souzatharsis/tamingllms"
        class="nav-link external">
          Github <outboundlink></outboundlink>
      </a>
    </div>
  

    </navlinks>
  </div>
</navbar>

      
      <div class="sidebar-mask" @click="toggleSidebar(false)">
      </div>
        <sidebar @toggle-sidebar="toggleSidebar">
          
          <navlinks>
            



  
    <div class="nav-item">
      <a href="https://github.com/souzatharsis/tamingllms"
        class="nav-link external">
          Github <outboundlink></outboundlink>
      </a>
    </div>
  

            
          </navlinks><div id="searchbox" class="searchbox" role="search">
  <div class="caption"><span class="caption-text">Quick search</span>
    <div class="searchformwrapper">
      <form class="search" action="../search.html" method="get">
        <input type="text" name="q" />
        <input type="submit" value="Search" />
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
      </form>
    </div>
  </div>
</div><div class="sidebar-links" role="navigation" aria-label="main navigation">
  
    <div class="sidebar-group">
      <p class="caption">
        <span class="caption-text"><a href="../markdown/toc.html#taming-large-language-models">taming large language models</a></span>
      </p>
      <ul class="current">
        
          <li class="toctree-l1 ">
            
              <a href="../markdown/intro.html" class="reference internal ">Introduction</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="output_size_limit.html" class="reference internal ">Output Size Limitations</a>
            

            
          </li>

        
          <li class="toctree-l1 current">
            
              <a href="#" class="reference internal current">Wrestling with Structured Output</a>
            

            
              <ul>
                
                  <li class="toctree-l2"><a href="#the-structured-output-challenges" class="reference internal">The Structured Output Challenges</a></li>
                
                  <li class="toctree-l2"><a href="#problem-statement" class="reference internal">Problem Statement</a></li>
                
                  <li class="toctree-l2"><a href="#solutions" class="reference internal">Solutions</a></li>
                
                  <li class="toctree-l2"><a href="#discussion" class="reference internal">Discussion</a></li>
                
                  <li class="toctree-l2"><a href="#conclusion" class="reference internal">Conclusion</a></li>
                
                  <li class="toctree-l2"><a href="#acknowledgements" class="reference internal">Acknowledgements</a></li>
                
                  <li class="toctree-l2"><a href="#references" class="reference internal">References</a></li>
                
              </ul>
            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="evals.html" class="reference internal ">Challenges of Evaluating LLM-based Applications</a>
            

            
          </li>

        
      </ul>
    </div>
  
</div>
        </sidebar>

      <page>
          <div class="body-header" role="navigation" aria-label="navigation">
  
  <ul class="breadcrumbs">
    <li><a href="../markdown/toc.html">Docs</a> &raquo;</li>
    
    <li><span class="section-number">3. </span>Wrestling with Structured Output</li>
  </ul>
  

  <ul class="page-nav">
  <li class="prev">
    <a href="output_size_limit.html"
       title="previous chapter">← <span class="section-number">2. </span>Output Size Limitations</a>
  </li>
  <li class="next">
    <a href="evals.html"
       title="next chapter"><span class="section-number">4. </span>Challenges of Evaluating LLM-based Applications →</a>
  </li>
</ul>
  
</div>
<hr>
          <div class="content" role="main" v-pre>
            
  <section class="tex2jax_ignore mathjax_ignore" id="wrestling-with-structured-output">
<h1><a class="toc-backref" href="#id14" role="doc-backlink"><span class="section-number">3. </span>Wrestling with Structured Output</a><a class="headerlink" href="#wrestling-with-structured-output" title="Permalink to this heading">¶</a></h1>
<blockquote class="epigraph">
<div><p>In limits, there is freedom. Creativity thrives within structure.</p>
<p class="attribution">—Julia B. Cameron</p>
</div></blockquote>
<nav class="contents" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#wrestling-with-structured-output" id="id14">Wrestling with Structured Output</a></p>
<ul>
<li><p><a class="reference internal" href="#the-structured-output-challenges" id="id15">The Structured Output Challenges</a></p></li>
<li><p><a class="reference internal" href="#problem-statement" id="id16">Problem Statement</a></p></li>
<li><p><a class="reference internal" href="#solutions" id="id17">Solutions</a></p>
<ul>
<li><p><a class="reference internal" href="#strategies" id="id18">Strategies</a></p></li>
<li><p><a class="reference internal" href="#techniques-and-tools" id="id19">Techniques and Tools</a></p>
<ul>
<li><p><a class="reference internal" href="#one-shot-prompts" id="id20">One-Shot Prompts</a></p></li>
<li><p><a class="reference internal" href="#structured-output-with-provider-specific-apis" id="id21">Structured Output with Provider-Specific APIs</a></p></li>
<li><p><a class="reference internal" href="#json-mode" id="id22">JSON Mode</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#langchain" id="id23">LangChain</a></p></li>
<li><p><a class="reference internal" href="#outlines" id="id24">Outlines</a></p>
<ul>
<li><p><a class="reference internal" href="#a-simple-example-multiple-choice-generation" id="id25">A Simple Example: Multiple Choice Generation</a></p></li>
<li><p><a class="reference internal" href="#pydantic-model" id="id26">Pydantic model</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#discussion" id="id27">Discussion</a></p>
<ul>
<li><p><a class="reference internal" href="#comparing-solutions" id="id28">Comparing Solutions</a></p></li>
<li><p><a class="reference internal" href="#best-practices" id="id29">Best Practices</a></p></li>
<li><p><a class="reference internal" href="#ongoing-debate-on-llms-structured-output" id="id30">Ongoing Debate on LLMs Structured Output</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#conclusion" id="id31">Conclusion</a></p></li>
<li><p><a class="reference internal" href="#acknowledgements" id="id32">Acknowledgements</a></p></li>
<li><p><a class="reference internal" href="#references" id="id33">References</a></p></li>
</ul>
</li>
</ul>
</nav>
<section id="the-structured-output-challenges">
<h2><a class="toc-backref" href="#id15" role="doc-backlink"><span class="section-number">3.1. </span>The Structured Output Challenges</a><a class="headerlink" href="#the-structured-output-challenges" title="Permalink to this heading">¶</a></h2>
<p>Large language models (LLMs) excel at generating human-like text, but they often struggle to produce output in a structured format consistently. This poses a significant challenge when we need LLMs to generate data that can be easily processed by other systems, such as databases, APIs, or other software applications.</p>
<p>Sometimes, even with a well-crafted prompt, an LLM might produce an unstructured response when a structured one is expected. This can be particularly challenging when integrating LLMs into systems that require specific data formats.</p>
<p>Throughout this notebook, we will consider as input a segment of a sample SEC filing of Apple Inc.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">MAX_LENGTH</span> <span class="o">=</span> <span class="mi">10000</span> <span class="c1"># We limit the input length to avoid token issues</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;../data/apple.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">sec_filing</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<span class="n">sec_filing</span> <span class="o">=</span> <span class="n">sec_filing</span><span class="p">[:</span><span class="n">MAX_LENGTH</span><span class="p">]</span> 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># Load environment variables from .env file</span>
<span class="n">load_dotenv</span><span class="p">(</span><span class="n">override</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>
<span class="c1"># Define the prompt expecting a structured JSON response</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Generate a two-person discussion about the key financial data from the following text in JSON format.</span>
<span class="s2">TEXT: </span><span class="si">{</span><span class="n">sec_filing</span><span class="si">}</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">response_content</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response_content</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Person 1: Wow, Apple Inc. seems to have a lot of different products and services they offer. It&#39;s interesting to see the breakdown of their revenue streams in their Form 10-K.

Person 2: Absolutely, they have a diverse portfolio with iPhones, Macs, iPads, wearables, and even services. It&#39;s impressive to see how they have capitalized on different technology trends.

Person 1: I noticed that they have a large market value of over $2.6 trillion as of March 29, 2024. That&#39;s a huge amount, and it shows the confidence investors have in the company.

Person 2: Definitely, that&#39;s a significant figure. It&#39;s also good to see that they are complying with all the required SEC regulations and filing their reports in a timely manner.

Person 1: Yes, it&#39;s crucial for investors to have access to accurate and up-to-date financial information. It helps in making informed decisions about their investments in the company.

Person 2: Absolutely, transparency and compliance with regulations are key in the financial industry. It&#39;s good to see that Apple Inc. is taking those aspects seriously.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">json</span>

<span class="k">def</span> <span class="nf">is_json</span><span class="p">(</span><span class="n">myjson</span><span class="p">):</span>
  <span class="k">try</span><span class="p">:</span>
    <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">myjson</span><span class="p">)</span>
  <span class="k">except</span> <span class="ne">ValueError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">False</span>
  <span class="k">return</span> <span class="kc">True</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">is_json</span><span class="p">(</span><span class="n">response_content</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>False
</pre></div>
</div>
</div>
</div>
<p>In this example, despite the prompt clearly asking for a JSON object, the LLM generates a natural language sentence instead. This highlights the inconsistency and unpredictability of LLMs when it comes to producing structured output.</p>
</section>
<section id="problem-statement">
<h2><a class="toc-backref" href="#id16" role="doc-backlink"><span class="section-number">3.2. </span>Problem Statement</a><a class="headerlink" href="#problem-statement" title="Permalink to this heading">¶</a></h2>
<p>Obtaining structured output from LLMs presents several significant challenges:</p>
<ul class="simple">
<li><p><strong>Inconsistency</strong>: LLMs often produce unpredictable results, sometimes generating well-structured output and other times deviating from the expected format.</p></li>
<li><p><strong>Lack of Type Safety</strong>: LLMs do not inherently understand data types, which can lead to errors when their output is integrated with systems requiring specific data formats.</p></li>
<li><p><strong>Prompt Engineering Complexity</strong>: Crafting prompts that effectively guide LLMs to produce the correct structured output is complex and requires extensive experimentation.</p></li>
</ul>
</section>
<section id="solutions">
<h2><a class="toc-backref" href="#id17" role="doc-backlink"><span class="section-number">3.3. </span>Solutions</a><a class="headerlink" href="#solutions" title="Permalink to this heading">¶</a></h2>
<p>Several strategies and tools can be employed to address the challenges of structured output from LLMs.</p>
<section id="strategies">
<h3><a class="toc-backref" href="#id18" role="doc-backlink"><span class="section-number">3.3.1. </span>Strategies</a><a class="headerlink" href="#strategies" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>Schema Guidance</strong>: Providing the LLM with a clear schema or blueprint of the desired output structure helps to constrain its generation and improve consistency. This can be achieved by using tools like Pydantic to define the expected data structure and then using that definition to guide the LLM’s output.</p></li>
<li><p><strong>Output Parsing</strong>: When LLMs don’t natively support structured output, parsing their text output using techniques like regular expressions or dedicated parsing libraries can extract the desired information. For example, you can use regular expressions to extract specific patterns from the LLM’s output, or you can use libraries like Pydantic to parse the output into structured data objects.</p></li>
<li><p><strong>Type Enforcement</strong>: Using tools that enforce data types, such as Pydantic in Python, can ensure that the LLM output adheres to the required data formats. This can help to prevent errors when integrating the LLM’s output with other systems.</p></li>
</ul>
</section>
<section id="techniques-and-tools">
<h3><a class="toc-backref" href="#id19" role="doc-backlink"><span class="section-number">3.3.2. </span>Techniques and Tools</a><a class="headerlink" href="#techniques-and-tools" title="Permalink to this heading">¶</a></h3>
<section id="one-shot-prompts">
<h4><a class="toc-backref" href="#id20" role="doc-backlink"><span class="section-number">3.3.2.1. </span>One-Shot Prompts</a><a class="headerlink" href="#one-shot-prompts" title="Permalink to this heading">¶</a></h4>
<p>In one-shot prompting, you provide a single example of the desired output format within the prompt.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Generate a two-person discussion about the key financial data from the following text in JSON format.</span>

<span class="s2">&lt;JSON_FORMAT&gt;</span>
<span class="si">{</span>
<span class="w">   </span><span class="s2">&quot;Person1&quot;</span><span class="si">:</span><span class="s2"> </span><span class="si">{</span>
<span class="w">     </span><span class="s2">&quot;name&quot;</span><span class="si">:</span><span class="s2"> &quot;Alice&quot;,</span>
<span class="s2">     &quot;statement&quot;: &quot;The revenue for Q1 has increased by 20% compared to last year.&quot;</span>
<span class="s2">   </span><span class="si">}</span><span class="s2">,</span>
<span class="s2">   &quot;Person2&quot;: </span><span class="si">{</span>
<span class="w">     </span><span class="s2">&quot;name&quot;</span><span class="si">:</span><span class="s2"> &quot;Bob&quot;,</span>
<span class="s2">     &quot;statement&quot;: &quot;That&#39;s great news! What about the net profit margin?&quot;</span>
<span class="s2">   </span><span class="si">}</span>
<span class="si">}</span>
<span class="s2">&lt;/JSON_FORMAT&gt;</span>

<span class="s2">TEXT: </span><span class="si">{</span><span class="n">sec_filing</span><span class="si">}</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">response_content</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response_content</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{
   &quot;Person1&quot;: {
     &quot;name&quot;: &quot;Alice&quot;,
     &quot;statement&quot;: &quot;The revenue for Q1 has increased by 20% compared to last year.&quot;
   },
   &quot;Person2&quot;: {
     &quot;name&quot;: &quot;Bob&quot;,
     &quot;statement&quot;: &quot;That&#39;s great news! What about the net profit margin?&quot;
   }
}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">is_json</span><span class="p">(</span><span class="n">response_content</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
</section>
<section id="structured-output-with-provider-specific-apis">
<h4><a class="toc-backref" href="#id21" role="doc-backlink"><span class="section-number">3.3.2.2. </span>Structured Output with Provider-Specific APIs</a><a class="headerlink" href="#structured-output-with-provider-specific-apis" title="Permalink to this heading">¶</a></h4>
<p>One-shot prompting is a simple technique that can lead to material improvements in structured output, though may not be sufficient for complex (e.g. nested) structures and / or when the model’s output needs to be restricted to a specific set of options or types.</p>
<p>Provider-specific APIs can offer ways to handle those challenges. We will explore two approaches here using OpenAI’s API:</p>
<ul class="simple">
<li><p><strong>JSON Mode</strong>: Most LLM APIs today offer features specifically designed for generating JSON output.</p></li>
<li><p><strong>Structured Outputs</strong>: Some LLM APIs offer features specifically designed for generating structured outputs with type safety.</p></li>
</ul>
</section>
<section id="json-mode">
<h4><a class="toc-backref" href="#id22" role="doc-backlink"><span class="section-number">3.3.2.3. </span>JSON Mode</a><a class="headerlink" href="#json-mode" title="Permalink to this heading">¶</a></h4>
<p>JSON mode is a feature provided by most LLM API providers, such as OpenAI, that allows the model to generate output in JSON format. This is particularly useful when you need structured data as a result, such as when parsing the output programmatically or integrating it with other systems that require JSON input. As depicted in <a class="reference internal" href="#id1"><span class="std std-numref">Fig. 3.1</span></a>, JSON mode is implemented by instructing theLLM model to use JSON as response format and optionally defining a target schema.</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="../_images/json.png"><img alt="JSON Mode" src="../_images/json.png" style="width: 822.0px; height: 506.5px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 3.1 </span><span class="caption-text">Conceptual overview of JSON mode.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>When using JSON mode with OpenAI’s API, it is recommended to instruct the model to produce JSON via some message in the conversation, for example via your system message. If you don’t include an explicit instruction to generate JSON, the model may generate an unending stream of whitespace and the request may run continually until it reaches the token limit. To help ensure you don’t forget, the API will throw an error if the string “JSON” does not appear somewhere in the context.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Generate a two-person discussion about the key financial data from the following text in JSON format.</span>
<span class="s2">TEXT: </span><span class="si">{</span><span class="n">sec_filing</span><span class="si">}</span>
<span class="s2">&quot;&quot;&quot;</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}],</span>
<span class="n">response_format</span> <span class="o">=</span> <span class="p">{</span> <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;json_object&quot;</span> <span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">response_content</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response_content</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{
  &quot;person1&quot;: &quot;I see that Apple Inc. reported a total market value of approximately $2,628,553,000,000 held by non-affiliates as of March 29, 2024. That&#39;s a significant amount!&quot;,
  &quot;person2&quot;: &quot;Yes, it definitely shows the scale and value of the company in the market. It&#39;s impressive to see the sheer size of the market value.&quot;,
  &quot;person1&quot;: &quot;Also, they mentioned having 15,115,823,000 shares of common stock issued and outstanding as of October 18, 2024. That&#39;s a large number of shares circulating in the market.&quot;,
  &quot;person2&quot;: &quot;Absolutely, the number of shares outstanding plays a crucial role in determining the company&#39;s market capitalization and investor interest.&quot;
}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">is_json</span><span class="p">(</span><span class="n">response_content</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<p>This example solution is specific to OpenAI’s API. Other LLM providers offer similar functionality, for example:</p>
<ul class="simple">
<li><p>Google’s Vertex AI offers a <code class="docutils literal notranslate"><span class="pre">parse</span></code> method for structured outputs.</p></li>
<li><p>Anthropic offers a <code class="docutils literal notranslate"><span class="pre">structured</span></code> method for structured outputs.</p></li>
</ul>
<p>JSON mode will not guarantee the output matches any specific schema, only that it is valid and parses without errors. For that purpose, we can leverage a new feature recently released by OpenAI called “Structured Outputs” to ensure the output data matches a target schema with type safety.</p>
<p><strong>Structured Output Mode</strong></p>
<p>Structured Outputs is a feature that ensures the model will always generate responses that adhere to your supplied JSON Schema, so you don’t need to worry about the model omitting a required key, or hallucinating an invalid enum value.</p>
<p>Some benefits of Structured Outputs include:</p>
<ul class="simple">
<li><p><strong>Reliable type-safety</strong>: No need to validate or retry incorrectly formatted responses.</p></li>
<li><p><strong>Explicit refusals</strong>: Safety-based model refusals are now programmatically detectable.</p></li>
<li><p><strong>Simpler prompting</strong>: No need for strongly worded prompts to achieve consistent formatting.</p></li>
</ul>
<p>Here’s a Python example demonstrating how to use the OpenAI API to generate a structured output. In this example, we aim at extracting structured data from our sample SEC filing, in particular: (i) entities and (ii) places mentioned in the input doc. This example uses the <code class="docutils literal notranslate"><span class="pre">response_format</span></code> parameter within the OpenAI API call. This functionality is supported by GPT-4o models, specifically <code class="docutils literal notranslate"><span class="pre">gpt-4o-mini-2024-07-18</span></code>, <code class="docutils literal notranslate"><span class="pre">gpt-4o-2024-08-06</span></code>, and later versions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span>
<span class="kn">from</span> <span class="nn">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>

<span class="k">class</span> <span class="nc">SECExtraction</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">mentioned_entities</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>
    <span class="n">mentioned_places</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">extract_from_sec_filing</span><span class="p">(</span><span class="n">sec_filing_text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SECExtraction</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extracts structured data from an input SEC filing text.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>
    <span class="n">completion</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span>
                <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span>
            <span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">sec_filing_text</span><span class="p">}</span>
        <span class="p">],</span>
        <span class="n">response_format</span><span class="o">=</span><span class="n">SECExtraction</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">completion</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">parsed</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ul class="simple">
<li><p><strong>Data Structures:</strong> The code defines one Pydantic model, <code class="docutils literal notranslate"><span class="pre">SECExtraction</span></code>, to represent the structured output of our parser. This model provide type hints and structure for the response.</p></li>
<li><p><strong>API Interaction:</strong> The <code class="docutils literal notranslate"><span class="pre">extract_from_sec_filing</span></code> function uses the OpenAI client to send a chat completion request to the <code class="docutils literal notranslate"><span class="pre">gpt-4o-mini-2024-07-18</span></code> model. The prompt instructs the model to extract our target attributes from input text. The <code class="docutils literal notranslate"><span class="pre">response_format</span></code> is set to <code class="docutils literal notranslate"><span class="pre">SECExtraction</span></code>, ensuring the response conforms to the specified Pydantic model.</p></li>
<li><p><strong>Output Processing:</strong> The returned response is parsed into the <code class="docutils literal notranslate"><span class="pre">SECExtraction</span></code> model. The code then returns the parsed data.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prompt_extraction</span> <span class="o">=</span> <span class="s2">&quot;You are an expert at structured data extraction. You will be given unstructured text from a SEC filing and extracted names of mentioned entities and places and should convert the response into the given structure.&quot;</span>
<span class="n">sec_extraction</span> <span class="o">=</span> <span class="n">extract_from_sec_filing</span><span class="p">(</span><span class="n">sec_filing</span><span class="p">,</span> <span class="n">prompt_extraction</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Extracted entities:&quot;</span><span class="p">,</span> <span class="n">sec_extraction</span><span class="o">.</span><span class="n">mentioned_entities</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Extracted places:&quot;</span><span class="p">,</span> <span class="n">sec_extraction</span><span class="o">.</span><span class="n">mentioned_places</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Extracted entities: [&#39;Apple Inc.&#39;, &#39;The Nasdaq Stock Market LLC&#39;]
Extracted places: [&#39;Washington, D.C.&#39;, &#39;California&#39;, &#39;Cupertino, California&#39;]
</pre></div>
</div>
</div>
</div>
<p>We observe that the model was able to extract the entities and places from the input text, and return them in the specified format.</p>
<p><strong>Benefits</strong></p>
<ul class="simple">
<li><p><strong>Structured Output:</strong> The use of Pydantic models and the <code class="docutils literal notranslate"><span class="pre">response_format</span></code> parameter enforces the structure of the model’s output, making it more reliable and easier to process.</p></li>
<li><p><strong>Schema Adherence:</strong>  Structured Outputs in OpenAI API guarantee that the response adheres to the provided schema.</p></li>
</ul>
<p>This structured approach improves the reliability and usability of your application by ensuring consistent, predictable output from the OpenAI API.</p>
<p>This example solution is specific to OpenAI’s API. That begs the question: How can we solve this problem generally for widely available LLM providers? In the next sections, we will explore how <code class="docutils literal notranslate"><span class="pre">LangChain</span></code> and <code class="docutils literal notranslate"><span class="pre">Outlines</span></code> may serve as general purpose tools that can help us do just that.</p>
</section>
</section>
<section id="langchain">
<h3><a class="toc-backref" href="#id23" role="doc-backlink"><span class="section-number">3.3.3. </span>LangChain</a><a class="headerlink" href="#langchain" title="Permalink to this heading">¶</a></h3>
<p>LangChain is a framework designed to simplify the development of LLM applications. It provider an abstraction layer over many LLM providers, including OpenAI, that offers several tools for parsing structured output.</p>
<p>In particular, LangChain offers the <code class="docutils literal notranslate"><span class="pre">with_structured_output</span></code> method, which can be used with LLMs that support structured output APIs, allowing you to enforce a schema directly within the prompt.</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">with_structured_output</span></code> takes a schema as input which specifies the names, types, and descriptions of the desired output attributes. The method returns a model-like Runnable, except that instead of outputting strings or messages it outputs objects corresponding to the given schema. The schema can be specified as a TypedDict class, JSON Schema or a Pydantic class. If TypedDict or JSON Schema are used then a dictionary will be returned by the Runnable, and if a Pydantic class is used then a Pydantic object will be returned.</p>
</div></blockquote>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-qU<span class="w"> </span>langchain-openai
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span>
<span class="k">def</span> <span class="nf">extract_from_sec_filing_langchain</span><span class="p">(</span><span class="n">sec_filing_text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SECExtraction</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extracts structured data from an input SEC filing text using LangChain.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">)</span>

    <span class="n">structured_llm</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">with_structured_output</span><span class="p">(</span><span class="n">SECExtraction</span><span class="p">)</span>

    <span class="n">prompt_template</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="p">(</span><span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="n">prompt</span><span class="p">),</span>
            <span class="p">(</span><span class="s2">&quot;human&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{sec_filing_text}</span><span class="s2">&quot;</span><span class="p">),</span>
        <span class="p">]</span>
    <span class="p">)</span>

    <span class="n">llm_chain</span> <span class="o">=</span> <span class="n">prompt_template</span> <span class="o">|</span> <span class="n">structured_llm</span>
    
    <span class="k">return</span> <span class="n">llm_chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">sec_filing_text</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prompt_extraction</span> <span class="o">=</span> <span class="s2">&quot;You are an expert at structured data extraction. You will be given unstructured text from a SEC filing and extracted names of mentioned entities and places and should convert the response into the given structure.&quot;</span>
<span class="n">sec_extraction_langchain</span> <span class="o">=</span> <span class="n">extract_from_sec_filing_langchain</span><span class="p">(</span><span class="n">sec_filing</span><span class="p">,</span> <span class="n">prompt_extraction</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Extracted entities:&quot;</span><span class="p">,</span> <span class="n">sec_extraction_langchain</span><span class="o">.</span><span class="n">mentioned_entities</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Extracted places:&quot;</span><span class="p">,</span> <span class="n">sec_extraction_langchain</span><span class="o">.</span><span class="n">mentioned_places</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Extracted entities: [&#39;Apple Inc.&#39;]
Extracted places: [&#39;California&#39;, &#39;Cupertino&#39;]
</pre></div>
</div>
</div>
</div>
<p>We observe that the model was able to extract the entities and places from the input text, and return them in the specified format. A full list of models that support <code class="docutils literal notranslate"><span class="pre">.with_structured_output()</span></code> can be found <a class="reference external" href="https://python.langchain.com/docs/integrations/chat/#featured-providers">here</a>.</p>
</section>
<section id="outlines">
<h3><a class="toc-backref" href="#id24" role="doc-backlink"><span class="section-number">3.3.4. </span>Outlines</a><a class="headerlink" href="#outlines" title="Permalink to this heading">¶</a></h3>
<p>Outlines <span id="id2">[<a class="reference internal" href="#id9" title="Outlines. Type-safe structured output from llms. https://dottxt-ai.github.io/outlines/latest/, 2024. Accessed: 2024.">Out24</a>]</span> is a library specifically focused on structured text generation from LLMs. Under the hood, Outlines works by adjusting the probability distribution of the model’s output logits - the raw scores from the final layer of the neural network that are normally converted into text tokens. By introducing carefully crafted logit biases, Outlines can guide the model to prefer certain tokens over others, effectively constraining its outputs to a predefined set of valid options. This provides fine-grained control over the model’s generation process. In that way, Outlines provides several powerful features:</p>
<ul class="simple">
<li><p><strong>Multiple Choice Generation</strong>: Restrict the LLM output to a predefined set of options.</p></li>
<li><p><strong>Regex-based structured generation</strong>: Guide the generation process using regular expressions.</p></li>
<li><p><strong>Pydantic model</strong>: Ensure the LLM output follows a Pydantic model.</p></li>
<li><p><strong>JSON Schema</strong>: Ensure the LLM output follows a JSON Schema.</p></li>
</ul>
<p>Outlines can support major proprietary LLM APIs (e.g. OpenAI’s via vLLM). However, one of its key advantages is the ability to ensure structured output for Open Source models, which often lack such guarantees by default.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>outlines
pip<span class="w"> </span>install<span class="w"> </span>transformers
</pre></div>
</div>
<p>In this example, we will use a Qwen2.5-0.5B model, a lightweight open source model from Alibaba Cloud known for its strong performance despite its small size. The model excels at instruction following and structured generation tasks while being efficient enough to run locally via Hugging Face’s <code class="docutils literal notranslate"><span class="pre">transformers</span></code> library.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">outlines</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">outlines</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">transformers</span><span class="p">(</span><span class="s2">&quot;Qwen/Qwen2.5-0.5B-Instruct&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="a-simple-example-multiple-choice-generation">
<h4><a class="toc-backref" href="#id25" role="doc-backlink"><span class="section-number">3.3.4.1. </span>A Simple Example: Multiple Choice Generation</a><a class="headerlink" href="#a-simple-example-multiple-choice-generation" title="Permalink to this heading">¶</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">TOP</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;You are a sentiment-labelling assistant specialized in Financial Statements.</span>
<span class="s2">Is the following document positive or negative?</span>

<span class="s2">Document: </span><span class="si">{</span><span class="n">sec_filing</span><span class="p">[:</span><span class="n">TOP</span><span class="p">]</span><span class="si">}</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">generator</span> <span class="o">=</span> <span class="n">outlines</span><span class="o">.</span><span class="n">generate</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;Positive&quot;</span><span class="p">,</span> <span class="s2">&quot;Negative&quot;</span><span class="p">])</span>
<span class="n">answer</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">answer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Negative
</pre></div>
</div>
</div>
</div>
<p>In this simple example, we use Outlines’ <code class="docutils literal notranslate"><span class="pre">choice</span></code> method to constrain the model output to a predefined set of options (“Positive” or “Negative”). This ensures the model can only return one of these values, avoiding any unexpected or malformed responses.</p>
</section>
<section id="pydantic-model">
<h4><a class="toc-backref" href="#id26" role="doc-backlink"><span class="section-number">3.3.4.2. </span>Pydantic model</a><a class="headerlink" href="#pydantic-model" title="Permalink to this heading">¶</a></h4>
<p>Outlines allows to guide the generation process so the output is guaranteed to follow a JSON schema or Pydantic model. Now we will go back to our example of extracting entities and places from a SEC filing. In order to do so, we simply need to pass our Pydantic model to the <code class="docutils literal notranslate"><span class="pre">json</span></code> method in Outlines’ <code class="docutils literal notranslate"><span class="pre">generate</span></code> module.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;You are an expert at structured data extraction. You will be given unstructured text from a SEC filing and extracted names of mentioned entities and places and should convert the response into the given structure. Document: </span><span class="si">{</span><span class="n">sec_filing</span><span class="p">[:</span><span class="n">TOP</span><span class="p">]</span><span class="si">}</span><span class="s2"> &quot;</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">outlines</span><span class="o">.</span><span class="n">generate</span><span class="o">.</span><span class="n">json</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">SECExtraction</span><span class="p">)</span>
<span class="n">sec_extraction_outlines</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Extracted entities:&quot;</span><span class="p">,</span> <span class="n">sec_extraction_outlines</span><span class="o">.</span><span class="n">mentioned_entities</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Extracted places:&quot;</span><span class="p">,</span> <span class="n">sec_extraction_outlines</span><span class="o">.</span><span class="n">mentioned_places</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Extracted entities: [&#39;Zsp&#39;, &#39;ZiCorp&#39;]
Extracted places: [&#39;California&#39;]
</pre></div>
</div>
</div>
</div>
<p>We observe that the model was able to extract the entities and places from the input text, and return them in the specified format. However, it is interesting to see that the model hallucinates a few entities, a phenomenon that is common for smaller Open Source models that were not fine-tuned on the task of entity extraction.</p>
</section>
</section>
</section>
<section id="discussion">
<h2><a class="toc-backref" href="#id27" role="doc-backlink"><span class="section-number">3.4. </span>Discussion</a><a class="headerlink" href="#discussion" title="Permalink to this heading">¶</a></h2>
<section id="comparing-solutions">
<h3><a class="toc-backref" href="#id28" role="doc-backlink"><span class="section-number">3.4.1. </span>Comparing Solutions</a><a class="headerlink" href="#comparing-solutions" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>Simplicity vs. Control</strong>: One-shot prompts are simple but offer limited control.  <code class="docutils literal notranslate"><span class="pre">LangChain</span></code>, and Outlines provide greater control but might have a steeper learning curve though quite manageable.</p></li>
<li><p><strong>Native LLM Support</strong>:  <code class="docutils literal notranslate"><span class="pre">with_structured_output</span></code> in LangChain relies on the underlying LLM having built-in support for structured output APIs, i.e. LangChain is a wrapper around the underlying LLM’s structured output API. Outlines, on the other hand, is more broadly applicable enabling a wider range of Open Source models.</p></li>
<li><p><strong>Flexibility</strong>:  Outlines and LangChain’s  <code class="docutils literal notranslate"><span class="pre">StructuredOutputParser</span></code>  offer the most flexibility for defining custom output structures.</p></li>
</ul>
</section>
<section id="best-practices">
<h3><a class="toc-backref" href="#id29" role="doc-backlink"><span class="section-number">3.4.2. </span>Best Practices</a><a class="headerlink" href="#best-practices" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p><strong>Clear Schema Definition</strong>: Define the desired output structure clearly. This can be done in several ways including schemas, types, or Pydantic models as appropriate. This ensures the LLM knows exactly what format is expected.</p></li>
<li><p><strong>Descriptive Naming</strong>: Use meaningful names for fields and elements in your schema. This makes the output more understandable and easier to work with.</p></li>
<li><p><strong>Detailed Prompting</strong>: Guide the LLM with well-crafted prompts that include examples and clear instructions.  A well-structured prompt improves the chances of getting the desired output.</p></li>
<li><p><strong>Integration</strong>: If you are connecting the model to tools, functions, data, etc. in your system, then you are highly encouraged to use a typed structured output (e.g. Pydantic models) to ensure the model’s output can be processed correctly by downstream systems.</p></li>
</ul>
</section>
<section id="ongoing-debate-on-llms-structured-output">
<h3><a class="toc-backref" href="#id30" role="doc-backlink"><span class="section-number">3.4.3. </span>Ongoing Debate on LLMs Structured Output</a><a class="headerlink" href="#ongoing-debate-on-llms-structured-output" title="Permalink to this heading">¶</a></h3>
<p>The use of structured output, like JSON or XML, for Large Language Models (LLMs) is a developing area. While structured output offers clear benefits in parsing, robustness, and integration, there is growing debate on whether it also potentially comes at the cost of reasoning abilities.</p>
<p>Recent research “Let Me Speak Freely? A Study on the Impact of Format Restrictions on Performance of Large Language Models” <span id="id3">[<a class="reference internal" href="#id10" title="Zhi Rui Tam, Cheng-Kuang Wu, Yi-Lin Tsai, Chieh-Yen Lin, Hung-yi Lee, and Yun-Nung Chen. Let me speak freely? a study on the impact of format restrictions on performance of large language models. 2024. URL: https://arxiv.org/abs/2408.02442, arXiv:2408.02442.">TWT+24</a>]</span> suggests that imposing format restrictions on LLMs might impact their performance, particularly in reasoning-intensive tasks. Further evidence <span id="id4">[<a class="reference internal" href="#id12" title="Aider. Code in json: structured output for llms. https://aider.chat/2024/08/14/code-in-json.html, 2024. Accessed: 2024.">Aid24</a>]</span> suggests LLMs may produce lower quality code if they’re asked to return it as part of a structured JSON response, in particular:</p>
<ul class="simple">
<li><p><strong>Potential performance degradation:</strong>  Enforcing structured output, especially through constrained decoding methods like JSON-mode, can negatively impact an LLM’s reasoning abilities. This is particularly evident in tasks that require multi-step reasoning or complex thought processes.</p></li>
<li><p><strong>Overly restrictive schemas:</strong> Imposing strict schemas can limit the expressiveness of LLM outputs and may hinder their ability to generate creative or nuanced responses.  In certain cases, the strictness of the schema might outweigh the benefits of structured output.</p></li>
<li><p><strong>Increased complexity in prompt engineering:</strong> Crafting prompts that effectively guide LLMs to generate structured outputs while maintaining performance can be challenging. It often requires careful consideration of the schema, the task instructions, and the desired level of detail in the response.</p></li>
</ul>
<p>On the other hand, those findings are not without criticism. The .txt team challenges the work of <span id="id5">[<a class="reference internal" href="#id10" title="Zhi Rui Tam, Cheng-Kuang Wu, Yi-Lin Tsai, Chieh-Yen Lin, Hung-yi Lee, and Yun-Nung Chen. Let me speak freely? a study on the impact of format restrictions on performance of large language models. 2024. URL: https://arxiv.org/abs/2408.02442, arXiv:2408.02442.">TWT+24</a>]</span>. The rebuttal argues that <strong>structured generation, when done correctly, actually <em>improves</em> performance</strong>.</p>
<figure class="align-center" id="structured-vs-unstructured">
<a class="reference internal image-reference" href="../_images/rebuttal.png"><img alt="Structured vs Unstructured Results by .txt team" src="../_images/rebuttal.png" style="width: 892.8px; height: 543.6px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 3.2 </span><span class="caption-text">Structured vs Unstructured Results by .txt team.</span><a class="headerlink" href="#structured-vs-unstructured" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The .txt team presents compelling evidence through their reproduction of the paper’s experiments. While their unstructured results align with the original paper’s findings, their structured results paint a dramatically different picture - demonstrating that structured generation actually improves performance (see <a class="reference internal" href="#structured-vs-unstructured"><span class="std std-numref">Fig. 3.2</span></a>). The team has made their experimental notebooks publicly available on GitHub for independent verification <span id="id6">[<a class="reference internal" href="#id13" title="Dottxt. Say what you mean: demos. https://github.com/dottxt-ai/demos/tree/main/say-what-you-mean, 2024. Accessed: 2024.">Dot24</a>]</span>.</p>
<p>.txt team identifies several flaws in the methodology of “Let Me Speak Freely?” that they believe led to inaccurate conclusions:</p>
<ul class="simple">
<li><p>The paper finds that structured output improves performance on classification tasks but doesn’t reconcile this finding with its overall negative conclusion about structured output.</p></li>
<li><p>The prompts used for unstructured generation were different from those used for structured generation, making the comparison uneven.</p></li>
<li><p>The prompts used for structured generation, particularly in JSON-mode, didn’t provide the LLM with sufficient information to properly complete the task.</p></li>
<li><p>The paper conflates “structured generation” with “JSON-mode”, when they are not the same thing.</p></li>
</ul>
<p>It is important to note that while .txt provides a compelling and verifiable argument in favor of (proper) structured output generation in LLMs further research and exploration are needed to comprehensively understand the nuances and trade-offs involved in using structured output for various LLM tasks and applications.</p>
<p>In summary, the debate surrounding structured output highlights the ongoing challenges in balancing LLM capabilities with real-world application requirements. While structured outputs offer clear benefits in parsing, robustness, and integration, their potential impact on performance, particularly in reasoning tasks is a topic of ongoing debate.</p>
<p>The ideal approach likely involves a nuanced strategy that considers the specific task, the desired level of structure, and the available LLM capabilities. Further research and development efforts are needed to mitigate potential drawbacks and unlock the full potential of LLMs for a wider range of applications.</p>
</section>
</section>
<section id="conclusion">
<h2><a class="toc-backref" href="#id31" role="doc-backlink"><span class="section-number">3.5. </span>Conclusion</a><a class="headerlink" href="#conclusion" title="Permalink to this heading">¶</a></h2>
<p>Extracting structured output from LLMs is crucial for integrating them into real-world applications. By understanding the challenges and employing appropriate strategies and tools, developers can improve the reliability and usability of LLM-powered systems, unlocking their potential to automate complex tasks and generate valuable insights.</p>
</section>
<section id="acknowledgements">
<h2><a class="toc-backref" href="#id32" role="doc-backlink"><span class="section-number">3.6. </span>Acknowledgements</a><a class="headerlink" href="#acknowledgements" title="Permalink to this heading">¶</a></h2>
<p>We would like to thank Cameron Pfiffer from the .txt team for his insightful review and feedback.</p>
</section>
<section id="references">
<h2><a class="toc-backref" href="#id33" role="doc-backlink"><span class="section-number">3.7. </span>References</a><a class="headerlink" href="#references" title="Permalink to this heading">¶</a></h2>
<div class="docutils container" id="id7">
<div class="citation" id="id12" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">Aid24</a><span class="fn-bracket">]</span></span>
<p>Aider. Code in json: structured output for llms. <a class="reference external" href="https://aider.chat/2024/08/14/code-in-json.html">https://aider.chat/2024/08/14/code-in-json.html</a>, 2024. Accessed: 2024.</p>
</div>
<div class="citation" id="id13" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">Dot24</a><span class="fn-bracket">]</span></span>
<p>Dottxt. Say what you mean: demos. <a class="reference external" href="https://github.com/dottxt-ai/demos/tree/main/say-what-you-mean">https://github.com/dottxt-ai/demos/tree/main/say-what-you-mean</a>, 2024. Accessed: 2024.</p>
</div>
<div class="citation" id="id9" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">Out24</a><span class="fn-bracket">]</span></span>
<p>Outlines. Type-safe structured output from llms. <a class="reference external" href="https://dottxt-ai.github.io/outlines/latest/">https://dottxt-ai.github.io/outlines/latest/</a>, 2024. Accessed: 2024.</p>
</div>
<div class="citation" id="id10" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>TWT+24<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id3">1</a>,<a role="doc-backlink" href="#id5">2</a>)</span>
<p>Zhi Rui Tam, Cheng-Kuang Wu, Yi-Lin Tsai, Chieh-Yen Lin, Hung-yi Lee, and Yun-Nung Chen. Let me speak freely? a study on the impact of format restrictions on performance of large language models. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2408.02442">https://arxiv.org/abs/2408.02442</a>, <a class="reference external" href="https://arxiv.org/abs/2408.02442">arXiv:2408.02442</a>.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

          </div>
          <div class="page-nav">
            <div class="inner"><ul class="page-nav">
  <li class="prev">
    <a href="output_size_limit.html"
       title="previous chapter">← <span class="section-number">2. </span>Output Size Limitations</a>
  </li>
  <li class="next">
    <a href="evals.html"
       title="next chapter"><span class="section-number">4. </span>Challenges of Evaluating LLM-based Applications →</a>
  </li>
</ul><div class="footer" role="contentinfo">
    <br>
    Created using <a href="http://sphinx-doc.org/">Sphinx</a> 6.2.1 with <a href="https://github.com/schettino72/sphinx_press_theme">Press Theme</a> 0.9.1.
</div>
            </div>
          </div>
      </page>
    </div></div>
    
    
  </body>
</html>