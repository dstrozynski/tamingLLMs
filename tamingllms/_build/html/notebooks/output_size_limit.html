
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Output Size Limitations &#8212; Taming Language Models: A Practical Guide to LLM Pitfalls with Python Examples</title>
    <link rel="stylesheet" href="../_static/basic.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/insipid.css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.4cbf315f70debaebd550c87a6162cf0f.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/default.css" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script defer src="../_static/insipid.js"></script>
    <script defer src="../_static/insipid-sidebar.js"></script>
    <link rel="canonical" href="https://souzatharsis.github.io/tamingllms/notebooks/output_size_limit.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="The Blessing and Curse of Non-determinism" href="../nondeterminism-NOTES.html" />
    <link rel="prev" title="Non-determinism" href="nondeterminism.html" />
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-capable" content="yes">
  </head><body>
    <script type="text/javascript">
        document.body.classList.add('js');
    </script>
    <input type="checkbox" id="sidebar-checkbox" style="display: none;">
    <label id="overlay" for="sidebar-checkbox"></label>
    <div class="sidebar-resize-handle"></div>
    <script type="text/javascript">
        try {
            let sidebar = localStorage.getItem('sphinx-sidebar');
            const sidebar_width = localStorage.getItem('sphinx-sidebar-width');
            if (sidebar_width) {
                document.documentElement.style.setProperty('--sidebar-width', sidebar_width);
            }
            // show sidebar on wide screen
            if (!sidebar && window.matchMedia('(min-width: 100rem)').matches) {
                sidebar = 'visible';
                // NB: We don't store the value in localStorage!
            }
            if (sidebar === 'visible') {
                document.getElementById('sidebar-checkbox').checked = true;
            }
        } catch(e) {
            console.info(e);
        }
    </script>
    <header id="topbar-placeholder">
      <div id="topbar">
        <div id="titlebar">
          <div class="buttons">
            <label for="sidebar-checkbox" id="sidebar-button" role="button" tabindex="0" aria-controls="sphinxsidebar" accesskey="M">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"/></svg>
            </label>
            <button id="search-button" type="button" title="Search" aria-label="Search" aria-expanded="false" aria-controls="search-form" accesskey="S">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
            </button>
          </div>
          <div class="title">
            <a class="parent" href="../markdown/toc.html" accesskey="U">Taming Language Models: A Practical Guide to LLM Pitfalls with Python Examples</a>
            <a class="top" href="#">Output Size Limitations</a>
          </div>
          <div class="buttons">
            <button id="fullscreen-button" type="button" aria-hidden="true">
              <span class="enable">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M212.686 315.314L120 408l32.922 31.029c15.12 15.12 4.412 40.971-16.97 40.971h-112C10.697 480 0 469.255 0 456V344c0-21.382 25.803-32.09 40.922-16.971L72 360l92.686-92.686c6.248-6.248 16.379-6.248 22.627 0l25.373 25.373c6.249 6.248 6.249 16.378 0 22.627zm22.628-118.628L328 104l-32.922-31.029C279.958 57.851 290.666 32 312.048 32h112C437.303 32 448 42.745 448 56v112c0 21.382-25.803 32.09-40.922 16.971L376 152l-92.686 92.686c-6.248 6.248-16.379 6.248-22.627 0l-25.373-25.373c-6.249-6.248-6.249-16.378 0-22.627z"/></svg>
              </span>
              <span class="disable">
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M4.686 427.314L104 328l-32.922-31.029C55.958 281.851 66.666 256 88.048 256h112C213.303 256 224 266.745 224 280v112c0 21.382-25.803 32.09-40.922 16.971L152 376l-99.314 99.314c-6.248 6.248-16.379 6.248-22.627 0L4.686 449.941c-6.248-6.248-6.248-16.379 0-22.627zM443.314 84.686L344 184l32.922 31.029c15.12 15.12 4.412 40.971-16.97 40.971h-112C234.697 256 224 245.255 224 232V120c0-21.382 25.803-32.09 40.922-16.971L296 136l99.314-99.314c6.248-6.248 16.379-6.248 22.627 0l25.373 25.373c6.248 6.248 6.248 16.379 0 22.627z"/></svg>
              </span>
            </button>
          </div>
        </div>
        <div id="searchbox" role="search">
          <form id="search-form" class="search" style="display: none" action="../search.html" method="get">
            <input type="search" name="q" placeholder="Search ..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" />
            <button>Go</button>
          </form>
        </div>
      </div>
    </header>
    <nav>
      <a href="nondeterminism.html" class="nav-icon previous" title="previous:&#13;Non-determinism" aria-label="Previous topic" accesskey="P" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M31.7 239l136-136c9.4-9.4 24.6-9.4 33.9 0l22.6 22.6c9.4 9.4 9.4 24.6 0 33.9L127.9 256l96.4 96.4c9.4 9.4 9.4 24.6 0 33.9L201.7 409c-9.4 9.4-24.6 9.4-33.9 0l-136-136c-9.5-9.4-9.5-24.6-.1-34z"/></svg>
      </a>
      <a href="../nondeterminism-NOTES.html" class="nav-icon next" title="next:&#13;The Blessing and Curse of Non-determinism" aria-label="Next topic" accesskey="N" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg>
      </a>
    </nav>

    <nav class="relbar">
      <a class="previous" href="nondeterminism.html" aria-label="Previous topic" >
        <div class="icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M257.5 445.1l-22.2 22.2c-9.4 9.4-24.6 9.4-33.9 0L7 273c-9.4-9.4-9.4-24.6 0-33.9L201.4 44.7c9.4-9.4 24.6-9.4 33.9 0l22.2 22.2c9.5 9.5 9.3 25-.4 34.3L136.6 216H424c13.3 0 24 10.7 24 24v32c0 13.3-10.7 24-24 24H136.6l120.5 114.8c9.8 9.3 10 24.8.4 34.3z"/></svg>
        </div>
        <div class="title">
          <span class="text">
            <span class="direction">previous</span>
            Non-determinism
          </span>
        </div>
      </a>
      <a class="next" href="../nondeterminism-NOTES.html" aria-label="Next topic" >
        <div class="title">
          <span class="text">
            <span class="direction">next</span>
            The Blessing and Curse of Non-determinism
          </span>
        </div>
        <div class="icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M190.5 66.9l22.2-22.2c9.4-9.4 24.6-9.4 33.9 0L441 239c9.4 9.4 9.4 24.6 0 33.9L246.6 467.3c-9.4 9.4-24.6 9.4-33.9 0l-22.2-22.2c-9.5-9.5-9.3-25 .4-34.3L311.4 296H24c-13.3 0-24-10.7-24-24v-32c0-13.3 10.7-24 24-24h287.4L190.9 101.2c-9.8-9.3-10-24.8-.4-34.3z"/></svg>
        </div>
      </a>
    </nav>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
<section class="tex2jax_ignore mathjax_ignore" id="output-size-limitations">
<h1>Output Size Limitations<a class="headerlink" href="#output-size-limitations" title="Permalink to this heading">¶</a></h1>
<section id="what-are-token-limits">
<h2>What are Token Limits?<a class="headerlink" href="#what-are-token-limits" title="Permalink to this heading">¶</a></h2>
<p>Tokens are the basic units that LLMs process text with. A token can be as short as a single character or as long as a complete word. In English, a general rule of thumb is that 1 token ≈ 4 characters or ¾ of a word.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">max_output_tokens</span></code> is parameter often available in modern LLMs that determines the maximum length of text that an LLM can generate in a single response. Contrary to what one might expect, the model does not “summarizes the answer” such that it does not surpass <code class="docutils literal notranslate"><span class="pre">max_output_tokens</span></code> limit. Instead, it will stop once it reaches this limit, even mid-sentence, i.e. the response may be truncated.</p>
<p><strong>Table 1: Token Cost and Length Limitation Comparison Across Key Models</strong></p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>max_output_tokens</p></th>
<th class="head"><p>max_input_tokens</p></th>
<th class="head"><p>input_cost_per_token</p></th>
<th class="head"><p>output_cost_per_token</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>meta.llama3-2-11b-instruct-v1:0</p></td>
<td><p>4096</p></td>
<td><p>128000</p></td>
<td><p>3.5e-7</p></td>
<td><p>3.5e-7</p></td>
</tr>
<tr class="row-odd"><td><p>claude-3-5-sonnet-20241022</p></td>
<td><p>8192</p></td>
<td><p>200000</p></td>
<td><p>3e-6</p></td>
<td><p>1.5e-5</p></td>
</tr>
<tr class="row-even"><td><p>gpt-4-0613</p></td>
<td><p>4096</p></td>
<td><p>8192</p></td>
<td><p>3e-5</p></td>
<td><p>6e-5</p></td>
</tr>
<tr class="row-odd"><td><p>gpt-4-turbo-2024-04-09</p></td>
<td><p>4096</p></td>
<td><p>128000</p></td>
<td><p>1e-5</p></td>
<td><p>3e-5</p></td>
</tr>
<tr class="row-even"><td><p>gpt-4o-mini</p></td>
<td><p>16384</p></td>
<td><p>128000</p></td>
<td><p>1.5e-7</p></td>
<td><p>6e-7</p></td>
</tr>
<tr class="row-odd"><td><p>gemini/gemini-1.5-flash-002</p></td>
<td><p>8192</p></td>
<td><p>1048576</p></td>
<td><p>7.5e-8</p></td>
<td><p>3e-7</p></td>
</tr>
<tr class="row-even"><td><p>gemini/gemini-1.5-pro-002</p></td>
<td><p>8192</p></td>
<td><p>2097152</p></td>
<td><p>3.5e-6</p></td>
<td><p>1.05e-5</p></td>
</tr>
</tbody>
</table>
</section>
<section id="problem-statement">
<h2>Problem Statement<a class="headerlink" href="#problem-statement" title="Permalink to this heading">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">max_output_tokens</span></code> limit in LLMs poses a significant challenge for users who need to generate long outputs, as it may result in truncated content and/or incomplete information.</p>
<ol class="arabic simple">
<li><p><strong>Truncated Content</strong>: Users aiming to generate extensive content, such as detailed reports or comprehensive articles, may find their outputs abruptly cut off due to the <code class="docutils literal notranslate"><span class="pre">max_output_tokens</span></code> limit. This truncation can result in incomplete information and disrupt the flow of the content.</p></li>
<li><p><strong>Shallow Responses</strong>: When users expect a complete and thorough response but receive only a partial output, it can lead to dissatisfaction and frustration. This is especially true in applications where the completeness of information is critical, such as in educational tools or content creation platforms.</p></li>
</ol>
<p>To effectively address these challenges, developers need to implement robust solutions that balance user expectations with technical and cost constraints, ensuring that long-form content generation remains feasible and efficient.</p>
</section>
<section id="content-chunking-with-contextual-linking">
<h2>Content Chunking with Contextual Linking<a class="headerlink" href="#content-chunking-with-contextual-linking" title="Permalink to this heading">¶</a></h2>
<p>Content chunking with contextual linking is a technique used to manage the <code class="docutils literal notranslate"><span class="pre">max_output_tokens</span></code> limitation by breaking down long-form content into smaller, manageable chunks. This approach allows the LLM to focus on smaller sections of the input, enabling it to generate more complete and detailed responses for each chunk while maintaining coherence and context across the entire output.</p>
<ol class="arabic simple">
<li><p><strong>Chunking the Content</strong>: The input content is split into smaller chunks. This allows the LLM to process each chunk individually, focusing on generating a complete and detailed response for that specific section of the input.</p></li>
<li><p><strong>Maintaining Context</strong>: Each chunk is linked with contextual information from the previous chunks. This helps in maintaining the flow and coherence of the content across multiple chunks.</p></li>
<li><p><strong>Generating Linked Prompts</strong>: For each chunk, a prompt is generated that includes the chunk’s content and its context. This prompt is then used to generate the output for that chunk.</p></li>
<li><p><strong>Combining the Outputs</strong>: The outputs of all chunks are combined to form the final long-form content.</p></li>
</ol>
<p>By following these steps, developers can effectively manage the <code class="docutils literal notranslate"><span class="pre">max_output_tokens</span></code> limitation and generate coherent long-form content without truncation.</p>
<p>Let’s examine an example implementation of this technique.</p>
<section id="generating-long-form-content">
<h3>Generating long-form content<a class="headerlink" href="#generating-long-form-content" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>Goal: Generate a long-form report analyzing a company’s financial statement.</p></li>
<li><p>Input: A company’s 10K SEC filing.</p></li>
</ul>
<section id="step-1-chunking-the-content">
<h4>Step 1: Chunking the Content<a class="headerlink" href="#step-1-chunking-the-content" title="Permalink to this heading">¶</a></h4>
<p>There are different methods for chunking, and each of them might be appropriate for different situations. However, we can broadly group chunking strategies in two types:</p>
<ul class="simple">
<li><p><strong>Fixed-size Chunking</strong>: This is the most common and straightforward approach to chunking. We simply decide the number of tokens in our chunk and, optionally, whether there should be any overlap between them. In general, we will want to keep some overlap between chunks to make sure that the semantic context doesn’t get lost between chunks. Fixed-sized chunking may be a reasonable path in many common cases. Compared to other forms of chunking, fixed-sized chunking is computationally cheap and simple to use since it doesn’t require the use of any specialied techniques or libraries.</p></li>
<li><p><strong>Content-aware Chunking</strong>: These are a set of methods for taking advantage of the nature of the content we’re chunking and applying more sophisticated chunking to it. Examples include:</p>
<ul>
<li><p><strong>Sentence Splitting</strong>: Many models are optimized for embedding sentence-level content. Naturally, we would use sentence chunking, and there are several approaches and tools available to do this, including naive splitting (e.g. splitting on periods), NLTK, and spaCy.</p></li>
<li><p><strong>Recursive Chunking</strong>: Recursive chunking divides the input text into smaller chunks in a hierarchical and iterative manner using a set of separators.</p></li>
<li><p><strong>Semantic Chunking</strong>: This is a class of methods that leverages embeddings to extract the semantic meaning present in your data, creating chunks that are made up of sentences that talk about the same theme or topic.</p></li>
</ul>
</li>
</ul>
<p>Here, we will utilize <code class="docutils literal notranslate"><span class="pre">langchain</span></code> for a content-aware sentence-splitting strategy for chunking. We will use the <code class="docutils literal notranslate"><span class="pre">CharacterTextSplitter</span></code> with <code class="docutils literal notranslate"><span class="pre">tiktoken</span></code> as our tokenizer to count the number of tokens per chunk which we can use to ensure that we do not surpass the input token limit of our model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_chunks</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Split input text into chunks of specified size with specified overlap.</span>

<span class="sd">    Args:</span>
<span class="sd">        text (str): The input text to be chunked.</span>
<span class="sd">        chunk_size (int): The maximum size of each chunk in tokens.</span>
<span class="sd">        chunk_overlap (int): The number of tokens to overlap between chunks.</span>

<span class="sd">    Returns:</span>
<span class="sd">        list: A list of text chunks.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">langchain_text_splitters</span> <span class="kn">import</span> <span class="n">CharacterTextSplitter</span>

    <span class="n">text_splitter</span> <span class="o">=</span> <span class="n">CharacterTextSplitter</span><span class="o">.</span><span class="n">from_tiktoken_encoder</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="n">chunk_size</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="n">chunk_overlap</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">text_splitter</span><span class="o">.</span><span class="n">split_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We will write a base prompt template which will serve as a foundational structure for all chunks, ensuring consistency in the instructions and context provided to the language model. The template includes the following parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">role</span></code>: Defines the role or persona the model should assume.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">context</span></code>: Provides the background information or context for the task.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">instruction</span></code>: Specifies the task or action the model needs to perform.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">input_text</span></code>: Contains the actual text input that the model will process.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">requirements</span></code>: Lists any specific requirements or constraints for the output.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="k">def</span> <span class="nf">get_base_prompt_template</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    
    <span class="n">base_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    ROLE: </span><span class="si">{role}</span>
<span class="s2">    CONTEXT: </span><span class="si">{context}</span>
<span class="s2">    INSTRUCTION: </span><span class="si">{instruction}</span>
<span class="s2">    INPUT: </span><span class="si">{input}</span>
<span class="s2">    REQUIREMENTS: </span><span class="si">{requirements}</span>
<span class="s2">    &quot;&quot;&quot;</span>
    
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">base_prompt</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">prompt</span>
</pre></div>
</div>
</div>
</div>
<p>We will write a simple function that returns an <code class="docutils literal notranslate"><span class="pre">LLMChain</span></code> which is a simple <code class="docutils literal notranslate"><span class="pre">langchain</span></code> construct that allows you to chain together a combination of prompt templates, language models and output parsers.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>
<span class="kn">from</span> <span class="nn">langchain_community.chat_models</span> <span class="kn">import</span> <span class="n">ChatLiteLLM</span>

<span class="k">def</span> <span class="nf">get_llm_chain</span><span class="p">(</span><span class="n">prompt_template</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns an LLMChain instance using langchain.</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt_template (str): The prompt template to use.</span>
<span class="sd">        model_name (str): The name of the model to use.</span>
<span class="sd">        temperature (float): The temperature setting for the model.</span>

<span class="sd">    Returns:</span>
<span class="sd">        llm_chain: An instance of the LLMChain.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
    <span class="kn">import</span> <span class="nn">os</span>

    <span class="c1"># Load environment variables from .env file</span>
    <span class="n">load_dotenv</span><span class="p">()</span>
    
    <span class="n">api_key_label</span> <span class="o">=</span> <span class="n">model_name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot;_API_KEY&quot;</span>
    <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatLiteLLM</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">api_key_label</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="n">llm_chain</span> <span class="o">=</span> <span class="n">prompt_template</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">llm_chain</span>
</pre></div>
</div>
</div>
</div>
<p>Now, we will write a function (<code class="docutils literal notranslate"><span class="pre">get_dynamic_prompt_template</span></code>) that constructs prompt parameters dynamically for each chunk.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>
<span class="k">def</span> <span class="nf">get_dynamic_prompt_params</span><span class="p">(</span><span class="n">prompt_params</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span> 
                            <span class="n">part_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> 
                            <span class="n">total_parts</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                            <span class="n">chat_context</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                            <span class="n">chunk</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Construct prompt template dynamically per chunk while maintaining the chat context of the response generation.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        prompt_params (Dict): Original prompt parameters</span>
<span class="sd">        part_idx (int): Index of current conversation part</span>
<span class="sd">        total_parts (int): Total number of conversation parts</span>
<span class="sd">        chat_context (str): Chat context from previous parts</span>
<span class="sd">        chunk (str): Current chunk of text to be processed</span>
<span class="sd">    Returns:</span>
<span class="sd">        str: Dynamically constructed prompt template with part-specific params</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dynamic_prompt_params</span> <span class="o">=</span> <span class="n">prompt_params</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="c1"># saves the chat context from previous parts</span>
    <span class="n">dynamic_prompt_params</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">chat_context</span>
    <span class="c1"># saves the current chunk of text to be processed as input</span>
    <span class="n">dynamic_prompt_params</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">chunk</span>
    
    <span class="c1"># Add part-specific instructions</span>
    <span class="k">if</span> <span class="n">part_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="c1"># Introduction part</span>
        <span class="n">dynamic_prompt_params</span><span class="p">[</span><span class="s2">&quot;instruction&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        You are generating the Introduction part of a long report.</span>
<span class="s2">        Don&#39;t cover any topics yet, just define the scope of the report.</span>
<span class="s2">        &quot;&quot;&quot;</span>
    <span class="k">elif</span> <span class="n">part_idx</span> <span class="o">==</span> <span class="n">total_parts</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span> <span class="c1"># Conclusion part</span>
        <span class="n">dynamic_prompt_params</span><span class="p">[</span><span class="s2">&quot;instruction&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        You are generating the last part of a long report. </span>
<span class="s2">        For this part, first discuss the below INPUT. Second, write a &quot;Conclusion&quot; section summarizing the main points discussed given in CONTEXT.</span>
<span class="s2">        &quot;&quot;&quot;</span>
    <span class="k">else</span><span class="p">:</span> <span class="c1"># Main analysis part</span>
        <span class="n">dynamic_prompt_params</span><span class="p">[</span><span class="s2">&quot;instruction&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        You are generating part </span><span class="si">{</span><span class="n">part_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> of </span><span class="si">{</span><span class="n">total_parts</span><span class="si">}</span><span class="s2"> parts of a long report.</span>
<span class="s2">        For this part, analyze the below INPUT.</span>
<span class="s2">        Organize your response in a way that is easy to read and understand either by creating new or merging with previously created structured sections given in CONTEXT.</span>
<span class="s2">        &quot;&quot;&quot;</span>
    
    <span class="k">return</span> <span class="n">dynamic_prompt_params</span>
</pre></div>
</div>
</div>
</div>
<p>Finally, we will write a function that generates the actual report by calling the <code class="docutils literal notranslate"><span class="pre">LLMChain</span></code> with the dynamically updated prompt parameters for each chunk and concatenating the results at the end.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate_report</span><span class="p">(</span><span class="n">input_content</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">llm_model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> 
                    <span class="n">role</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">requirements</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                    <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="c1"># stores the parts of the report, each generated by an individual LLM call</span>
    <span class="n">report_parts</span> <span class="o">=</span> <span class="p">[]</span> 
    <span class="c1"># split the input content into chunks</span>
    <span class="n">chunks</span> <span class="o">=</span> <span class="n">get_chunks</span><span class="p">(</span><span class="n">input_content</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="p">)</span>
    <span class="c1"># initialize the chat context with the input content</span>
    <span class="n">chat_context</span> <span class="o">=</span> <span class="n">input_content</span>
    <span class="c1"># number of parts to be generated</span>
    <span class="n">num_parts</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span>

    <span class="n">prompt_params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="n">role</span><span class="p">,</span> <span class="c1"># user-provided</span>
        <span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="c1"># dinamically updated per part</span>
        <span class="s2">&quot;instruction&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="c1"># dynamically updated per part</span>
        <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="c1"># dynamically updated per part</span>
        <span class="s2">&quot;requirements&quot;</span><span class="p">:</span> <span class="n">requirements</span> <span class="c1">#user-priovided</span>
    <span class="p">}</span>

    <span class="c1"># get the LLMChain with the base prompt template</span>
    <span class="n">llm_chain</span> <span class="o">=</span> <span class="n">get_llm_chain</span><span class="p">(</span><span class="n">get_base_prompt_template</span><span class="p">(),</span> 
                                 <span class="n">llm_model_name</span><span class="p">)</span>

    <span class="c1"># dynamically update prompt_params per part</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generating </span><span class="si">{</span><span class="n">num_parts</span><span class="si">}</span><span class="s2"> report parts&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chunks</span><span class="p">):</span>
        <span class="n">dynamic_prompt_params</span> <span class="o">=</span> <span class="n">get_dynamic_prompt_params</span><span class="p">(</span>
            <span class="n">prompt_params</span><span class="p">,</span>
            <span class="n">part_idx</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
            <span class="n">total_parts</span><span class="o">=</span><span class="n">num_parts</span><span class="p">,</span>
            <span class="n">chat_context</span><span class="o">=</span><span class="n">chat_context</span><span class="p">,</span>
            <span class="n">chunk</span><span class="o">=</span><span class="n">chunk</span>
        <span class="p">)</span>
        
        <span class="c1"># invoke the LLMChain with the dynamically updated prompt parameters</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">llm_chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">dynamic_prompt_params</span><span class="p">)</span>

        <span class="c1"># update the chat context with the cummulative response</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">chat_context</span> <span class="o">=</span> <span class="n">response</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">chat_context</span> <span class="o">=</span> <span class="n">chat_context</span> <span class="o">+</span> <span class="n">response</span>
            
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generated part </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">num_parts</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">report_parts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>

    <span class="n">report</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">report_parts</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">report</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="example-usage">
<h2>Example Usage<a class="headerlink" href="#example-usage" title="Permalink to this heading">¶</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the text from sample 10K SEC filing</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/apple.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the chunk and chunk overlap size</span>
<span class="n">MAX_CHUNK_SIZE</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">MAX_CHUNK_OVERLAP</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">report</span> <span class="o">=</span> <span class="n">generate_report</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">llm_model_name</span><span class="o">=</span><span class="s2">&quot;gemini/gemini-1.5-flash-latest&quot;</span><span class="p">,</span> 
                           <span class="n">role</span><span class="o">=</span><span class="s2">&quot;Financial Analyst&quot;</span><span class="p">,</span> 
                           <span class="n">requirements</span><span class="o">=</span><span class="s2">&quot;The report should be in a readable, structured format, easy to understand and follow. Focus on finding risk factors and market moving insights.&quot;</span><span class="p">,</span>
                           <span class="n">chunk_size</span><span class="o">=</span><span class="n">MAX_CHUNK_SIZE</span><span class="p">,</span> 
                           <span class="n">chunk_overlap</span><span class="o">=</span><span class="n">MAX_CHUNK_OVERLAP</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Generating 5 report parts
Generated part 1/5.
Generated part 2/5.
Generated part 3/5.
Generated part 4/5.
Generated part 5/5.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Save the generated report to a local file</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/apple_report.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">report</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">report</span><span class="p">[:</span><span class="mi">300</span><span class="p">]</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">...</span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">report</span><span class="p">[</span><span class="o">-</span><span class="mi">300</span><span class="p">:])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>**Introduction**

This report provides a comprehensive analysis of Apple Inc.&#39;s financial performance and position for the fiscal year ended September 28, 2024, as disclosed in its Form 10-K filing with the United States Securities and Exchange Commission.  The analysis will focus on identifying key
...
luation.  The significant short-term obligations, while manageable given Apple&#39;s cash position, highlight the need for continued financial discipline and effective risk management.  A deeper, more granular analysis of the financial statements and notes is recommended for a more complete assessment.
</pre></div>
</div>
</div>
</div>
<section id="discussion">
<h3>Discussion<a class="headerlink" href="#discussion" title="Permalink to this heading">¶</a></h3>
<p>Results from the generated report present a few interesting aspects:</p>
<ul class="simple">
<li><p><strong>Coherence</strong>: The generated report demonstrates a high level of coherence. The sections are logically structured, and the flow of information is smooth. Each part of the report builds upon the previous sections, providing a comprehensive analysis of Apple Inc.’s financial performance and key risk factors. The use of headings and subheadings helps in maintaining clarity and organization throughout the document.</p></li>
<li><p><strong>Adherence to Instructions</strong>: The LLM followed the provided instructions effectively. The report is in a readable, structured format, and it focuses on identifying risk factors and market-moving insights as requested. The analysis is detailed and covers various aspects of Apple’s financial performance, including revenue segmentation, profitability, liquidity, and capital resources. The inclusion of market-moving insights adds value to the report, aligning with the specified requirements.</p></li>
</ul>
<p>Despite the high quality of the results, there are some limitations to consider:</p>
<ul class="simple">
<li><p><strong>Depth of Analysis</strong>: While the report covers a wide range of topics, the depth of analysis in certain sections may not be as comprehensive as a human expert’s evaluation. Some nuances and contextual factors might be overlooked by the LLM. Splitting the report into multiple parts helps in mitigating this issue.</p></li>
<li><p><strong>Chunking Strategy</strong>: The current approach splits the text into chunks based on size, which ensures that each chunk fits within the model’s token limit. However, this method may disrupt the logical flow of the document, as sections of interest might be split across multiple chunks. An alternative approach could be “structured” chunking, where the text is divided based on meaningful sections or topics. This would preserve the coherence of each section, making it easier to follow and understand. Implementing structured chunking requires additional preprocessing to identify and segment the text appropriately, but it can significantly enhance the readability and logical flow of the generated report.</p></li>
</ul>
</section>
</section>
<section id="implications">
<h2>Implications<a class="headerlink" href="#implications" title="Permalink to this heading">¶</a></h2>
<p>Implementing context chunking with contextual linking is a practical solution to manage the output size limitations of LLMs. However, this approach comes with its own set of implications that developers must consider.</p>
<ol class="arabic simple">
<li><p><strong>Increased Development Complexity</strong>: Implementing strategies to overcome the maximum output token length introduces additional layers of complexity to the application design. It necessitates meticulous management of context across multiple outputs to maintain coherence. Ensuring that each chunk retains the necessary context for the conversation or document can be challenging and often requires advanced logic to handle transitions seamlessly.</p></li>
<li><p><strong>Cost Implications</strong>: Attempting to circumvent the <code class="docutils literal notranslate"><span class="pre">max_output_tokens</span></code> limitation by making multiple requests can increase the number of tokens processed, thereby raising the operational costs associated with using LLM services. Each additional request contributes to the overall token usage, which can quickly escalate costs, especially for applications with high-frequency interactions or large volumes of data.</p></li>
<li><p><strong>Performance Bottlenecks</strong>: Generating long outputs in segments can lead to performance bottlenecks, as each segment may require additional processing time and resources, impacting the overall efficiency of the application. The need to manage and link multiple chunks can introduce latency and reduce the responsiveness of the system, which is critical for real-time applications.</p></li>
</ol>
<p>By understanding these implications, developers can better prepare for the challenges associated with context chunking and contextual linking, ensuring that their applications remain efficient, cost-effective, and user-friendly.</p>
</section>
<section id="future-considerations">
<h2>Future Considerations<a class="headerlink" href="#future-considerations" title="Permalink to this heading">¶</a></h2>
<p>As models evolve, we can expect several advancements that will significantly impact how we handle output size limitations:</p>
<ol class="arabic simple">
<li><p><strong>Contextual Awareness</strong>: Future LLMs will likely have improved contextual awareness - or as Mustafa Suleyman would call “infinite memory”, enabling them to better understand and manage the context of a conversation or document over long interactions. This will reduce the need for repetitive context setting and improve the overall user experience.</p></li>
<li><p><strong>More Efficient Token Usage</strong>: Advances in model architecture and tokenization strategies will lead to more efficient token usage. This means that models will be able to convey the same amount of information using fewer tokens, reducing costs and improving performance.</p></li>
<li><p><strong>Improved Compression Techniques</strong>: As research progresses, we can expect the development of more sophisticated compression techniques that allow models to retain essential information while reducing the number of tokens required. This will be particularly useful for applications that need to summarize or condense large amounts of data.</p></li>
<li><p><strong>Adaptive Token Limits</strong>: Future models may implement adaptive token limits that dynamically adjust based on the complexity and requirements of the task at hand. This will provide more flexibility and efficiency in handling diverse use cases.</p></li>
<li><p><strong>Enhanced Memory Management</strong>: Innovations in memory management will allow models to handle larger outputs without a significant increase in computational resources. This will make it feasible to deploy advanced LLMs in resource-constrained environments.</p></li>
</ol>
<p>These advancements will collectively enhance the capabilities of LLMs, making them more powerful and versatile tools for a wide range of applications. However, they will also introduce new challenges and considerations that developers and researchers will need to address to fully harness their potential.</p>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this heading">¶</a></h2>
<p>In conclusion, while managing output size limitations in LLMs presents significant challenges, it also drives innovation in application design and optimization strategies. By implementing techniques such as context chunking, efficient prompt templates, and graceful fallbacks, developers can mitigate these limitations and enhance the performance and cost-effectiveness of their applications. As the technology evolves, advancements in contextual awareness, token efficiency, and memory management will further empower developers to build more robust and scalable LLM-powered systems. It is crucial to stay informed about these developments and continuously adapt to leverage the full potential of LLMs while addressing their inherent constraints.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://langchain.readthedocs.io/en/latest/modules/text_splitter.html">LangChain Text Splitter</a>.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>
            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">

            <p class="logo"><a href="../markdown/toc.html">
              <img class="logo" src="../_static/logo.png" alt="Logo"/>
            </a></p>
          <div class="sidebar-resize-handle"></div>

<h3><a href="../markdown/toc.html">Table of Contents</a></h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../markdown/intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="nondeterminism.html">Non-determinism</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Output Size Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nondeterminism-NOTES.html">The Blessing and Curse of Non-determinism</a></li>
</ul>

<div><hr class="docutils"></div>
<ul>
  <li class="toctree-l1"><a class="reference internal" href="../genindex.html" accesskey="I">General Index</a></li>
</ul>
<div id="ethical-ad-placement"></div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>

    <nav class="relbar">
      <a class="previous" href="nondeterminism.html" aria-label="Previous topic" >
        <div class="icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M257.5 445.1l-22.2 22.2c-9.4 9.4-24.6 9.4-33.9 0L7 273c-9.4-9.4-9.4-24.6 0-33.9L201.4 44.7c9.4-9.4 24.6-9.4 33.9 0l22.2 22.2c9.5 9.5 9.3 25-.4 34.3L136.6 216H424c13.3 0 24 10.7 24 24v32c0 13.3-10.7 24-24 24H136.6l120.5 114.8c9.8 9.3 10 24.8.4 34.3z"/></svg>
        </div>
        <div class="title">
          <span class="text">
            <span class="direction">previous</span>
            Non-determinism
          </span>
        </div>
      </a>
      <a class="next" href="../nondeterminism-NOTES.html" aria-label="Next topic" >
        <div class="title">
          <span class="text">
            <span class="direction">next</span>
            The Blessing and Curse of Non-determinism
          </span>
        </div>
        <div class="icon">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!-- Font Awesome Free 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) --><path d="M190.5 66.9l22.2-22.2c9.4-9.4 24.6-9.4 33.9 0L441 239c9.4 9.4 9.4 24.6 0 33.9L246.6 467.3c-9.4 9.4-24.6 9.4-33.9 0l-22.2-22.2c-9.5-9.5-9.3-25 .4-34.3L311.4 296H24c-13.3 0-24-10.7-24-24v-32c0-13.3 10.7-24 24-24h287.4L190.9 101.2c-9.8-9.3-10-24.8-.4-34.3z"/></svg>
        </div>
      </a>
    </nav>

    <footer role="contentinfo">
      Created using <a class="reference external" href="https://www.sphinx-doc.org/">Sphinx</a> 6.2.1.
      <a class="reference external" href="https://insipid-sphinx-theme.readthedocs.io/">Insipid Theme</a>.
<a class="reference internal" href="../_sources/notebooks/output_size_limit.ipynb" rel="nofollow">Show Source</a>.
    </footer>
  </body>
</html>