{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Blessing and Curse of Non-determinism\n",
    "\n",
    "One of the most fundamental challenges when building products with Large Language Models (LLMs) is their non-deterministic nature. Unlike traditional software systems where the same input reliably produces the same output, LLMs can generate different responses each time they're queried - even with identical prompts. This characteristic is both a strength and a significant engineering challenge.\n",
    "\n",
    "## Understanding the Challenge\n",
    "\n",
    "### What is Non-determinism in LLMs?\n",
    "\n",
    "When you ask ChatGPT or any other LLM the same question multiple times, you'll likely get different responses. This isn't a bug - it's a fundamental feature of how these models work. The \"temperature\" parameter, which controls the randomness of outputs, allows models to be creative and generate diverse responses. However, this same feature makes it incredibly difficult to build reliable, testable systems.\n",
    "\n",
    "### Real-world Impact\n",
    "\n",
    "Consider a financial services company using LLMs to generate investment advice summaries. The non-deterministic nature of these models means that:\n",
    "- The same market data could yield different analysis conclusions\n",
    "- Testing becomes exponentially more complex\n",
    "- Regulatory compliance becomes challenging to guarantee\n",
    "- User trust may be affected by inconsistent responses\n",
    "\n",
    "## Technical Deep-dive: Sources of Non-determinism\n",
    "\n",
    "### Temperature and Sampling\n",
    "\n",
    "The primary source of non-determinism in LLMs comes from their sampling strategies. During text generation, the model:\n",
    "1. Calculates probability distributions for each next token\n",
    "2. Samples from these distributions based on temperature settings\n",
    "3. Uses techniques like nucleus sampling to balance creativity and coherence\n",
    "\n",
    "### The Temperature Spectrum\n",
    "\n",
    "- Temperature = 0: Most deterministic, but potentially repetitive\n",
    "- Temperature = 1: Balanced creativity and coherence\n",
    "- Temperature > 1: Increased randomness, potentially incoherent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "def generate_responses(\n",
    "    prompt: str,\n",
    "    temperatures: List[float],\n",
    "    attempts: int = 3\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate multiple responses at different temperature settings\n",
    "    to demonstrate non-deterministic behavior.\n",
    "    \"\"\"\n",
    "    client = OpenAI()\n",
    "    results = []\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        for attempt in range(attempts):\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temp,\n",
    "                max_tokens=50\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                'temperature': temp,\n",
    "                'attempt': attempt + 1,\n",
    "                'response': response.choices[0].message.content\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def demonstrate_temperature_effects(prompt: str,\n",
    "                                    attempts: int = 3, \n",
    "                                    temperatures: List[float] = [0.0, 1.0, 2.0]):\n",
    "    \"\"\"\n",
    "    Run a demonstration of temperature effects.\n",
    "    \"\"\"\n",
    "    results = generate_responses(prompt, temperatures, attempts)\n",
    "    \n",
    "    # Display results grouped by temperature\n",
    "    for temp in temperatures:\n",
    "        print(f\"\\nTemperature = {temp}\")\n",
    "        print(\"-\" * 40)\n",
    "        temp_responses = results[results['temperature'] == temp]\n",
    "        for _, row in temp_responses.iterrows():\n",
    "            print(f\"Attempt {row['attempt']}: {row['response']}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Temperature = 0.0\n",
      "----------------------------------------\n",
      "Attempt 1: \"Café Perk\"\n",
      "Attempt 2: \"Café Perk\"\n",
      "Attempt 3: \"Café Perk\"\n",
      "\n",
      "Temperature = 1.0\n",
      "----------------------------------------\n",
      "Attempt 1: \"Bean Bliss Cafe\"\n",
      "Attempt 2: \"Caffeine Haven\"\n",
      "Attempt 3: \"Café Haven\"\n",
      "\n",
      "Temperature = 2.0\n",
      "----------------------------------------\n",
      "Attempt 1: \"Caffeine Chronicles\"\n",
      "Attempt 2: Earthbean Café\n",
      "Attempt 3: \"Beans & Birch: Grab a Brew\"\n"
     ]
    }
   ],
   "source": [
    "df_results = demonstrate_temperature_effects(\"Suggest a name for a coffee shop\", temperatures=[0.0, 1.0, 2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
